As ANNs continue to grow increasingly large, and the neuron models increasingly complex, the energy requirements of our computations become a serious concern.
The people who paved way for our modern logic-gate, computer architecture, also wrote about intelligent machines, and thus,
it makes sense to believe that their 1940s foundation of this architecture was inspired by the theories of intelligence that were popular at that time.
In general, scientists agreed that intelligence was mainly large sequences of logic. If this consensus inspired our modern computers,
one could argue that it was not far from reality. After all, our modern software is seemingly intelligent, in many ways \cite{haenlein_brief_2019}.
Now that the latest neuronal models no longer resemble logic-gates, and scientists mostly agree that cognition is about more than logic,
it might be a good idea to search for computer architectures inspired by more advanced models than logic-gates.
Especially if the goal is to recreate the functionality and efficiency of biological brains.
However, the core of a computer is mainly performing arithmetic operations, as the name \textbf{compute}-r might suggest,
and arithmetic operations are little more than sequences of logic operations. Therefore, a machine not based on logic operations,
might not be called a computer.
Such machines was first proposed by Carver Mead in 1990, and was termed Neuromorphic Elecronic Systems, and the sole aim is to let the architectures take inspiration from biology at a much lower heuristic level than the architecture of the 40s \cite{mead_neuromorphic_1990}.

The system proposed by Mead was an adaptive analog wafer-scale silicon fabrication.
In would be more robust and use far less power than a traditional computer,
and would in theory be more robust and use far less power than a traditional computer \cite{mead_neuromorphic_1990}.
The search for a good Neuromorphic System is currently in an explorational phase, with architectures varying between analog, digital and mixed-signal \cite{schuman_survey_2017}.
Most are built to simulate SNNs with higher efficiency than modern computers, with components for neurons and synapses that uses less computational power to model their biological counterparts.
(From \vref{sect:snn} it is clear that SNN is strictly not logic-gate networks.)
The most promising Neuromorphic Systems to date are IBM BlueNorth, SpiNNaker, Tianjic, and BrainScaleS \cite{furber_large-scale_2016} \cite{schuman_survey_2017} \cite{pei_towards_2019}.
Most of these are massively parallel digital logic-gate systems, but with architecture inspired by the detail of the synaptic connections modeled in SNNs.\cite{aamir_accelerated_2018}
A few architectures are a hybrid between digital and analog electronics, like the Tianjic. While BrainScaleS is based on an analog wafer-scale architecture.
For this project, all neuromorphic development happens on the BrainScaleS system.

\subsection{BrainScaleS}
Based in Heidelberg, and essential part of the European Human Brain Project,
BrainScaleS-1 is a large computing system consisting that can simulate
up to 4(?) million physical Leaky Integrate-and-Fire neurons using
HICANN chips, that are composed connected on up to 20 Wafers.
The electrical spikes generated on the chips are converted to digital
signals using Field Programmable Gate Arrays (FPGAs), and transferred to a traditional High Performance
Computing cluster that processes the information \cite{schemmel_wafer-scale_2010}.

Each Wafer consist of 48 HICANNs, that are configurable as Evolvable
Hardware \cite{trefzer_evolvable_2015}, in terms of adjusting the synaptic
connections and weights \cite{schemmel_wafer-scale_2010}. In addition to reading
the spike output, the FPGAs are responsible for input stimulus and
routing and mapping of the neural networks. The programming of these
FPGAs are developed as a sort of firmware to the system, while the
end-user is mainly concerned with a Python interface, running on the
cluster.


















