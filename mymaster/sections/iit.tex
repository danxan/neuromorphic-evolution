The Integrated Information Theory (IIT) has been proposed as a mathematical way to understand consciousness.
It revolves around analysing the phenomenology of a system, that is, how a system experiences the environment.
IIT 3.0 essentially does this by finding the maximally irreducible conceptual structures formed by a causal structure interacting with the environment.
The theory tries to assume properties of consciousness (axioms), and from there it posulates the properties that must be necessary in a physical substrate hosting the system.
The causal structure is based on measures of how a system is able to integrate information, and the tools provided is the specification of a cause-effect structure (CES).
The amount of information is larger the higher the amount of possible states in the system.
Integrated information is when there is no way to cut without loosing information.
The CES is an unfolding of a model of the neural substrate, in the sense that the model is cut in every possible way (imagine cutting a network of nodes).
The CES is mapped to the properties of experience, and the CES can be quantified by $\Phi$. \cite{oizumi_phenomenology_2014}

Although IIT might seem overly abstract, the first tangible result was published as a study on the question "Why does space feel the way it does?".
Here a model of the Visual Cortex 1 and 2 (V1 and V2) was cut into a CES, using the knowledge of the grid cells that have been proven to be important for localization \cite{haun_why_2019}.

The following is an attempt to summarize some corollaries of the theory, based on the article by Oizumi et. al \cite{oizumi_phenomenology_2014}.
A conscious system will usually consist of a main \textbf{complex}, and smaller supporting complexes.
This main complex will be the most conscious part of the system, like the part of our brain that we could not function without, while the supporting complexes could be compared to smaller parts of the brain, like for example the visual cortex.
Conscious systems will not have modular, that is, produce less functionality than their components, and they have to produce functionality that is more than a higher order of the combined components.
Inactive systems can be conscious, in the way that a system may have big parts that are ready to be activated or that are passively affecting the state of the system.
A system can perform very complex functions, but still not be conscious.
In particular, feed-forward networks would not be conscious.
An example if this is a microprocessor implementing a neural network that, in some cases, can recognize thousands of different objects faster than a human can recognize one object.
IIT states that it is not only the functionality of a system that determines how conscious it is, but also how the function is performed internally.
Obviously, a network that can recognize objects based on internal states and previous experiences, is more conscious compared to a feed-forward neural network who only recognizes the object based on the external input it gets.
Networks that are not necessarily feed-forward only, but simulated on a physical substrate that implements functionality based on numerical approximation, would not be termed conscious in IIT 3.0 \cite{marshall_integrated_2016}.
This leads to a final corollary, that says that a conscious system is able to develop concepts based on other concepts within itself, without the need of external stimuli.
These internal concepts would more often be connected to a large number of other concepts, and not contribute to specific details.

In this project, I hope to utilize IIT as a way to analyze the perception of automated agents, evolved as SNN \vref{sect:snn} animats \vref{sect:agent} on BrainScaleS \vref{sect:bss}.
As the complexity and states of the animats is known, the animats can be analyzed.
Analyzing the animats in regards to IIT will only be attempted if time permits.
