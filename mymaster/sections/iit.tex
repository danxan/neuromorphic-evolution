The Integrated Information Theory (IIT) has is proposed as a mathematical way to understand consciousness.
It revolves around analyzing the phenomenology of a system, that is, how a system experiences the environment.
IIT 3.0 mostly does this by finding the maximally irreducible conceptual structures formed by a causal structure interacting with the environment.
The theory tries to assume properties of consciousness (axioms), and from there, it postulates the properties that must be necessary for a physical substrate hosting the system.
The causal structure, also called a cause-effect structure (CES), is analyzed to find conceptual structures that measure how well a system integrates information.
A more substantial amount of integrated information results in a higher amount of possible states in the system.
Integrated information is when there is no way to cut the CES without losing information.
The CES is an unfolding of a model of the neural substrate in the sense that the model is cut in every possible way (imagine cutting a network of nodes).
The CES maps to the properties of experience, and the CES can be quantified by $\Phi$ \cite{oizumi_phenomenology_2014}.

Although IIT might seem overly abstract, the first tangible result was published as a study on the question "Why does space feel the way it does?".
Here a model of the Visual Cortex 1 and 2 (V1 and V2) was cut into a CES, using the knowledge of the grid cells that are proven to be essential for localization \cite{haun_why_2019}.

\paragraph{A summary of corollaries} of IIT, based on the article by Oizumi et. al \cite{oizumi_phenomenology_2014}:
An intelligent system will usually consist of a main \textbf{complex}, and smaller supporting complexes.
This central complex will be the most conscious part of the system, like the part of our brain that we could not function without, while the supporting complexes could be compared to smaller parts of the brain, like, for example, the visual cortex.
Intelligent systems will not be modular, that is, if the system produces less functionality than their components, and they have to produce functionality that is more than a higher order of the combined components.
Inactive systems can be conscious in the way that a system may have significant parts that are ready to be activated or that are passively affecting the state of the system.
A system can perform very complex functions but still not be conscious.
In particular, feed-forward networks would not be conscious.
An example if this is a microprocessor implementing a neural network that, in some cases, can recognize thousands of different objects faster than a human can recognize one object.
IIT states that it is not only the functionality of a system that determines how conscious it is but also how the function performs internally.
A network that can recognize objects based on internal states and previous experiences is more conscious compared to a feed-forward neural network, which only recognizes the object based on the external input it gets.
Networks that are not necessarily feed-forward only, but simulated on a physical substrate that implements functionality based on numerical approximation, would not be termed conscious in IIT 3.0 \cite{marshall_integrated_2016}.
A final corollary that summarises the ones above says that an intelligent system can develop concepts based on other concepts within itself without the need for external stimuli.
These internal concepts would more often be connected to a large number of other concepts and not contribute to specific details.

\paragraph{In this project} I hope to utilize IIT as a way to analyze the perception of automated agents, evolved as SNN \vref{sect:snn} animats \vref{sect:agent} on BrainScaleS \vref{sect:bss}.
As the complexity and states of the animats are known, the animats can be analyzed.
Analyzing the animats in regards to IIT will only be attempted if time permits.
