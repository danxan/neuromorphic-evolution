\paragraph{Today} most developers consider the von Neumann architecture as the most efficient and scalable architecture for doing arithmetic computation.
However, as our expectations of the functionality of the computer grow increasingly similar to the functionality of biological systems, we will require devices that are not arithmetic machines but rather "thinking machines."
It is essential to consider how computer architecture usually has been modeled after then brain since the very beginning, but always at a simplified notion compared to the current knowledge of biology and psychology.

\paragraph{Consider the history} of our computer architecture:
The philosopher George Boole's maybe most excellent work was his mathematical An Investigation of the Laws of Thought, where the notion that human reasoning could be summed up by symbolic logic \cite{boole_investigation_1854}.
This work later inspired Claude Shannon to build the first "switching circuits" that facilitated the design for combinatorial logic circuits and led to the vacuum tube technology used in early computers.
John von Neumann draws parallels to the human neurons in his report on the EDVAC, which was used by Alan Turing as a theoretical source of understanding while designing the Automatic Computing Engine \cite{von_neumann_first_1993}.
The theoretical attempt to "capture human intelligence" and mechanical analogies to neurons led to our computer architecture.
Combinatorial logic eventually led to the development of modern Information Theory, which was proposed by the same Shannon in his A Mathematical Theory of Communication.
Thus, the foundational basics of our computer programs are also rooted in the search for intelligent systems.
The most successful artificial intelligence programs were, for a long time, only compositions of logical if-then sentences \cite{haenlein_brief_2019}.
Today most artificial intelligence programs are based on arithmetic approximation of complex functions through the use of millions of logic gates in the form of transistors.
Over the past 70 years, our understanding of the nervous system and human psychology has developed drastically. However, modern computing is still rooted in the same "outdated" knowledge of the human brain.

\paragraph{The efficiency} of our brains outperforms modern supercomputers in several aspects, like pattern recognition, grasping the "essence" of books, and creative thinking.
In other aspects, like arithmetics and approximation, the supercomputer is far more efficient.
Even though the human brain can perform arithmetics, it is an activity that consumes a lot more power than a modern cellphone does for the same amount of arithmetic work. However, the human brain performs other marvels, like image recognition, without spending much energy at all, to the very contrary of modern cellphones \cite{meier_special_2017}. The brain has a small, underdeveloped center for arithmetics, while the core of the computer only performs arithmetics \cite{dehaene_arithmetic_2004}.
To successfully create machines that can efficiently perform functions similar to perception or even sensing, we ought, therefore, to explore solutions based on modern biology and psychology.
Even many of the founders of computing, like Ada Lovelace, Neumann, and Turing, admitted that the digital computer could not achieve the same functionality as the human brain.
A more biologically inspired machine was attempted by Rosenblatt with the Perceptron as mentioned in \vref{sect:intro}, and this is now the foundational algorithm for the ANNs powering state-of-the-art artificial intelligence, on modern von Neumann computers.
However, modern computers are becoming increasingly power-hungry, and we are reaching the end of Moore's law \cite{mead_neuromorphic_1990} \cite{moore_cramming_1998}.
The very same man that coined the term "Moore's Law" after his friend George Moore proposed a theoretical system that could overcome the limits of modern digital computers.
This man was Carver Mead, who proposed Neuromorphic Electronic Systems, as devices implemented and organized after the same principles as the nervous system.
The idea is that the nervous system performs "computational" marvels with excellent efficiency and that this is mainly due to how the nervous system utilizes the physical characteristics of its units \cite{mead_neuromorphic_1990}.

\paragraph{The challenges} related to building a neuromorphic system are many;
An accurate brain model is needed to design better devices.
To be able to build the devices, developers first need to research proper techniques and materials.
Any utilization of the systems requires a programming framework.
Lastly, the developers need to prove that the system has applicability to real-world problems.
\cite{schuman_survey_2017}.

\subsection{Models, Algorithms, Devices, and Frameworks}
There are several popular categories of spiking neuron models.

\paragraph{Biologically Plausible and Biologically Inspired models} incorporate some aspects of biological systems, but that has been proved useful in applications.
An example is the Hodkin-Huxley model, which describes the electrical characteristics of neurons, much like a conductance-based electrical circuit.
Another example is the popular Izhikevich model, which is simple but reproduces many of the computational features of the neuron.
\cite{schuman_survey_2017}

\paragraph{Integrate-and-Fire (IAF) models} are varying on the complexity spectrum. Some are advanced, while others are less biologically realistic, but useful.
The model Leaky IAF is the most popular, and it comes with several additions, like Exponential Leaky IAF and Adaptive Exponential Leaky IAF. Typical SNNs usually incorporate one of these variations of IAF.
\cite{schuman_survey_2017}

\paragraph{Synaptic models} typically include ion channels or Spike-timing-dependent plasticity (STDP).
STDP can be simplified to cover: pre-synaptic and post-synaptic "weights," axonal restructuring, dendritic restructuring and neuronal excitability, Long-Term Potentiation, and Long-Term Depression.
These are the processes of neuronal dynamics that are most important to model when building a neuromorphic system.
The BrainScaleS system implements several of these STDP process, including LTP and LTD \cite{meier_mixed-signal_2015}\cite{schemmel_implementing_2006}.
\cite{schuman_survey_2017}

\paragraph{Network models} are usually SNNs, and popular variations are Spiking Feed-Forward, Spiking Deep Belief, Spiking Hebbian, Spiking Hopfield, Associative Memories, Spiking Winner-Take-All, Spiking Probabilistic, Spiking Random. Common for all these is that the typical training is similar to traditional ANNs, which will not utilize the full potential of either the model or the neuromorphic system.
\cite{schuman_survey_2017}

\paragraph{Algorithms and Learning} are specific to network characteristics, and can be divided into two types, on-chip training or transferred learning from off-chip training.
Typically, online training is unsupervised, while offline training is unsupervised.
    Many researchers advocate that neuromorphic systems may have big self-learning capabilities that have yet to be proven. Catherine Schuman wrote about this: ``In particular, we need to understand how to best utilize the hardware itself in training and learning, as neuromorphic hardware systems will likely allow us to explore larger-scale spiking neural networks in a more computationally and resource-efficient way than is possible on traditional von Neumann architectures.''
\cite{schuman_survey_2017}

\paragraph{Devices and Hardware} divides into three categories; digital, analog, and mixed analog/digital.
The digital hardware consists of Boolean logic-gates and is usually synchronous a clock-based, but not always.
FPGA is often used to either prototype or to give the feature of radically different network topologies, models, and algorithms.
IBM TrueNorth is a digital system that has fully custom ASIC design, is partially asynchronous, but uses a clock for necessary time steps.
It has deterministic behavior but can generate stochasticity similar to what is possible with software.
SpiNNaker is a fully custom digital, massively parallel system.
It uses ARM integer chips, with custom interconnect to handle spikes.
The interconnect is very advanced and highly flexible but makes the system less efficient than some other systems \cite{furber_large-scale_2016} \cite{schuman_survey_2017}.
Analog neuromorphic systems benefit from the physical characteristics of analog circuits and are often asynchronous, fuzzy, with the conservation of charge, amplification thresholding, and integration.
As mentioned earlier, the first neuromorphic system proposed was a wafer-scale analog system \cite{mead_neuromorphic_1990}.
The idea was to utilize the unique characteristics of transistors to make computing more energy efficient.
Researches have used different types of Field-Programmable Transistor Arrays (FPTA), and these are often used for analog circuit design or to model neurons, synapses, or other components of the nervous system.
Traditionally, computing with analog circuits is complicated because of global asynchrony and noisy, unreliable components. Researchers theorize that neuromorphic design might overcome these traditional issues and use evolutionary algorithms to help design circuits that better utilize the characteristics of the analog components. \cite{langeheine_cmos_2001}.
For digital communication and storage of information, analog neuromorphic systems often utilize some digital components. A common term for a hybrid neuromorphic system is \textbf{mixed analog/digital neuromorphic systems}.
An example of values to be stored digitally is synapse weight values, intrinsic information of the system, or time series of activity. The systems which use some digital components are also easier to program, which is essential for development.
A mixed analog/digital system is Neurogrid, an analog chip that has an architecture very close to Mead's definition \cite{mead_neuromorphic_1990}.
It works in subthreshold mode.
Another hybrid is the Tianjic chip architecture, which performs computing with both analog and digital circuitry \cite{pei_towards_2019}.
A third mixed analog/digital system is BrainScaleS, which is the system of focus for this project.
It has a wafer-scale analog architecture, which was also proposed by Mead \cite{mead_neuromorphic_1990} and works in a supertreshold mode because of the high rate of operation.
\cite{schuman_survey_2017}

\subsection{Evolvable Hardware}\label{sect:eh}
\subfile{sections/eh}

\subsection{BrainScaleS}\label{sect:bss}
\subfile{sections/bss}
