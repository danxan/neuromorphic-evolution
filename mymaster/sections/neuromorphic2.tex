Today, most developers consider the von Neumann architecture as the most efficient and scalable architecture for doing arithmetic computation.
However, as our expectations of the functionality of the computer grow increasingly similar to the functionality of biological systems, we will require devices that are not arithmetic machines but rather "thinking machines."
It is essential to consider how computer architecture usually has been modeled after then brain since the very beginning, but always at a simplified notion compared to the current knowledge of biology and psychology.
George Boole's maybe most excellent work was his philosophical and mathematical An Investigation of the Laws of Thought, where the notion that human reasoning could be summed up by symbolic logic \cite{boole_investigation_1854}.
This work later inspired Claude Shannon to build the first "switching circuits" that facilitated the design for combinatorial logic circuits, which found the basis of our modern computers.
John von Neumann draws parallels to the human neurons in his report on the EDVAC, which was used by Alan Turing as a source for understanding the design of a digital computer \cite{von_neumann_first_1993}.
For further development of the modern computer and its algorithms, Boolean logic as been quintessential. The most successful artificial intelligence programs were, for a long time, only compositions of if-then sentences \cite{haenlein_brief_2019}.
Through this brief investigation into the history of computation, it is safe to assume that modern arithmetic computer architecture bases itself on outdated knowledge of the best "thinking machine" that we know of; the human brain.
Even though the human brain can perform arithmetics, it is an activity that consumes a lot more power than a modern cellphone does for the same amount of arithmetic work. However, the human brain performs other marvels, like image recognition, without spending much energy at all, to the very contrary of modern cellphones \cite{meier_special_2017}. The brain has a small, underdeveloped center for arithmetics, while the core of the computer only performs arithmetics \cite{dehaene_arithmetic_2004}.
To successfully create machines that can efficiently perform functions similar to perception or even sensing, we ought, therefore, to explore solutions based on modern biology and psychology.
Even many of the founders of computing, like Ada Lovelace, Neumann, and Turing, admitted that the digital computer could not achieve the same functionality as the human brain.
A more biologically inspired machine was attempted by Rosenblatt with the Perceptron as mentioned in \vref{introduction}, and this is now the foundational algorithm for the ANNs powering state-of-the-art artificial intelligence, on modern von Neumann computers.
However, modern computers are becoming increasingly power-hungry, and we are reaching the end of Moore's law \cite{mead_neuromorphic_1990} \cite{moore_cramming_1998}.
The very same man that coined the term "Moore's Law" after his friend George Moore proposed a theoretical system that could overcome the limits of modern digital computers.
This man was Carver Mead, who proposed Neuromorphic Electronic Systems, as devices implemented and organized after the same principles as the nervous system.
The idea is that the nervous system performs "computational" marvels with excellent efficiency and that this is mainly due to how the nervous system utilizes the physical characteristics of its units \cite{mead_neuromorphic_1990}.

There are several challenges related to building such a neuromorphic system.
An accurate brain model is needed to design better devices.
To be able to build the devices, developers first need to research proper techniques and materials.
Any utilization of the systems requires a programming framework.
Lastly, the developers need to prove that the system has applicability to real-world problems.
\cite{schuman_survey_2017}.

\subsection{Models, Algorithms, Devices, and Frameworks}
There are several popular categories of spiking neuron models.

\paragraph{Biologically Plausible and Biologically Inspired models} incorporate some aspects of biological systems, but that has been proved useful in applications.
An example is the Hodkin-Huxley model, which describes the electrical characteristics of neurons, much like a conductance-based electrical circuit.
Another example is the popular Izhikevich model, which is simple but reproduces many of the computational features of the neuron.
\cite{schuman_survey_2017}

\paragraph{Integrate-and-Fire (IAF) models} are varying on the complexity spectrum. Some are advanced, while others are less biologically realistic but useful.
The model Leaky IAF is the most popular, and it comes with several additions, like Exponential Leaky IAF and Adaptive Exponential Leaky IAF. Typical SNNs usually incorporate one of these variations of IAF.
\cite{schuman_survey_2017}

\paragraph{Synaptic models} typically include ion channels or Spike-timing-dependent plasticity (STDP).
STDP can be simplified to cover: pre-synaptic and post-synaptic "weights," axonal restructuring, dendritic restructuring and neuronal excitability, Long-Term Potentiation, and Long-Term Depression.
These are the processes of neuronal dynamics that are most important to model when building a neuromorphic system.
The BrainScaleS system implements several of these STDP process, including LTP and LTD \cite{meier_mixed-signal_2015}\cite{schemmel_implementing_2006}.
\cite{schuman_survey_2017}

\paragraph{Network models} are usually SNNs, and popular variations are Spiking Feed-Forward, Spiking Deep Belief, Spiking Hebbian, Spiking Hopfield, Associative Memories, Spiking Winner-Take-All, Spiking Probabilistic, Spiking Random. Common for all these is that the typical training is similar to traditional ANNs, which will not utilize the full potential of either the model or the neuromorphic system.
\cite{schuman_survey_2017}

\paragraph{Algorithms and Learning} are specific to network characteristics, and can be divided into two types, on-chip training or transferred learning from off-chip training.
Typically, online training is unsupervised, while offline training is unsupervised.
    Many researchers advocate that neuromorphic systems may have big self-learning capabilities that have yet to be proven. Catherine Schuman wrote about this: ``In particular, we need to understand how to best utilize the hardware itself in training and learning, as neuromorphic hardware systems will likely allow us to explore larger-scale spiking neural networks in a more computationally and resource-efficient way than is possible on traditional von Neumann architectures.''
\cite{schuman_survey_2017}

\paragraph{Devices and Hardware} divides into three categories; digital, analog, and mixed analog/digital.
The digital hardware consists of Boolean logic-gates and is usually synchronous a clock-based, but not always.
FPGA is often used to either prototype or to give the feature of radically different network topologies, models, and algorithms.
IBM TrueNorth is a digital system that has fully custom ASIC design, is partially asynchronous but uses a clock for necessary time steps.
It has deterministic behavior but can generate stochasticity similar to what is possible with software.
SpiNNaker is a fully custom digital, massively parallel system.
It uses ARM integer chips, with custom interconnect to handle spikes.
The interconnect is very advanced and highly flexible but makes the system less efficient than some other systems \cite{furber_large-scale_2016} \cite{schuman_survey_2017}.
Analog neuromorphic systems benefit from the physical characteristics of analog circuits and are often asynchronous, fuzzy, with the conservation of charge, amplification thresholding, and integration.
As mentioned earlier, the first neuromorphic system proposed was a wafer-scale analog system \cite{mead_neuromorphic_1990}.
The idea was to utilize the unique characteristics of transistors to make computing more energy efficient.
Researches have used different types of Field-Programmable Transistor Arrays (FPTA), and these are often used for analog circuit design or to model neurons, synapses, or other components of the nervous system.
Traditionally, computing with analog circuits is complicated because of global asynchrony and noisy, unreliable components. Researchers theorize that neuromorphic design might overcome these traditional issues and use evolutionary algorithms to help design circuits that better utilize the characteristics of the analog components. \cite{langeheine_cmos_2001}.
For digital communication and storage of information, analog neuromorphic systems often utilize some digital components. These systems can be classified as mixed analog/digital neuromorphic systems.
An example of values to be stored digitally is synapse weight values, intrinsic information of the system, or time series of activity. The systems which use some digital components are also easier to program, which is essential for development.
A mixed analog/digital system is Neurogrid, an analog chip that has an architecture very close to Mead's definition \cite{mead_neuromorphic_1990}.
It works in subthreshold mode.
Another hybrid is the Tianjic chip architecture, which performs computing with both analog and digital circuitry \cite{pei_towards_2019}.
A third mixed analog/digital system is BrainScaleS, which is the system of focus for this project.
It has a wafer-scale analog architecture, which was also proposed by Mead \cite{mead_neuromorphic_1990} and works in a supertreshold mode because of the high rate of operation.
\cite{schuman_survey_2017}

\subsection{Evolutionary Hardware}\label{sect:eh}
\subfile{sections/eh}

\subsection{BrainScaleS}
\subfile{sections/bss}
