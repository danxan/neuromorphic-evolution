
@article{stoltz_augmented_2017,
	title = {Augmented Reality in Warehouse Operations: Opportunities and Barriers},
	volume = {50},
	issn = {24058963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405896317324291},
	doi = {10.1016/j.ifacol.2017.08.1807},
	shorttitle = {Augmented Reality in Warehouse Operations},
	pages = {12979--12984},
	number = {1},
	journaltitle = {{IFAC}-{PapersOnLine}},
	shortjournal = {{IFAC}-{PapersOnLine}},
	author = {Stoltz, Marie-Hélène and Giannikas, Vaggelis and {McFarlane}, Duncan and Strachan, James and Um, Jumyung and Srinivasan, Rengarajan},
	urldate = {2019-11-04},
	date = {2017-07},
	langid = {english},
	file = {Stoltz et al. - 2017 - Augmented Reality in Warehouse Operations Opportu.pdf:/home/danielsan/Zotero/storage/E7TNXH3S/Stoltz et al. - 2017 - Augmented Reality in Warehouse Operations Opportu.pdf:application/pdf}
}

@article{reif_pick-by-vision:_2009,
	title = {Pick-by-vision: augmented reality supported order picking},
	volume = {25},
	issn = {0178-2789, 1432-2315},
	url = {http://link.springer.com/10.1007/s00371-009-0348-y},
	doi = {10.1007/s00371-009-0348-y},
	shorttitle = {Pick-by-vision},
	abstract = {Order picking is one of the most important process steps in logistics. Due to their ﬂexibility, human beings cannot be replaced by machines. But if workers in order picking systems are equipped with a head-mounted display, Augmented Reality can improve the information visualization. In this paper the development of such a Pick-by-Vision system is presented. It is evaluated in a user study performed in a real storage environment. Important logistic ﬁgures as well as subjective ﬁgures were measured. The results show that Pick-by-Vision can improve order picking processes on a big scale.},
	pages = {461--467},
	number = {5},
	journaltitle = {The Visual Computer},
	shortjournal = {Vis Comput},
	author = {Reif, Rupert and Günthner, Willibald A.},
	urldate = {2019-11-04},
	date = {2009-05},
	langid = {english},
	file = {Reif and Günthner - 2009 - Pick-by-vision augmented reality supported order .pdf:/home/danielsan/Zotero/storage/TAU97XGI/Reif and Günthner - 2009 - Pick-by-vision augmented reality supported order .pdf:application/pdf}
}

@article{ginters_low_2013,
	title = {Low Cost Augmented Reality and {RFID} Application for Logistics Items Visualization},
	volume = {26},
	issn = {18770509},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S187705091301274X},
	doi = {10.1016/j.procs.2013.12.002},
	abstract = {One important component of the gross domestic product ({GDP}) is logistics services the quality and added value of which is growing due to the application of modern information and communication technologies and electronics. {RFID} use increases the performance of logistics items identification, however some errors, which could cause substantial damage and losses, remain. The amount of potential errors could be diminished by the additional checking of items using 3D visualisation. The authors researched the use of augmented reality for item visualisation in a warehouse combining {AR} and {RFID} solutions.},
	pages = {3--13},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Ginters, Egils and Martin-Gutierrez, Jorge},
	urldate = {2019-11-04},
	date = {2013},
	langid = {english},
	file = {Ginters and Martin-Gutierrez - 2013 - Low Cost Augmented Reality and RFID Application fo.pdf:/home/danielsan/Zotero/storage/FJQYXXJF/Ginters and Martin-Gutierrez - 2013 - Low Cost Augmented Reality and RFID Application fo.pdf:application/pdf}
}

@article{moral_sequential_2006,
	title = {Sequential Monte Carlo samplers},
	volume = {68},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2006.00553.x},
	doi = {10.1111/j.1467-9868.2006.00553.x},
	abstract = {Summary. We propose a methodology to sample sequentially from a sequence of probability distributions that are defined on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
	pages = {411--436},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Moral, Pierre Del and Doucet, Arnaud and Jasra, Ajay},
	urldate = {2019-10-16},
	date = {2006},
	langid = {english},
	keywords = {Importance sampling, Markov chain Monte Carlo methods, Ratio of normalizing constants, Resampling, Sequential Monte Carlo methods, Simulated annealing},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/PIA46L73/Moral et al. - 2006 - Sequential Monte Carlo samplers.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/WWQA7S2P/j.1467-9868.2006.00553.html:text/html}
}

@article{doucet_efficient_2006,
	title = {Efficient Block Sampling Strategies for Sequential Monte Carlo Methods},
	volume = {15},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186006X142744},
	doi = {10.1198/106186006X142744},
	abstract = {Sequential Monte Carlo ({SMC}) methods are a powerful set of simulation-based techniques for sampling sequentially from a sequence of complex probability distributions. These methods rely on a combination of importance sampling and resampling techniques. In a Markov chain Monte Carlo ({MCMC}) framework, block sampling strategies often perform much better than algorithms based on one-at-a-time sampling strategies if “good” proposal distributions to update blocks of variables can be designed. In an {SMC} framework, standard algorithms sequentially sample the variables one at a time whereas, like {MCMC}, the efficiency of algorithms could be improved significantly by using block sampling strategies. Unfortunately, a direct implementation of such strategies is impossible as it requires the knowledge of integrals which do not admit closed-form expressions. This article introduces a new methodology which by-passes this problem and is a natural extension of standard {SMC} methods. Applications to several sequential Bayesian inference problems demonstrate these methods.},
	pages = {693--711},
	number = {3},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Doucet, Arnaud and Briers, Mark and Sénécal, Stéphane},
	urldate = {2019-10-16},
	date = {2006-09-01},
	keywords = {Importance sampling, Block sequential Monte Carlo, Markov chain Monte Carlo, Optimal filtering, Particle filtering, State-space models},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/PEXB9KZN/Doucet et al. - 2006 - Efficient Block Sampling Strategies for Sequential.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/2JZWWDRB/106186006X142744.html:text/html}
}

@inproceedings{vahdat_mobile_2007,
	title = {Mobile robot global localization using differential evolution and particle swarm optimization},
	doi = {10.1109/CEC.2007.4424654},
	abstract = {For a mobile robot to move in a known environment and operate successfully, first it needs to robustly determine its initial position and orientation relative to the map, and then update its position while moving in the environment. Thus determining robot's position is one of the most important tasks in mobile robotics. This task consists of "global localization" and "robot's pose tracking". In this paper two recent sample-based evolutionary methods for globally localizing the position of a mobile robot are proposed. The first method is a modified version of genetic algorithm called Differential Evolution ({DE}) which is based on natural selection. The second one is Particle Swarm Optimization ({PSO}) which is based on bird flocking. {DE} evaluates initial population using the probabilistic motion and observation models and the evolution of the individuals is performed by evolutionary operators. {PSO} adjusts the velocity and location of particles towards target (robot's pose) through a problem space on the basis of information about each particle's previous best location and the best previous location of its neighbors. Our results illustrate the excellence of these two methods over standard Monte Carlo localization algorithm with regard to convergence rate, speed and computational cost.},
	eventtitle = {2007 {IEEE} Congress on Evolutionary Computation},
	pages = {1527--1534},
	booktitle = {2007 {IEEE} Congress on Evolutionary Computation},
	author = {Vahdat, Ali R. and Naser {NourAshrafoddin} and Saeed Shiry Ghidary},
	date = {2007-09},
	note = {{ISSN}: 1089-778X, 1941-0026},
	keywords = {bird flocking, Birds, Computational efficiency, Convergence, differential evolution, genetic algorithm, genetic algorithms, Genetic algorithms, global localization, Global localization, mobile robot, mobile robots, Mobile robots, Monte Carlo methods, observation models, Orbital robotics, particle swarm optimisation, particle swarm optimization, Particle swarm optimization, Performance evaluation, pose tracking, probabilistic motion, Robustness, sample-based evolutionary methods, standard Monte Carlo localization algorithm},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/J35YLHVG/4424654.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/JHCCYLS9/Vahdat et al. - 2007 - Mobile robot global localization using differentia.pdf:application/pdf}
}

@article{kanellakis_survey_2017,
	title = {Survey on Computer Vision for {UAVs}: Current Developments and Trends},
	volume = {87},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-017-0483-z},
	doi = {10.1007/s10846-017-0483-z},
	shorttitle = {Survey on Computer Vision for {UAVs}},
	abstract = {During last decade the scientific research on Unmanned Aerial Vehicless ({UAVs}) increased spectacularly and led to the design of multiple types of aerial platforms. The major challenge today is the development of autonomously operating aerial agents capable of completing missions independently of human interaction. To this extent, visual sensing techniques have been integrated in the control pipeline of the {UAVs} in order to enhance their navigation and guidance skills. The aim of this article is to present a comprehensive literature review on vision based applications for {UAVs} focusing mainly on current developments and trends. These applications are sorted in different categories according to the research topics among various research groups. More specifically vision based position-attitude control, pose estimation and mapping, obstacle detection as well as target tracking are the identified components towards autonomous agents. Aerial platforms could reach greater level of autonomy by integrating all these technologies onboard. Additionally, throughout this article the concept of fusion multiple sensors is highlighted, while an overview on the challenges addressed and future trends in autonomous agent development will be also provided.},
	pages = {141--168},
	number = {1},
	journaltitle = {Journal of Intelligent \& Robotic Systems},
	shortjournal = {J Intell Robot Syst},
	author = {Kanellakis, Christoforos and Nikolakopoulos, George},
	urldate = {2019-09-04},
	date = {2017-07-01},
	langid = {english},
	keywords = {Obstacle avoidance, {SLAM}, Target tracking, {UAVs}, Visual servoing},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/WZN34CXX/Kanellakis and Nikolakopoulos - 2017 - Survey on Computer Vision for UAVs Current Develo.pdf:application/pdf}
}

@article{radovic_object_2017,
	title = {Object Recognition in Aerial Images Using Convolutional Neural Networks},
	volume = {3},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2313-433X/3/2/21},
	doi = {10.3390/jimaging3020021},
	abstract = {There are numerous applications of unmanned aerial vehicles ({UAVs}) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As {UAV} applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks ({CNNs}) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of {CNNs} depend on the network’s training and the selection of operational parameters. This paper details the {CNN} training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a {CNN} can detect and classify objects with a high level of accuracy (97.5\%) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “{YOLO}” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by {UAVs} in real-time.},
	pages = {21},
	number = {2},
	journaltitle = {Journal of Imaging},
	author = {Radovic, Matija and Adarkwa, Offei and Wang, Qiaosong},
	urldate = {2019-09-04},
	date = {2017-06},
	langid = {english},
	keywords = {convolutional neural networks, object recognition and detection, Unmanned Aerial Vehicle ({UAV})},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/3LWMJ4UP/Radovic et al. - 2017 - Object Recognition in Aerial Images Using Convolut.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/5S6JLJAQ/htm.html:text/html}
}

@inproceedings{hudjakov_aerial_2009,
	location = {Istanbul, Turkey},
	title = {Aerial imagery terrain classification for long-range autonomous navigation},
	isbn = {978-1-4244-4209-6},
	url = {http://ieeexplore.ieee.org/document/5326104/},
	doi = {10.1109/ISOT.2009.5326104},
	abstract = {This article focuses on the problem of terrain classification from aerial imagery with the intention to increase Unmanned Ground Vehicle ({UGV}) road and off-road performance by providing means to analyze data from Unmanned Aerial Vehicle ({UAV}).},
	eventtitle = {2009 International Symposium on Optomechatronic Technologies ({ISOT} 2009)},
	pages = {88--91},
	booktitle = {2009 International Symposium on Optomechatronic Technologies},
	publisher = {{IEEE}},
	author = {Hudjakov, Robert and Tamre, Mart},
	urldate = {2019-09-04},
	date = {2009-09},
	langid = {english},
	file = {Hudjakov and Tamre - 2009 - Aerial imagery terrain classification for long-ran.pdf:/home/danielsan/Downloads/Hudjakov and Tamre - 2009 - Aerial imagery terrain classification for long-ran.pdf:application/pdf}
}

@article{prorok_reciprocal_nodate,
	title = {A Reciprocal Sampling Algorithm for Lightweight Distributed Multi-Robot Localization},
	abstract = {This work is situated in the context of collaboratively solving the localization problem for unknown initial conditions. We address this problem with a novel, fully decentralized, real-time particle ﬁlter algorithm, designed to accommodate realistic robotic assumptions including noisy sensors, and asynchronous and lossy communication. In particular, we introduce a collaborative reciprocal sampling algorithm which allows a drastic reduction in the number of particles needed to achieve localization. We elaborate an analysis of our reciprocal sampling method and support our conclusions with simulation results. Finally, we validate our approach on a team of four real robots within a controlled experimental setup.},
	pages = {7},
	author = {Prorok, Amanda and Martinoli, Alcherio},
	langid = {english},
	file = {Prorok and Martinoli - A Reciprocal Sampling Algorithm for Lightweight Di.pdf:/home/danielsan/Downloads/Prorok and Martinoli - A Reciprocal Sampling Algorithm for Lightweight Di.pdf:application/pdf}
}

@incollection{ruiz-del-solar_cooperative_2011,
	location = {Berlin, Heidelberg},
	title = {Cooperative Localization Based on Visually Shared Objects},
	volume = {6556},
	isbn = {978-3-642-20216-2 978-3-642-20217-9},
	url = {http://link.springer.com/10.1007/978-3-642-20217-9_30},
	abstract = {In this paper we describe a cooperative localization algorithm based on a modiﬁcation of the Monte Carlo Localization algorithm where, when a robot detects it is lost, particles are spread not uniformly in the state space, but rather according to the information on the location of an object whose distance and bearing is measured by the lost robot. The object location is provided by other robots of the same team using explicit (wireless) communication. Results of application of the method to a team of real robots are presented.},
	pages = {350--361},
	booktitle = {{RoboCup} 2010: Robot Soccer World Cup {XIV}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lima, Pedro U. and Santos, Pedro and Oliveira, Ricardo and Ahmad, Aamir and Santos, João},
	editor = {Ruiz-del-Solar, Javier and Chown, Eric and Plöger, Paul G.},
	urldate = {2019-09-04},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-3-642-20217-9_30},
	file = {Lima et al. - 2011 - Cooperative Localization Based on Visually Shared .pdf:/home/danielsan/Downloads/Lima et al. - 2011 - Cooperative Localization Based on Visually Shared .pdf:application/pdf}
}

@inproceedings{wang_dynamic_2010,
	location = {Tianjin, China},
	title = {A dynamic size {MCL} algorithm for mobile robot localization},
	isbn = {978-1-4244-9319-7},
	url = {http://ieeexplore.ieee.org/document/5723426/},
	doi = {10.1109/ROBIO.2010.5723426},
	abstract = {Mobile robot localization is a very important problem in robotics as most robot’s tasks need the positional information. Monte Carlo Localization({MCL}) is one of the most popular and efﬁcient localization algorithms for mobile robot localization. {MCL} algorithm represents a robot’s pose by a set of weighted particles. In order to further improve the performance of {MCL}, many extensions have been proposed. In this paper, we proposed an algorithm called dynamic size {MCL}, an extension of {MCL}. We incorporate the clustering approach into traditional {MCL}. With the help of clustering information, our algorithm could reduce the number of particles during the process of localization, which lower the computational cost. Experimental results demonstrate the effectiveness of the proposed method.},
	eventtitle = {2010 {IEEE} International Conference on Robotics and Biomimetics ({ROBIO})},
	pages = {785--790},
	booktitle = {2010 {IEEE} International Conference on Robotics and Biomimetics},
	publisher = {{IEEE}},
	author = {Wang, Yuefeng and Wu, Dan and Wu, Libing},
	urldate = {2019-09-04},
	date = {2010-12},
	langid = {english},
	file = {Wang et al. - 2010 - A dynamic size MCL algorithm for mobile robot loca.pdf:/home/danielsan/Downloads/Wang et al. - 2010 - A dynamic size MCL algorithm for mobile robot loca.pdf:application/pdf}
}

@inproceedings{min_active_2009,
	location = {Zhuhai/Macau, China},
	title = {Active particle in {MCL}: An evolutionary view},
	isbn = {978-1-4244-3607-1},
	url = {http://ieeexplore.ieee.org/document/5205079/},
	doi = {10.1109/ICINFA.2009.5205079},
	shorttitle = {Active particle in {MCL}},
	abstract = {Mobile robot localization is the task of determining a robot's pose in a known environment, which is one of the most important problems in mobile robotics. The state-of-the-art Monte Carlo Localization ({MCL}) algorithm requires a large amount of particles and thus converges slowly. Also, {MCL} performs poorly in low noise sensor input.},
	eventtitle = {2009 International Conference on Information and Automation ({ICIA})},
	pages = {1087--1092},
	booktitle = {2009 International Conference on Information and Automation},
	publisher = {{IEEE}},
	author = {Min, Hua-Qing and Chen, Huan and Luo, Rong-Hua},
	urldate = {2019-09-04},
	date = {2009-06},
	langid = {english},
	file = {Min et al. - 2009 - Active particle in MCL An evolutionary view.pdf:/home/danielsan/Downloads/Min et al. - 2009 - Active particle in MCL An evolutionary view.pdf:application/pdf}
}

@article{bruederle_establishing_2009,
	title = {Establishing a Novel Modeling Tool: A Python-based Interface for a Neuromorphic Hardware System},
	volume = {3},
	issn = {16625196},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.11.017.2009/abstract},
	doi = {10.3389/neuro.11.017.2009},
	shorttitle = {Establishing a Novel Modeling Tool},
	abstract = {Neuromorphic hardware systems provide new possibilities for the neuroscience modeling community. Due to the intrinsic parallelism of the micro-electronic emulation of neural computation, such models are highly scalable without a loss of speed. However, the communities of software simulator users and neuromorphic engineering in neuroscience are rather disjoint. We present a software concept that provides the possibility to establish such hardware devices as valuable modeling tools. It is based on the integration of the hardware interface into a simulator-independent language which allows for uniﬁed experiment descriptions that can be run on various simulation platforms without modiﬁcation, implying experiment portability and a huge simpliﬁcation of the quantitative comparison of hardware and simulator results. We introduce an accelerated neuromorphic hardware device and describe the implementation of the proposed concept for this system. An example setup and results acquired by utilizing both the hardware system and a software simulator are demonstrated.},
	journaltitle = {Frontiers in Neuroinformatics},
	shortjournal = {Front. Neuroinform.},
	author = {Bruederle, Daniel},
	urldate = {2019-08-29},
	date = {2009},
	langid = {english},
	file = {Bruederle - 2009 - Establishing a Novel Modeling Tool A Python-based.pdf:/home/danielsan/Downloads/Bruederle - 2009 - Establishing a Novel Modeling Tool A Python-based.pdf:application/pdf}
}

@inproceedings{schemmel_wafer-scale_2010,
	location = {Paris, France},
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	isbn = {978-1-4244-5308-5},
	url = {http://ieeexplore.ieee.org/document/5536970/},
	doi = {10.1109/ISCAS.2010.5536970},
	abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project ”{FACETS}” this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a uniﬁed neural system description language, called {PyNN}, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within {FACETS} is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
	eventtitle = {2010 {IEEE} International Symposium on Circuits and Systems - {ISCAS} 2010},
	pages = {1947--1950},
	booktitle = {Proceedings of 2010 {IEEE} International Symposium on Circuits and Systems},
	publisher = {{IEEE}},
	author = {Schemmel, Johannes and Briiderle, Daniel and Griibl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	urldate = {2019-08-29},
	date = {2010-05},
	langid = {english},
	file = {Schemmel et al. - 2010 - A wafer-scale neuromorphic hardware system for lar.pdf:/home/danielsan/Downloads/Schemmel et al. - 2010 - A wafer-scale neuromorphic hardware system for lar.pdf:application/pdf}
}

@article{aamir_accelerated_2018,
	title = {An Accelerated {LIF} Neuronal Network Array for a Large-Scale Mixed-Signal Neuromorphic Architecture},
	volume = {65},
	issn = {1549-8328, 1558-0806},
	url = {https://ieeexplore.ieee.org/document/8398542/},
	doi = {10.1109/TCSI.2018.2840718},
	abstract = {We present an array of leaky integrate-andﬁre ({LIF}) neuron circuits designed for the second-generation {BrainScaleS} mixed-signal 65-nm {CMOS} neuromorphic hardware. The neuronal array is embedded in the analog network core of a scaled-down prototype high input count analog neural network with digital learning system chip. Designed as continuoustime circuits, the neurons are highly tunable and reconﬁgurable elements with accelerated dynamics. Each neuron integrates input current from a multitude of incoming synapses and evokes a digital spike event output. The circuit offers a wide tuning range for synaptic and membrane time constants, as well as for refractory periods to cover a number of computational models. We elucidate our design methodology, underlying circuit design, calibration, and measurement results from individual sub-circuits across multiple dies. The circuit dynamics matches with the behavior of the {LIF} mathematical model. We further demonstrate a winner-take-all network on the prototype chip as a typical element of cortical processing.},
	pages = {4299--4312},
	number = {12},
	journaltitle = {{IEEE} Transactions on Circuits and Systems I: Regular Papers},
	shortjournal = {{IEEE} Trans. Circuits Syst. I},
	author = {Aamir, Syed Ahmed and Stradmann, Yannik and Muller, Paul and Pehle, Christian and Hartel, Andreas and Grubl, Andreas and Schemmel, Johannes and Meier, Karlheinz},
	urldate = {2019-08-29},
	date = {2018-12},
	langid = {english},
	file = {Aamir et al. - 2018 - An Accelerated LIF Neuronal Network Array for a La.pdf:/home/danielsan/Downloads/Aamir et al. - 2018 - An Accelerated LIF Neuronal Network Array for a La.pdf:application/pdf}
}

@article{mantelli_novel_2019,
	title = {A novel measurement model based on {abBRIEF} for global localization of a {UAV} over satellite images},
	volume = {112},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S092188901830438X},
	doi = {10.1016/j.robot.2018.12.006},
	abstract = {This paper presents a method for global localization and tracking of an Unmanned Aerial Vehicle ({UAV}) over satellite images. We propose a new measurement model based on a novel version of {BRIEF} descriptor and apply it in a Monte Carlo Localization system that estimates the {UAV} pose in 4 degrees of freedom. The model is used to compare images obtained from the {UAV} downward looking camera against patches of satellite images such as the ones available on {GoogleTMEarth}. The proposed method was validated using real flights sequences and has yield good results with different maps of the same region spawning many years and covering large areas.},
	pages = {304--319},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Mantelli, Mathias and Pittol, Diego and Neuland, Renata and Ribacki, Arthur and Maffei, Renan and Jorge, Vitor and Prestes, Edson and Kolberg, Mariana},
	urldate = {2019-08-28},
	date = {2019-02},
	langid = {english},
	file = {Mantelli et al. - 2019 - A novel measurement model based on abBRIEF for glo.pdf:/home/danielsan/Downloads/Mantelli et al. - 2019 - A novel measurement model based on abBRIEF for glo.pdf:application/pdf}
}

@incollection{menegatti_localization_2016,
	location = {Cham},
	title = {Localization of Unmanned Aerial Vehicles Using Terrain Classification from Aerial Images},
	volume = {302},
	isbn = {978-3-319-08337-7 978-3-319-08338-4},
	url = {http://link.springer.com/10.1007/978-3-319-08338-4_60},
	abstract = {In this paper we investigate the beneﬁt of terrain classiﬁcation for selflocalization of a ﬂying robot. The key idea is to use aerial images, which are already available from online databases such as {GoogleMaps}™, as reference map and to match images taken with a downward looking camera with this map. Using different terrain classes as features, we can make sure that our method is invariant to lighting/weather changes as well as seasonal variations or minor changes in the environment. A particle ﬁlter is used to register the query image with parts of the map. The proposed method has shown to work on image data from both simulated and real ﬂights.},
	pages = {831--842},
	booktitle = {Intelligent Autonomous Systems 13},
	publisher = {Springer International Publishing},
	author = {Masselli, Andreas and Hanten, Richard and Zell, Andreas},
	editor = {Menegatti, Emanuele and Michael, Nathan and Berns, Karsten and Yamaguchi, Hiroaki},
	urldate = {2019-08-28},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-08338-4_60},
	file = {Masselli et al. - 2016 - Localization of Unmanned Aerial Vehicles Using Ter.pdf:/home/danielsan/Downloads/Masselli et al. - 2016 - Localization of Unmanned Aerial Vehicles Using Ter.pdf:application/pdf}
}

@inproceedings{dellaert_monte_1999,
	location = {Detroit, {MI}, {USA}},
	title = {Monte Carlo localization for mobile robots},
	volume = {2},
	isbn = {978-0-7803-5180-6},
	url = {http://ieeexplore.ieee.org/document/772544/},
	doi = {10.1109/ROBOT.1999.772544},
	abstract = {To navigate reliablyin indoor environments,a mobile robot must know where it is. Thus, reliableposition estimation is a key problem in mobile robotics. We believe that probabilistic approaches are among the most promising candidates to providing a comprehensive and real-time solution to the robot localization problem. Howevel; current methods still face considerable hurdles. In particular; the problems encountered are closely related to the type of representation used to represent probability densities over the robot’s state space. Recent work on Bayesian jiltering with particle-based density representations opens up a new approachfor mobile robot localization, based on these principles. In this paper we introduce the Monte Carlo Localization method, where we represent the probability density involved by maintaining a set of samples that are randomly drawnfrom it. By using a sampling-based representation we obtain a localization method that can represent arbitrary distributions. We show experimentally that the resulting method is able to efficiently localize a mobile robot without knowledge of its starting location. It is faster; more accurate and less memory-intensive than earlier grid-based methods.},
	eventtitle = {International Conference on Robotics and Automation},
	pages = {1322--1328},
	booktitle = {Proceedings 1999 {IEEE} International Conference on Robotics and Automation (Cat. No.99CH36288C)},
	publisher = {{IEEE}},
	author = {Dellaert, F. and Fox, D. and Burgard, W. and Thrun, S.},
	urldate = {2019-08-28},
	date = {1999},
	langid = {english},
	file = {Dellaert et al. - 1999 - Monte Carlo localization for mobile robots.pdf:/home/danielsan/Downloads/Dellaert et al. - 1999 - Monte Carlo localization for mobile robots.pdf:application/pdf}
}

@article{maass_noise_2014,
	title = {Noise as a Resource for Computation and Learning in Networks of Spiking Neurons},
	volume = {102},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2014.2310593},
	abstract = {We are used to viewing noise as a nuisance in computing systems. This is a pity, since noise will be abundantly available in energy-efficient future nanoscale devices and circuits. I propose here to learn from the way the brain deals with noise, and apparently even benefits from it. Recent theoretical results have provided insight into how this can be achieved: how noise enables networks of spiking neurons to carry out probabilistic inference through sampling and also enables creative problem solving. In addition, noise supports the self-organization of networks of spiking neurons, and learning from rewards. I will sketch here the main ideas and some consequences of these results. I will also describe why these results are paving the way for a qualitative jump in the computational capability and learning performance of neuromorphic networks of spiking neurons with noise, and for other future computing systems that are able to treat noise as a resource.},
	pages = {860--880},
	number = {5},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Maass, W.},
	date = {2014-05},
	keywords = {brain, computational capability, Computational power, Computer architecture, computing systems, creative problem solving, energy-efficient device, learning performance, Markov processes, medical computing, nanoscale circuits, nanoscale devices, neural nets, neural networks, Neural networks, neuromorphic hardware, neuromorphic networks, Neurons, neurophysiology, Neuroscience, noise, Noise measurement, nuisance, probabilistic inference, Probabilistic logic, probability, qualitative jump, resource, sampling, self-organization, Self-organizing networks, spiking neuron networks, spiking neurons, stochastic computing, Stochastic processes},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/N8SLKGGE/6797856.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/XKGAVW5G/Maass - 2014 - Noise as a Resource for Computation and Learning i.pdf:application/pdf}
}

@inproceedings{zoschke_full_2017,
	title = {Full wafer redistribution and wafer embedding as key technologies for a multi-scale neuromorphic hardware cluster},
	doi = {10.1109/EPTC.2017.8277579},
	abstract = {Together with the Kirchhoff-Institute for Physics the Fraunhofer {IZM} has developed a full wafer redistribution and embedding technology as base for a large-scale neuromorphic hardware system. The paper will give an overview of the neuromorphic computing platform at the Kirchhoff-Institute for Physics and the associated hardware requirements which drove the described technological developments. In the first phase of the project standard redistribution technologies from wafer level packaging were adapted to enable a high density reticle-to-reticle routing on 200 mm {CMOS} wafers. Neighboring reticles were interconnected across the scribe lines with an 8 μm pitch routing based on semi-additive copper metallization which was photo defined by full field mask aligning equipment. Passivation by photo sensitive benzocyclobutene ({BCB}) was used to enable a second intra-reticle routing layer. Final {IO} pads of nickel with flash gold were generated on top of each reticle. For final electrical connection the wafers were placed into mechanical fixtures and the {IOs} of all reticles were touched by elastomeric connectors. With that concept neuromorphic systems based on full wafers could be assembled and tested. The fabricated high density inter-reticle routing revealed a very high yield of larger than 99.9 \%. In order to allow an upscaling of the system size to a large number of wafers with feasible effort a full wafer embedding concept for printed circuit boards was developed and proven in the second phase of the project. The wafers were thinned to 250 μm and laminated with additional prepreg layers and copper foils into a core material. A 200 mm circular cut was done into the core material and the inner prepreg layers to create the required clearance for the wafer. After lamination of the {PCB} panel the reticle {IOs} of the embedded wafer were accessed by micro via drilling, copper electroplating, lithography and subtractive etching of the {PCB} wiring structure. The created wiring with 50 μm line width enabled an access of the reticle {IOs} on the embedded wafer as well as a board level routing. The panels with the embedded wafers were subsequently stressed with up to 1000 thermal cycles between 0 °C and 100 °C and have shown no severe failure formation over the cycle time.},
	eventtitle = {2017 {IEEE} 19th Electronics Packaging Technology Conference ({EPTC})},
	pages = {1--8},
	booktitle = {2017 {IEEE} 19th Electronics Packaging Technology Conference ({EPTC})},
	author = {Zoschke, K. and Güttler, M. and Böttcher, L. and Grübl, A. and Husmann, D. and Schemmel, J. and Meier, K. and Ehrmann, O.},
	date = {2017-12},
	keywords = {Neurons, associated hardware requirements, {CMOS} digital integrated circuits, {CMOS} wafers, concept neuromorphic systems, copper, Copper, described technological developments, electric connectors, electroplating, embedding technology, etching, fabricated high density inter-reticle routing, Field programmable gate arrays, Fraunhofer {IZM}, gold, integrated circuit interconnections, integrated circuit reliability, interconnections, intra-reticle routing layer, Kirchhoff-Institute for Physics, large-scale neuromorphic hardware system, lithography, masks, multiscale neuromorphic hardware cluster, neighboring reticles, network routing, neural chips, neuromorphic computing platform, Polymers, printed circuit manufacture, printed circuits, project standard redistribution technologies, reticle {IO}, reticle-to-reticle routing, Routing, size 200.0 mm, size 250.0 mum, size 50.0 mum, size 8.0 mum, sputter etching, Standards, temperature 0.0 {degC} to 100.0 {degC}, wafer embedding, wafer level packaging, wafer redistribution, Wiring},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/G4CSE3N5/8277579.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/GBWJ8YUM/Zoschke et al. - 2017 - Full wafer redistribution and wafer embedding as k.pdf:application/pdf}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	rights = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	pages = {529--533},
	number = {7540},
	journaltitle = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	urldate = {2019-08-21},
	date = {2015-02},
	langid = {english},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/HWRA5KJY/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/NRJ4B77E/nature14236.html:text/html}
}

@article{friedmann_demonstrating_2017,
	title = {Demonstrating Hybrid Learning in a Flexible Neuromorphic Hardware System},
	volume = {11},
	issn = {1932-4545},
	doi = {10.1109/TBCAS.2016.2579164},
	abstract = {We present results from a new approach to learning and plasticity in neuromorphic hardware systems: to enable flexibility in implementable learning mechanisms while keeping high efficiency associated with neuromorphic implementations, we combine a general-purpose processor with full-custom analog elements. This processor is operating in parallel with a fully parallel neuromorphic system consisting of an array of synapses connected to analog, continuous time neuron circuits. Novel analog correlation sensor circuits process spike events for each synapse in parallel and in real-time. The processor uses this pre-processing to compute new weights possibly using additional information following its program. Therefore, to a certain extent, learning rules can be defined in software giving a large degree of flexibility. Synapses realize correlation detection geared towards Spike-Timing Dependent Plasticity ({STDP}) as central computational primitive in the analog domain. Operating at a speed-up factor of 1000 compared to biological time-scale, we measure time-constants from tens to hundreds of micro-seconds. We analyze variability across multiple chips and demonstrate learning using a multiplicative {STDP} rule. We conclude that the presented approach will enable flexible and efficient learning as a platform for neuroscientific research and technological applications.},
	pages = {128--142},
	number = {1},
	journaltitle = {{IEEE} Transactions on Biomedical Circuits and Systems},
	author = {Friedmann, S. and Schemmel, J. and Grübl, A. and Hartel, A. and Hock, M. and Meier, K.},
	date = {2017-02},
	keywords = {neuromorphic hardware, Neurons, neurophysiology, analog continuous time neuron circuits, analog correlation sensor circuits, bioelectric phenomena, Biological system modeling, Correlation, Digital signal processing, flexible neuromorphic hardware system, full-custom analog elements, Hardware, hybrid learning, learning, learning (artificial intelligence), Machine Learning, Mathematical model, medical signal processing, Models, Neurological, Neural Networks (Computer), neuromorphic implementations, Neuromorphics, Neuronal Plasticity, neuroscientific research, spike events, spike-time dependent plasticity, spike-timing dependent plasticity, {STDP}, synapse circuit, synapses, Synapses},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/I9K243BX/7563782.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/SWMETM8I/Friedmann et al. - 2017 - Demonstrating Hybrid Learning in a Flexible Neurom.pdf:application/pdf}
}

@online{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
	titleaddon = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	urldate = {2019-08-21},
	date = {1952-08-28},
	langid = {english},
	doi = {10.1113/jphysiol.1952.sp004764},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/I5QP6N5X/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/X78KL5GV/jphysiol.1952.html:text/html}
}

@article{scholze_32_2012,
	title = {A 32 {GBit}/s communication {SoC} for a waferscale neuromorphic system},
	volume = {45},
	issn = {01679260},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167926011000538},
	doi = {10.1016/j.vlsi.2011.05.003},
	abstract = {State-of-the-art large-scale neuromorphic systems require a sophisticated, high-bandwidth communication infrastructure for the exchange of spike events between units of the neural network. These communication infrastructures are usually built around custom-designed {FPGA} systems. However, the overall bandwidth requirements and the integration density of very large neuromorphic systems necessitate a signiﬁcantly more targeted approach, i.e. the development of dedicated integrated circuits. We present a {VLSI} realization of a neuromorphic communication system-on-chip ({SoC}) with a cumulative throughput of 32 {GBit}/s in 0:18 mm {CMOS}, employing state-of-the-art circuit blocks. Several of these circuits exhibit improved performance compared to current literature, e.g. a priority queue with a speed of 31 Mkeys/s at 1.3 {mW}, or a 1 {GHz} {PLL} at 5 {mW}. The {SoC} contains additional neuromorphic functionality, such as conﬁgurable event delays and event ordering. The complete conﬁguration of the neuromorphic system is also handled by the spike communication channels, in contrast to the separate channels required in the majority of current systems. At 865 Mevent/s, the {SoC} delivers at least a factor of eight more bandwidth than other current neuromorphic communication infrastructures.},
	pages = {61--75},
	number = {1},
	journaltitle = {Integration},
	shortjournal = {Integration},
	author = {Scholze, Stefan and Eisenreich, Holger and Höppner, Sebastian and Ellguth, Georg and Henker, Stephan and Ander, Mario and Hänzsche, Stefan and Partzsch, Johannes and Mayr, Christian and Schüffny, René},
	urldate = {2019-08-21},
	date = {2012-01},
	langid = {english},
	file = {Scholze et al. - 2012 - A 32 GBits communication SoC for a waferscale neu.pdf:/home/danielsan/Downloads/Scholze et al. - 2012 - A 32 GBits communication SoC for a waferscale neu.pdf:application/pdf}
}

@article{sun_organic_2018,
	title = {Organic synaptic devices for neuromorphic systems},
	volume = {51},
	issn = {0022-3727, 1361-6463},
	url = {http://stacks.iop.org/0022-3727/51/i=31/a=314004?key=crossref.b4e835c96a6ecbe4766f89b7016c8b56},
	doi = {10.1088/1361-6463/aacd99},
	abstract = {The development of synaptic devices with biologically-inspired information processing functions and low power consumption is critically important for the hardware implementation of highly anticipated brain-like computing systems. Organic materials are regarded as the most promising candidates for synaptic devices and bio-electronics due to several advantages such as low cost, easy processability, mechanical flexibility and ductility. In this review, a description of the current advances in organic synaptic devices, including two-terminal memristors and three-terminal transistors, is provided. Organic two-terminal memristors with the characteristics of non-volatility and reasonable on/off switching ratio are reported to be popular synaptic devices. On the other hand, organic memristive and electrochemical electric-double-layer transistors can accurately select working devices by applying a gate spike to the corresponding gate electrode. Therefore, these three-terminal organic devices provide an alternative approach to the development of neuromorphic systems. Lastly, the novel applications of organic synaptic devices are discussed, and some current challenges are presented.},
	pages = {314004},
	number = {31},
	journaltitle = {Journal of Physics D: Applied Physics},
	shortjournal = {J. Phys. D: Appl. Phys.},
	author = {Sun, Jia and Fu, Ying and Wan, Qing},
	urldate = {2019-08-21},
	date = {2018-08-08},
	langid = {english},
	file = {Sun et al. - 2018 - Organic synaptic devices for neuromorphic systems.pdf:/home/danielsan/Downloads/Sun et al. - 2018 - Organic synaptic devices for neuromorphic systems.pdf:application/pdf}
}

@article{indiveri_integration_2013,
	title = {Integration of nanoscale memristor synapses in neuromorphic computing architectures},
	volume = {24},
	issn = {0957-4484, 1361-6528},
	url = {http://stacks.iop.org/0957-4484/24/i=38/a=384010?key=crossref.021ddc6521275e62cd80c69b1b841f4f},
	doi = {10.1088/0957-4484/24/38/384010},
	abstract = {Conventional neuro-computing architectures and artiﬁcial neural networks have often been developed with no or loose connections to neuroscience. As a consequence, they have largely ignored key features of biological neural processing systems, such as their extremely low-power consumption features or their ability to carry out robust and efﬁcient computation using massively parallel arrays of limited precision, highly variable, and unreliable components. Recent developments in nano-technologies are making available extremely compact and low power, but also variable and unreliable solid-state devices that can potentially extend the offerings of availing {CMOS} technologies. In particular, memristors are regarded as a promising solution for modeling key features of biological synapses due to their nanoscale dimensions, their capacity to store multiple bits of information per element and the low energy required to write distinct states. In this paper, we ﬁrst review the neuro- and neuromorphic computing approaches that can best exploit the properties of memristor and scale devices, and then propose a novel hybrid memristor-{CMOS} neuromorphic circuit which represents a radical departure from conventional neuro-computing approaches, as it uses memristors to directly emulate the biophysics and temporal dynamics of real synapses. We point out the differences between the use of memristors in conventional neuro-computing architectures and the hybrid memristor-{CMOS} circuit proposed, and argue how this circuit represents an ideal building block for implementing brain-inspired probabilistic computing paradigms that are robust to variability and fault tolerant by design.},
	pages = {384010},
	number = {38},
	journaltitle = {Nanotechnology},
	shortjournal = {Nanotechnology},
	author = {Indiveri, Giacomo and Linares-Barranco, Bernabé and Legenstein, Robert and Deligeorgis, George and Prodromakis, Themistoklis},
	urldate = {2019-08-21},
	date = {2013-09-27},
	langid = {english},
	file = {Indiveri et al. - 2013 - Integration of nanoscale memristor synapses in neu.pdf:/home/danielsan/Downloads/Indiveri et al. - 2013 - Integration of nanoscale memristor synapses in neu.pdf:application/pdf}
}

@article{broccard_neuromorphic_2017,
	title = {Neuromorphic neural interfaces: from neurophysiological inspiration to biohybrid coupling with nervous systems},
	volume = {14},
	issn = {1741-2560, 1741-2552},
	url = {http://stacks.iop.org/1741-2552/14/i=4/a=041002?key=crossref.d8833bdd885d2d1769d7475fb038dcc1},
	doi = {10.1088/1741-2552/aa67a9},
	shorttitle = {Neuromorphic neural interfaces},
	abstract = {Objective. Computation in nervous systems operates with different computational primitives, and on different hardware, than traditional digital computation and is thus subjected to different constraints from its digital counterpart regarding the use of physical resources such as time, space and energy. In an effort to better understand neural computation on a physical medium with similar spatiotemporal and energetic constraints, the field of neuromorphic engineering aims to design and implement electronic systems that emulate in very large-scale integration ({VLSI}) hardware the organization and functions of neural systems at multiple levels of biological organization, from individual neurons up to large circuits and networks. Mixed analog/digital neuromorphic {VLSI} systems are compact, consume little power and operate in real time independently of the size and complexity of the model. Approach. This article highlights the current efforts to interface neuromorphic systems with neural systems at multiple levels of biological organization, from the synaptic to the system level, and discusses the prospects for future biohybrid systems with neuromorphic circuits of greater complexity. Main results. Single silicon neurons have been interfaced successfully with invertebrate and vertebrate neural networks. This approach allowed the investigation of neural properties that are inaccessible with traditional techniques while providing a realistic biological context not achievable with traditional numerical modeling methods. At the network level, populations of neurons are envisioned to communicate bidirectionally with neuromorphic processors of hundreds or thousands of silicon neurons. Recent work on brain–machine interfaces suggests that this is feasible with current neuromorphic technology. Significance. Biohybrid interfaces between biological neurons and {VLSI} neuromorphic systems of varying complexity have started to emerge in the literature. Primarily intended as a computational tool for investigating fundamental questions related to neural dynamics, the sophistication of current neuromorphic systems now allows direct interfaces with large neuronal networks and circuits, resulting in potentially interesting clinical applications for neuroengineering systems, neuroprosthetics and neurorehabilitation.},
	pages = {041002},
	number = {4},
	journaltitle = {Journal of Neural Engineering},
	shortjournal = {J. Neural Eng.},
	author = {Broccard, Frédéric D and Joshi, Siddharth and Wang, Jun and Cauwenberghs, Gert},
	urldate = {2019-08-21},
	date = {2017-08-01},
	langid = {english},
	file = {Broccard et al. - 2017 - Neuromorphic neural interfaces from neurophysiolo.pdf:/home/danielsan/Downloads/Broccard et al. - 2017 - Neuromorphic neural interfaces from neurophysiolo.pdf:application/pdf}
}

@article{furber_large-scale_2016,
	title = {Large-scale neuromorphic computing systems},
	volume = {13},
	issn = {1741-2560, 1741-2552},
	url = {http://stacks.iop.org/1741-2552/13/i=5/a=051001?key=crossref.c170009b6af4be582bc1fbfbf3ccad04},
	doi = {10.1088/1741-2560/13/5/051001},
	abstract = {Neuromorphic computing covers a diverse range of approaches to information processing all of which demonstrate some degree of neurobiological inspiration that differentiates them from mainstream conventional computing systems. The philosophy behind neuromorphic computing has its origins in the seminal work carried out by Carver Mead at Caltech in the late 1980s. This early work inﬂuenced others to carry developments forward, and advances in {VLSI} technology supported steady growth in the scale and capability of neuromorphic devices. Recently, a number of large-scale neuromorphic projects have emerged, taking the approach to unprecedented scales and capabilities. These large-scale projects are associated with major new funding initiatives for brain-related research, creating a sense that the time and circumstances are right for progress in our understanding of information processing in the brain. In this review we present a brief history of neuromorphic engineering then focus on some of the principal current large-scale projects, their main features, how their approaches are complementary and distinct, their advantages and drawbacks, and highlight the sorts of capabilities that each can deliver to neural modellers.},
	pages = {051001},
	number = {5},
	journaltitle = {Journal of Neural Engineering},
	shortjournal = {J. Neural Eng.},
	author = {Furber, Steve},
	urldate = {2019-08-21},
	date = {2016-10-01},
	langid = {english},
	file = {Furber - 2016 - Large-scale neuromorphic computing systems.pdf:/home/danielsan/Downloads/Furber - 2016 - Large-scale neuromorphic computing systems.pdf:application/pdf}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2019-08-21},
	date = {2015-05},
	langid = {english},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/danielsan/Downloads/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}

@article{albantakis_evolution_2014,
	title = {Evolution of Integrated Causal Structures in Animats Exposed to Environments of Increasing Complexity},
	volume = {10},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003966},
	doi = {10.1371/journal.pcbi.1003966},
	abstract = {Natural selection favors the evolution of brains that can capture fitness-relevant features of the environment’s causal structure. We investigated the evolution of small, adaptive logic-gate networks (‘‘animats’’) in task environments where falling blocks of different sizes have to be caught or avoided in a ‘Tetris-like’ game. Solving these tasks requires the integration of sensor inputs and memory. Evolved networks were evaluated using measures of information integration, including the number of evolved concepts and the total amount of integrated conceptual information. The results show that, over the course of the animats’ adaptation, i) the number of concepts grows; ii) integrated conceptual information increases; iii) this increase depends on the complexity of the environment, especially on the requirement for sequential memory. These results suggest that the need to capture the causal structure of a rich environment, given limited sensors and internal mechanisms, is an important driving force for organisms to develop highly integrated networks (‘‘brains’’) with many concepts, leading to an increase in their internal complexity.},
	pages = {e1003966},
	number = {12},
	journaltitle = {{PLoS} Computational Biology},
	author = {Albantakis, Larissa and Hintze, Arend and Koch, Christof and Adami, Christoph and Tononi, Giulio},
	editor = {Polani, Daniel},
	urldate = {2019-04-04},
	date = {2014-12-18},
	langid = {english},
	file = {Albantakis et al. - 2014 - Evolution of Integrated Causal Structures in Anima.PDF:/home/danielsan/Zotero/storage/LR3TAQUN/Albantakis et al. - 2014 - Evolution of Integrated Causal Structures in Anima.PDF:application/pdf}
}

@inproceedings{schmitt_neuromorphic_2017,
	location = {Anchorage, {AK}},
	title = {Neuromorphic hardware in the loop: Training a deep spiking network on the {BrainScaleS} wafer-scale system},
	isbn = {978-1-5090-6182-2},
	url = {http://ieeexplore.ieee.org/document/7966125/},
	doi = {10.1109/IJCNN.2017.7966125},
	shorttitle = {Neuromorphic hardware in the loop},
	abstract = {Emulating spiking neural networks on analog neuromorphic hardware offers several advantages over simulating them on conventional computers, particularly in terms of speed and energy consumption. However, this usually comes at the cost of reduced control over the dynamics of the emulated networks. In this paper, we demonstrate how iterative training of a hardware-emulated network can compensate for anomalies induced by the analog substrate. We ﬁrst convert a deep neural network trained in software to a spiking network on the {BrainScaleS} wafer-scale neuromorphic system, thereby enabling an acceleration factor of 10 000 compared to the biological time domain. This mapping is followed by the in-the-loop training, where in each training step, the network activity is ﬁrst recorded in hardware and then used to compute the parameter updates in software via backpropagation. An essential ﬁnding is that the parameter updates do not have to be precise, but only need to approximately follow the correct gradient, which simpliﬁes the computation of updates. Using this approach, after only several tens of iterations, the spiking network shows an accuracy close to the ideal software-emulated prototype. The presented techniques show that deep spiking networks emulated on analog neuromorphic devices can attain good computational performance despite the inherent variations of the analog substrate.},
	eventtitle = {2017 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {2227--2234},
	booktitle = {2017 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Schmitt, Sebastian and Klahn, Johann and Bellec, Guillaume and Grubl, Andreas and Guttler, Maurice and Hartel, Andreas and Hartmann, Stephan and Husmann, Dan and Husmann, Kai and Jeltsch, Sebastian and Karasenko, Vitali and Kleider, Mitja and Koke, Christoph and Kononov, Alexander and Mauch, Christian and Muller, Eric and Muller, Paul and Partzsch, Johannes and Petrovici, Mihai A. and Schiefer, Stefan and Scholze, Stefan and Thanasoulis, Vasilis and Vogginger, Bernhard and Legenstein, Robert and Maass, Wolfgang and Mayr, Christian and Schuffny, Rene and Schemmel, Johannes and Meier, Karlheinz},
	urldate = {2019-04-04},
	date = {2017-05},
	langid = {english},
	file = {Schmitt et al. - 2017 - Neuromorphic hardware in the loop Training a deep.pdf:/home/danielsan/Zotero/storage/HRP5M6SY/Schmitt et al. - 2017 - Neuromorphic hardware in the loop Training a deep.pdf:application/pdf}
}

@book{petrovici_form_2016,
	location = {Cham},
	title = {Form Versus Function: Theory and Models for Neuronal Substrates},
	isbn = {978-3-319-39551-7 978-3-319-39552-4},
	url = {http://link.springer.com/10.1007/978-3-319-39552-4},
	series = {Springer Theses},
	shorttitle = {Form Versus Function},
	publisher = {Springer International Publishing},
	author = {Petrovici, Mihai Alexandru},
	urldate = {2019-04-04},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-39552-4},
	file = {Petrovici - 2016 - Form Versus Function Theory and Models for Neuron.pdf:/home/danielsan/Zotero/storage/L7VDR2RY/Petrovici - 2016 - Form Versus Function Theory and Models for Neuron.pdf:application/pdf}
}

@article{wunderlich_demonstrating_2019,
	title = {Demonstrating Advantages of Neuromorphic Computation: A Pilot Study},
	volume = {13},
	issn = {1662-453X},
	url = {http://arxiv.org/abs/1811.03618},
	doi = {10.3389/fnins.2019.00260},
	shorttitle = {Demonstrating Advantages of Neuromorphic Computation},
	abstract = {Neuromorphic devices represent an attempt to mimic aspects of the brain’s architecture and dynamics with the aim of replicating its hallmark functional capabilities in terms of computational power, robust learning and energy efﬁciency. We employ a single-chip prototype of the {BrainScaleS} 2 neuromorphic system to implement a proof-of-concept demonstration of reward-modulated spike-timing-dependent plasticity in a spiking network that learns to play a simpliﬁed version of the Pong video game by smooth pursuit. This system combines an electronic mixed-signal substrate for emulating neuron and synapse dynamics with an embedded digital processor for on-chip learning, which in this work also serves to simulate the virtual environment and learning agent. The analog emulation of neuronal membrane dynamics enables a 1000-fold acceleration with respect to biological real-time, with the entire chip operating on a power budget of 57 {mW}. Compared to an equivalent simulation using state-of-the-art software, the on-chip emulation is at least one order of magnitude faster and three orders of magnitude more energy-efﬁcient. We demonstrate how on-chip learning can mitigate the effects of ﬁxed-pattern noise, which is unavoidable in analog substrates, while making use of temporal variability for action exploration. Learning compensates imperfections of the physical substrate, as manifested in neuronal parameter variability, by adapting synaptic weights to match respective excitability of individual neurons.},
	journaltitle = {Frontiers in Neuroscience},
	author = {Wunderlich, Timo and Kungl, Akos F. and Müller, Eric and Hartel, Andreas and Stradmann, Yannik and Aamir, Syed Ahmed and Grübl, Andreas and Heimbrecht, Arthur and Schreiber, Korbinian and Stöckel, David and Pehle, Christian and Billaudelle, Sebastian and Kiene, Gerd and Mauch, Christian and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
	urldate = {2019-04-04},
	date = {2019-03-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.03618},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Neural and Evolutionary Computing},
	file = {Wunderlich et al. - 2019 - Demonstrating Advantages of Neuromorphic Computati.pdf:/home/danielsan/Zotero/storage/D5DUX5R2/Wunderlich et al. - 2019 - Demonstrating Advantages of Neuromorphic Computati.pdf:application/pdf}
}

@article{al-dabbagh_parallel_quicksort_algorithm_using_openm_2016,
	title = {Parallel\_Quicksort\_Algorithm\_using\_OpenM},
	url = {https://figshare.com/articles/Parallel_Quicksort_Algorithm_using_OpenM/3470033},
	doi = {10.6084/m9.figshare.3470033},
	abstract = {In this paper we aims to parallelization the Quicksort algorithm using multithreading ({OpenMP}) {\textless}br{\textgreater}platform. ‎The proposed method examined on two standard dataset (‎File 1: Hamlet.txt 180 {KB} and File 2: {\textless}br{\textgreater}Moby ‎Dick.txt ‎‎1.18 {MB}) ‎ with different number of threads. The fundamental idea of the proposed algorithm {\textless}br{\textgreater}is to creating many additional temporary sub-arrays according to a number of ‎characters in each word, the {\textless}br{\textgreater}sizes of each one of these sub-arrays are adopted based on a number of ‎elements with the exact same number {\textless}br{\textgreater}of characters in the input array. The elements of the input ‎datasets is distributing into these temporary sub-{\textless}br{\textgreater}arrays depending on the number of characters in each ‎word.‎ ‎As a conclusion, the experimental results of this {\textless}br{\textgreater}study  reveal  that  the  performance  of  parallelization  the  proposed  ‎Quicksort  algorithm  has  shown {\textless}br{\textgreater}improvement  when  compared  ‎to  the  sequential  Quicksort  algorithm  by  ‎delivering  improved  Execution {\textless}br{\textgreater}Time, ‎Speedup and Efficiency.},
	journaltitle = {Figshare},
	author = {{AL}-Dabbagh, Sinan},
	urldate = {2019-03-25},
	date = {2016},
	langid = {english},
	file = {AL-Dabbagh - 2016 - Parallel_Quicksort_Algorithm_using_OpenM.pdf:/home/danielsan/Zotero/storage/CZN4VVE7/AL-Dabbagh - 2016 - Parallel_Quicksort_Algorithm_using_OpenM.pdf:application/pdf}
}

@inproceedings{basu_deepsat:_2015,
	location = {Bellevue, Washington},
	title = {{DeepSat}: a learning framework for satellite imagery},
	isbn = {978-1-4503-3967-4},
	url = {http://dl.acm.org/citation.cfm?doid=2820783.2820816},
	doi = {10.1145/2820783.2820816},
	shorttitle = {{DeepSat}},
	abstract = {Satellite image classiﬁcation is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classiﬁcation approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled highresolution dataset with multiple class labels. The contributions of this paper are twofold – (1) ﬁrst, we present two new satellite datasets called {SAT}-4 and {SAT}-6, and (2) then, we propose a classiﬁcation framework that extracts features from an input image, normalizes them and feeds the normalized feature vectors to a Deep Belief Network for classiﬁcation. On the {SAT}-4 dataset, our best network produces a classiﬁcation accuracy of 97.95\% and outperforms three state-of-the-art object recognition algorithms, namely Deep Belief Networks, Convolutional Neural Networks and Stacked Denoising Autoencoders by ∼11\%. On {SAT}-6, it produces a classiﬁcation accuracy of 93.9\% and outperforms the other algorithms by ∼15\%. Comparative studies with a Random Forest classiﬁer show the advantage of an unsupervised learning approach over traditional supervised learning techniques. A statistical analysis based on Distribution Separability Criterion and Intrinsic Dimensionality Estimation substantiates the effectiveness of our approach in learning better representations for satellite imagery.},
	eventtitle = {the 23rd {SIGSPATIAL} International Conference},
	pages = {1--10},
	booktitle = {Proceedings of the 23rd {SIGSPATIAL} International Conference on Advances in Geographic Information Systems - {GIS} '15},
	publisher = {{ACM} Press},
	author = {Basu, Saikat and Ganguly, Sangram and Mukhopadhyay, Supratik and {DiBiano}, Robert and Karki, Manohar and Nemani, Ramakrishna},
	urldate = {2019-11-07},
	date = {2015},
	langid = {english},
	file = {Basu et al. - 2015 - DeepSat a learning framework for satellite imager.pdf:/home/danielsan/Zotero/storage/GMTIHY4J/Basu et al. - 2015 - DeepSat a learning framework for satellite imager.pdf:application/pdf}
}

@article{helber_eurosat:_2019,
	title = {{EuroSAT}: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},
	volume = {12},
	issn = {1939-1404, 2151-1535},
	doi = {10.1109/JSTARS.2019.2918242},
	shorttitle = {{EuroSAT}},
	abstract = {In this paper, we present a patch-based land use and land cover classification approach using Sentinel-2 satellite images. The Sentinel-2 satellite images are openly and freely accessible, and are provided in the earth observation program Copernicus. We present a novel dataset, based on these images that covers 13 spectral bands and is comprised of ten classes with a total of 27 000 labeled and geo-referenced images. Benchmarks are provided for this novel dataset with its spectral bands using state-of-the-art deep convolutional neural networks. An overall classification accuracy of 98.57\% was achieved with the proposed novel dataset. The resulting classification system opens a gate toward a number of earth observation applications. We demonstrate how this classification system can be used for detecting land use and land cover changes, and how it can assist in improving geographical maps. The geo-referenced dataset {EuroSAT} is made publicly available at https://github.com/phelber/eurosat.},
	pages = {2217--2226},
	number = {7},
	journaltitle = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
	date = {2019-07},
	keywords = {learning (artificial intelligence), Benchmark testing, convolutional neural nets, Dataset, deep convolutional neural network, deep convolutional neural networks, deep learning, deep learning benchmark, Earth, earth observation, Earth observation program Copernicus, feature extraction, Feature extraction, geo-referenced dataset {EuroSAT}, geo-referenced images, geographical maps, geophysical image processing, image classification, land cover, land cover changes, land cover classification, land cover classification approach, land use, land use classification, machine learning, Machine learning, patch-based land use, remote sensing, Remote sensing, satellite image classification, satellite images, Satellites, Sentinel-2 satellite images, Spatial resolution},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/ZTKGW4SN/8736785.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/6S83MNSM/Helber et al. - 2019 - EuroSAT A Novel Dataset and Deep Learning Benchma.pdf:application/pdf}
}

@article{trivedi_low-level_1986,
	title = {Low-Level Segmentation of Aerial Images with Fuzzy Clustering},
	volume = {16},
	issn = {0018-9472, 2168-2909},
	doi = {10.1109/TSMC.1986.289264},
	abstract = {A low-level segmentation methodology based upon fuzzy clustering principles is developed. The approach utilizes region growing concepts and a pyramid data structure for the hierarchical analysis of aerial images. It is assumed that measurement vectors corresponding to perceptually homogeneous regions cluster together in the measurement space. The fuzzy c-means ({FCM}) clustering algorithm is used in the formulation. Utilization of the fuzzy partitioning allows one to derive a correspondence between the cluster membership function values and (the proportions of) the classes constituting a region. Thus cluster membership values can be used to split mixture regions into smaller regions at a higher resolution level. The feasibility of the methodology is evaluated using a three-channel Landsat image. The results show that the {FCM} clustering can be used in the single-level segmentation; and that cluster membership function values derived using this algorithm can be utilized effectively as indicators of region homogeneity.},
	pages = {589--598},
	number = {4},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics},
	author = {Trivedi, Mohan M. and Bezdek, James C.},
	date = {1986-07},
	keywords = {Cameras, Clustering algorithms, Computer vision, Image analysis, Image processing, Image segmentation, Machine vision, Robot sensing systems, Robot vision systems, Service robots},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/BFGYUHDB/4075616.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/GJ2TB8EQ/Trivedi and Bezdek - 1986 - Low-Level Segmentation of Aerial Images with Fuzzy.pdf:application/pdf}
}

@article{marmanis_semantic_2016,
	title = {{SEMANTIC} {SEGMENTATION} {OF} {AERIAL} {IMAGES} {WITH} {AN} {ENSEMBLE} {OF} {CNNS}},
	volume = {{III}-3},
	issn = {2194-9050},
	url = {http://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/III-3/473/2016/isprs-annals-III-3-473-2016.pdf},
	doi = {10.5194/isprsannals-III-3-473-2016},
	abstract = {This paper describes a deep learning approach to semantic segmentation of very high resolution (aerial) images. Deep neural architectures hold the promise of end-to-end learning from raw images, making heuristic feature design obsolete. Over the last decade this idea has seen a revival, and in recent years deep convolutional neural networks ({CNNs}) have emerged as the method of choice for a range of image interpretation tasks like visual recognition and object detection. Still, standard {CNNs} do not lend themselves to per-pixel semantic segmentation, mainly because one of their fundamental principles is to gradually aggregate information over larger and larger image regions, making it hard to disentangle contributions from different pixels. Very recently two extensions of the {CNN} framework have made it possible to trace the semantic information back to a precise pixel position: deconvolutional network layers undo the spatial downsampling, and Fully Convolution Networks ({FCNs}) modify the fully connected classiﬁcation layers of the network in such a way that the location of individual activations remains explicit. We design a {FCN} which takes as input intensity and range data and, with the help of aggressive deconvolution and recycling of early network layers, converts them into a pixelwise classiﬁcation at full resolution. We discuss design choices and intricacies of such a network, and demonstrate that an ensemble of several networks achieves excellent results on challenging data such as the {ISPRS} semantic labeling benchmark, using only the raw data as input.},
	pages = {473--480},
	journaltitle = {{ISPRS} Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Marmanis, D. and Wegner, J. D. and Galliani, S. and Schindler, K. and Datcu, M. and Stilla, U.},
	urldate = {2019-11-07},
	date = {2016-06-06},
	langid = {english},
	file = {Marmanis et al. - 2016 - SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENS.pdf:/home/danielsan/Zotero/storage/GQILDPFV/Marmanis et al. - 2016 - SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENS.pdf:application/pdf}
}

@article{karkus_particle_2018,
	title = {Particle Filter Networks with Application to Visual Localization},
	url = {http://arxiv.org/abs/1805.08975},
	abstract = {Particle ﬁltering is a powerful approach to sequential state estimation and ﬁnds application in many domains, including robot localization, object tracking, etc. To apply particle ﬁltering in practice, a critical challenge is to construct probabilistic system models, especially for systems with complex dynamics or rich sensory inputs such as camera images. This paper introduces the Particle Filter Network ({PFnet}), which encodes both a system model and a particle ﬁlter algorithm in a single neural network. The {PF}-net is fully differentiable and trained end-to-end from data. Instead of learning a generic system model, it learns a model optimized for the particle ﬁlter algorithm. We apply the {PF}-net to a visual localization task, in which a robot must localize in a rich 3-D world, using only a schematic 2-D ﬂoor map. In simulation experiments, {PF}-net consistently outperforms alternative learning architectures, as well as a traditional model-based method, under a variety of sensor inputs. Further, {PF}-net generalizes well to new, unseen environments.},
	journaltitle = {{arXiv}:1805.08975 [cs, stat]},
	author = {Karkus, Peter and Hsu, David and Lee, Wee Sun},
	urldate = {2019-11-07},
	date = {2018-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.08975},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {Karkus et al. - 2018 - Particle Filter Networks with Application to Visua.pdf:/home/danielsan/Zotero/storage/8B2F5VRV/Karkus et al. - 2018 - Particle Filter Networks with Application to Visua.pdf:application/pdf}
}

@incollection{hutchison_introduction_2012,
	location = {Berlin, Heidelberg},
	title = {An Introduction to Random Forests for Multi-class Object Detection},
	volume = {7474},
	isbn = {978-3-642-34090-1 978-3-642-34091-8},
	url = {http://link.springer.com/10.1007/978-3-642-34091-8_11},
	abstract = {Object detection in large-scale real-world scenes requires eﬃcient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection eﬃciently. The most prominent example is the commercial application of random forests for gaming [36]. In this chapter, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners.},
	pages = {243--263},
	booktitle = {Outdoor and Large-Scale Real-World Scene Analysis},
	publisher = {Springer Berlin Heidelberg},
	author = {Gall, Juergen and Razavi, Nima and Van Gool, Luc},
	editor = {Dellaert, Frank and Frahm, Jan-Michael and Pollefeys, Marc and Leal-Taixé, Laura and Rosenhahn, Bodo},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2019-11-08},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-34091-8_11},
	file = {Gall et al. - 2012 - An Introduction to Random Forests for Multi-class .pdf:/home/danielsan/Zotero/storage/2C43ZS3L/Gall et al. - 2012 - An Introduction to Random Forests for Multi-class .pdf:application/pdf}
}

@inproceedings{hudjakov_aerial_2009-1,
	location = {Istanbul, Turkey},
	title = {Aerial imagery terrain classification for long-range autonomous navigation},
	isbn = {978-1-4244-4209-6},
	url = {http://ieeexplore.ieee.org/document/5326104/},
	doi = {10.1109/ISOT.2009.5326104},
	abstract = {The paper presents a method of terrain classification and path planning for unmanned ground vehicles. The terrain classification is done on imagery that is acquired from {UAV} (Unmanned Aerial Vehicle) and is used for {UGV} (Unmanned Ground Vehicle) path planning thus introducing collaboration capabilities to the system of two. The system complements {UGV} on-board navigation system by increasing its perception distance and providing long-range path planning capability.},
	eventtitle = {2009 International Symposium on Optomechatronic Technologies ({ISOT} 2009)},
	pages = {88--91},
	booktitle = {2009 International Symposium on Optomechatronic Technologies},
	publisher = {{IEEE}},
	author = {Hudjakov, Robert and Tamre, Mart},
	urldate = {2019-11-21},
	date = {2009-09},
	langid = {english},
	file = {Hudjakov and Tamre - 2009 - Aerial imagery terrain classification for long-ran.pdf:/home/danielsan/Zotero/storage/F2HER8G8/Hudjakov and Tamre - 2009 - Aerial imagery terrain classification for long-ran.pdf:application/pdf}
}

@inproceedings{rublee_orb:_2011,
	location = {Barcelona, Spain},
	title = {{ORB}: An efficient alternative to {SIFT} or {SURF}},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126544/},
	doi = {10.1109/ICCV.2011.6126544},
	shorttitle = {{ORB}},
	abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on {BRIEF}, called {ORB}, which is rotation invariant and resistant to noise. We demonstrate through experiments how {ORB} is at two orders of magnitude faster than {SIFT}, while performing as well in many situations. The efﬁciency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
	eventtitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2564--2571},
	booktitle = {2011 International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
	urldate = {2019-11-21},
	date = {2011-11},
	langid = {english},
	file = {Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf:/home/danielsan/Zotero/storage/9PENBF67/Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf:application/pdf}
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2019-11-22},
	date = {2016-06},
	langid = {english},
	file = {Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:/home/danielsan/Zotero/storage/B82H8PD2/Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf:application/pdf}
}

@online{noauthor_visual_nodate,
	title = {visual localization of uav using ml},
	url = {https://www.overleaf.com/project/5dc4038d85adaf00019cf2cc},
	abstract = {An online {LaTeX} editor that's easy to use. No installation, real-time collaboration, version control, hundreds of {LaTeX} templates, and more.},
	urldate = {2019-11-22},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/BYRXCH2Y/5dc4038d85adaf00019cf2cc.html:text/html}
}

@incollection{fleet_30hz_2014,
	location = {Cham},
	title = {30Hz Object Detection with {DPM} V5},
	volume = {8689},
	isbn = {978-3-319-10589-5 978-3-319-10590-1},
	url = {http://link.springer.com/10.1007/978-3-319-10590-1_5},
	abstract = {We describe an implementation of the Deformable Parts Model [1] that operates in a user-deﬁned time-frame. Our implementation uses a variety of mechanism to trade-oﬀ speed against accuracy. Our implementation can detect all 20 {PASCAL} 2007 objects simultaneously at 30Hz with an {mAP} of 0.26. At 15Hz, its {mAP} is 0.30; and at 100Hz, its {mAP} is 0.16. By comparison the reference implementation of [1] runs at 0.07Hz and {mAP} of 0.33 and a fast {GPU} implementation runs at 1Hz. Our technique is over an order of magnitude faster than the previous fastest {DPM} implementation. Our implementation exploits a series of important speedup mechanisms. We use the cascade framework of [3] and the vector quantization technique of [2]. To speed up feature computation, we compute {HOG} features at few scales, and apply many interpolated templates. A hierarchical vector quantization method is used to compress {HOG} features for fast template evaluation. An object proposal step uses hash-table methods to identify locations where evaluating templates would be most useful; these locations are inserted into a priority queue, and processed in a detection phase. Both proposal and detection phases have an any-time property. Our method applies to legacy templates, and no retraining is required.},
	pages = {65--79},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Sadeghi, Mohammad Amin and Forsyth, David},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2019-11-22},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10590-1_5},
	file = {Sadeghi and Forsyth - 2014 - 30Hz Object Detection with DPM V5.pdf:/home/danielsan/Zotero/storage/2ABRNLNR/Sadeghi and Forsyth - 2014 - 30Hz Object Detection with DPM V5.pdf:application/pdf}
}

@inproceedings{lowe_object_1999,
	location = {Kerkyra, Greece},
	title = {Object recognition from local scale-invariant features},
	isbn = {978-0-7695-0164-2},
	url = {http://ieeexplore.ieee.org/document/790410/},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and afﬁne or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efﬁciently detected through a staged ﬁltering approach that identiﬁes stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identiﬁes candidate object matches. Final veriﬁcation of each match is achieved by ﬁnding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {1150--1157 vol.2},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Lowe, D.G.},
	urldate = {2019-11-22},
	date = {1999},
	langid = {english},
	file = {Lowe - 1999 - Object recognition from local scale-invariant feat.pdf:/home/danielsan/Zotero/storage/6Q847YQC/Lowe - 1999 - Object recognition from local scale-invariant feat.pdf:application/pdf}
}

@article{uijlings_selective_2013,
	title = {Selective Search for Object Recognition},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi. unitn.it/{\textasciitilde}uijlings/{SelectiveSearch}.html).},
	pages = {154--171},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	urldate = {2019-11-22},
	date = {2013-09},
	langid = {english},
	file = {Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:/home/danielsan/Zotero/storage/SAHKV8BN/Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf}
}

@article{gould_region-based_nodate,
	title = {Region-based Segmentation and Object Detection},
	abstract = {Object detection and multi-class image segmentation are two closely related tasks that can be greatly improved when solved jointly by feeding information from one task to the other [10, 11]. However, current state-of-the-art models use a separate representation for each task making joint inference clumsy and leaving the classiﬁcation of many parts of the scene ambiguous.},
	pages = {9},
	author = {Gould, Stephen and Gao, Tianshi and Koller, Daphne},
	langid = {english},
	file = {Gould et al. - Region-based Segmentation and Object Detection.pdf:/home/danielsan/Zotero/storage/ZU2JHKPJ/Gould et al. - Region-based Segmentation and Object Detection.pdf:application/pdf}
}

@incollection{fleet_edge_2014,
	location = {Cham},
	title = {Edge Boxes: Locating Object Proposals from Edges},
	volume = {8693},
	isbn = {978-3-319-10601-4 978-3-319-10602-1},
	url = {http://link.springer.com/10.1007/978-3-319-10602-1_26},
	shorttitle = {Edge Boxes},
	abstract = {The use of object proposals is an eﬀective recent approach for increasing the computational eﬃciency of object detection. We propose a novel method for generating object bounding box proposals using edges. Edges provide a sparse yet informative representation of an image. Our main observation is that the number of contours that are wholly contained in a bounding box is indicative of the likelihood of the box containing an object. We propose a simple box objectness score that measures the number of edges that exist in the box minus those that are members of contours that overlap the box’s boundary. Using eﬃcient data structures, millions of candidate boxes can be evaluated in a fraction of a second, returning a ranked set of a few thousand top-scoring proposals. Using standard metrics, we show results that are signiﬁcantly more accurate than the current state-of-the-art while being faster to compute. In particular, given just 1000 proposals we achieve over 96\% object recall at overlap threshold of 0.5 and over 75\% recall at the more challenging overlap of 0.7. Our approach runs in 0.25 seconds and we additionally demonstrate a near real-time variant with only minor loss in accuracy.},
	pages = {391--405},
	booktitle = {Computer Vision – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Zitnick, C. Lawrence and Dollár, Piotr},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	urldate = {2019-11-22},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-319-10602-1_26},
	file = {Zitnick and Dollár - 2014 - Edge Boxes Locating Object Proposals from Edges.pdf:/home/danielsan/Zotero/storage/GRTQKTM5/Zitnick and Dollár - 2014 - Edge Boxes Locating Object Proposals from Edges.pdf:application/pdf}
}

@article{shinn-ying_ho_intelligent_2004,
	title = {Intelligent evolutionary algorithms for large parameter optimization problems},
	volume = {8},
	issn = {1941-0026},
	doi = {10.1109/TEVC.2004.835176},
	abstract = {This work proposes two intelligent evolutionary algorithms {IEA} and {IMOEA} using a novel intelligent gene collector ({IGC}) to solve single and multiobjective large parameter optimization problems, respectively. {IGC} is the main phase in an intelligent recombination operator of {IEA} and {IMOEA}. Based on orthogonal experimental design, {IGC} uses a divide-and-conquer approach, which consists of adaptively dividing two individuals of parents into N pairs of gene segments, economically identifying the potentially better one of two gene segments of each pair, and systematically obtaining a potentially good approximation to the best one of all combinations using at most 2N fitness evaluations. {IMOEA} utilizes a novel generalized Pareto-based scale-independent fitness function for efficiently finding a set of Pareto-optimal solutions to a multiobjective optimization problem. The advantages of {IEA} and {IMOEA} are their simplicity, efficiency, and flexibility. It is shown empirically that {IEA} and {IMOEA} have high performance in solving benchmark functions comprising many parameters, as compared with some existing {EAs}.},
	pages = {522--541},
	number = {6},
	journaltitle = {{IEEE} Transactions on Evolutionary Computation},
	author = {Shinn-Ying Ho and Li-Sun Shu and Jian-Hung Chen},
	date = {2004-12},
	keywords = {Genetic algorithms, 2N fitness evaluations, Councils, Design for experiments, Design optimization, divide and conquer methods, divide-and-conquer approach, economical identification, Evolution (biology), Evolutionary algorithm ({EA}), evolutionary computation, Evolutionary computation, gene segments, genetic algorithm ({GA}), Genetic mutations, Genetic programming, intelligent evolutionary algorithms, intelligent gene collector, intelligent gene collector ({IGC}), intelligent recombination operator, large parameter optimization problem, multiobjective optimization, multiobjective optimization problem, Optimization methods, orthogonal experimental design, Pareto-based scale-independent fitness function, Pareto-optimal solution},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/ADKT4R33/1369245.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/2M6YSV8A/Shinn-Ying Ho et al. - 2004 - Intelligent evolutionary algorithms for large para.pdf:application/pdf}
}

@article{ji_invariant_2019,
	title = {Invariant Information Clustering for Unsupervised Image Classification and Segmentation},
	url = {http://arxiv.org/abs/1807.06653},
	abstract = {We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include {STL}10, an unsupervised variant of {ImageNet}, and {CIFAR}10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8\% accuracy on {STL}10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90\% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/{IIC}},
	journaltitle = {{arXiv}:1807.06653 [cs]},
	author = {Ji, Xu and Henriques, João F. and Vedaldi, Andrea},
	urldate = {2019-11-22},
	date = {2019-08-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.06653},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {Ji et al. - 2019 - Invariant Information Clustering for Unsupervised .pdf:/home/danielsan/Zotero/storage/4IAX8679/Ji et al. - 2019 - Invariant Information Clustering for Unsupervised .pdf:application/pdf}
}

@article{strydom_visual_2014,
	title = {Visual Odometry: Autonomous {UAV} Navigation using Optic Flow and Stereo},
	abstract = {Visual odometry is vital to the future of mobile robotics. In this paper, we demonstrate a method that combines information from optic flow and stereo to estimate and control the current position of a quadrotor along a pre-defined trajectory. The absolute translation in 3D is computed by combining the optic flow measurements between successive frames and stereo-based height over ground. The current 3D position, as estimated from path integration of the incremental translations, is controlled in closed loop to follow the prescribed trajectory. The performance of the system is evaluated by measuring the error between the initial and final positions in closed circuits. This error is approximately 1.7\% of the total path length.},
	pages = {10},
	author = {Strydom, Reuben and Thurrowgood, Saul and Srinivasan, Mandyam V},
	date = {2014},
	langid = {english},
	file = {Strydom et al. - 2014 - Visual Odometry Autonomous UAV Navigation using O.pdf:/home/danielsan/Zotero/storage/75SSW67Z/Strydom et al. - 2014 - Visual Odometry Autonomous UAV Navigation using O.pdf:application/pdf}
}

@inproceedings{yol_vision-based_2014,
	title = {Vision-based absolute localization for unmanned aerial vehicles},
	doi = {10.1109/IROS.2014.6943040},
	abstract = {This paper presents a method for localizing an Unmanned Aerial Vehicle ({UAV}) using georeferenced aerial images. Easily maneuverable and more and more affordable, {UAVs} have become a real center of interest. In the last few years, their utilization has significantly increased. Today, they are used for multiple tasks such as navigation, transportation or vigilance. Nevertheless, the success of these tasks could not be possible without a highly accurate localization which can, unfortunately be often laborious. Here we provide a multiple usage localization algorithm based on vision only. However, a major drawback with vision-based algorithms is the lack of robustness. Most of the approaches are sensitive to scene variations (like season or environment changes) due to the fact that they use the Sum of Squared Differences ({SSD}). To prevent that, we choose to use the Mutual Information ({MI}) which is very robust toward local and global scene variations. However, dense approaches are often related to drift disadvantages. Here, we solve this problem by using georeferenced images. The localization algorithm has been implemented and experimental results are presented demonstrating the localization of a hexarotor {UAV} fitted with a downward looking camera during real flight tests.},
	eventtitle = {2014 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	pages = {3429--3434},
	booktitle = {2014 {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems},
	author = {Yol, Aurélien and Delabarre, Bertrand and Dame, Amaury and Dartois, Jean-Émile and Marchand, Eric},
	date = {2014-09},
	note = {{ISSN}: 2153-0866},
	keywords = {Robustness, Cameras, autonomous aerial vehicles, camera, cameras, Estimation, flight tests, georeferenced aerial images, georeferenced images, Global Positioning System, global scene variations, image registration, Image registration, local scene variations, {MI}, multiple usage localization algorithm, mutual information, Mutual information, robot vision, {UAV}, unmanned aerial vehicles, vision-based absolute localization},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/I7NMU8Z2/6943040.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/PJSAU3RY/Yol et al. - 2014 - Vision-based absolute localization for unmanned ae.pdf:application/pdf}
}

@article{carroll_vulnerability_2003,
	title = {Vulnerability Assessment of the U.S. Transportation Infrastructure that Relies on the Global Positioning System},
	volume = {56},
	issn = {0373-4633, 1469-7785},
	url = {https://www.cambridge.org/core/product/identifier/S0373463303002273/type/journal_article},
	doi = {10.1017/S0373463303002273},
	pages = {185--193},
	number = {2},
	journaltitle = {Journal of Navigation},
	author = {Carroll, James V.},
	urldate = {2019-11-22},
	date = {2003-05},
	langid = {english},
	file = {Carroll - 2003 - Vulnerability Assessment of the U.S. Transportatio.pdf:/home/danielsan/Zotero/storage/ET3CZZ7E/Carroll - 2003 - Vulnerability Assessment of the U.S. Transportatio.pdf:application/pdf}
}

@inproceedings{zheng_rotation_2014,
	title = {Rotation and affine-invariant {SIFT} descriptor for matching {UAV} images with satellite images},
	doi = {10.1109/CGNCC.2014.7007582},
	abstract = {Image matching is a key issue in Vision-Based {UAV} navigation problems. This paper presents an affine and rotation-invariant {SIFT} features descriptor for matching {UAV} image with satellite images. The {SIFT} and {ASIFT} algorithm are nowadays widely applied for robust image matching, but it also has a high computational complexity. {SURF} is used for real-time {UAV} position estimation but is not satisfied for affine invariant. We introduce the new {SIFT} feature descriptor based on pie chart region. This descriptor is invariant for rotation, affine, scale and the dimension of the feature vector is relatively reduced. Therefore, this method satisfies robustness and low computational complexity. Experiments show that this method can improve the matching accuracy and robustness.},
	eventtitle = {Proceedings of 2014 {IEEE} Chinese Guidance, Navigation and Control Conference},
	pages = {2624--2628},
	booktitle = {Proceedings of 2014 {IEEE} Chinese Guidance, Navigation and Control Conference},
	author = {Zheng, Mingguo and Wu, Chengdong and Chen, Dongyue and Meng, Zhexiu},
	date = {2014-08},
	note = {{ISSN}: null},
	keywords = {satellite images, Satellites, autonomous aerial vehicles, affine-invariant {SIFT} feature descriptor, {ASIFT} algorithm, computer vision, Educational institutions, image matching, Image matching, pie chart region, real-time {UAV} position estimation, rotation-invariant {SIFT} features descriptor, Satellite navigation systems, Shape, {SURF}, transforms, Transforms, {UAV} image matching, unmanned aerial vehicle, Vectors, vision-based {UAV} navigation problems},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/P3T69NUC/7007582.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/5ICKA4IQ/Zheng et al. - 2014 - Rotation and affine-invariant SIFT descriptor for .pdf:application/pdf}
}

@incollection{hutchison_brief:_2010,
	location = {Berlin, Heidelberg},
	title = {{BRIEF}: Binary Robust Independent Elementary Features},
	volume = {6314},
	isbn = {978-3-642-15560-4 978-3-642-15561-1},
	url = {http://link.springer.com/10.1007/978-3-642-15561-1_56},
	shorttitle = {{BRIEF}},
	abstract = {We propose to use binary strings as an eﬃcient feature point descriptor, which we call {BRIEF}. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity diﬀerence tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very eﬃcient to compute, instead of the L2 norm as is usually done.},
	pages = {778--792},
	booktitle = {Computer Vision – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	urldate = {2019-11-22},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-15561-1_56},
	file = {Hutchison et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf:/home/danielsan/Zotero/storage/Q55JAPHX/Hutchison et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf:application/pdf}
}

@inproceedings{khan_visual_2012,
	title = {Visual terrain classification by flying robots},
	doi = {10.1109/ICRA.2012.6224988},
	abstract = {In this paper we investigate the effectiveness of {SURF} features for visual terrain classification for outdoor flying robots. A quadrocopter fitted with a single camera is flown over different terrains to take images of the ground below. Each image is divided into a grid and {SURF} features are calculated at grid intersections. A classifier is then used to learn to differentiate between different terrain types. Classification results of the {SURF} descriptor are compared with results from other texture descriptors like Local Binary Patterns and Local Ternary Patterns. Six different terrain types are considered in this approach. Random forests are used for classification on each descriptor. It is shown that {SURF} features perform better than other descriptors at higher resolutions.},
	eventtitle = {2012 {IEEE} International Conference on Robotics and Automation},
	pages = {498--503},
	booktitle = {2012 {IEEE} International Conference on Robotics and Automation},
	author = {Khan, Yasir Niaz and Masselli, Andreas and Zell, Andreas},
	date = {2012-05},
	note = {{ISSN}: 1050-4729},
	keywords = {mobile robots, Feature extraction, image classification, Cameras, Accuracy, control engineering computing, helicopters, Image resolution, image texture, local binary patterns, local ternary patterns, outdoor flying robots, quadrocopter, random forests, Robots, {SURF} features, terrain mapping, texture descriptors, Vegetation, visual terrain classification, Visualization},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/RSNIRKMY/6224988.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/9Q5GQC7B/Khan et al. - 2012 - Visual terrain classification by flying robots.pdf:application/pdf}
}

@article{radovic_object_2017-1,
	title = {Object Recognition in Aerial Images Using Convolutional Neural Networks},
	volume = {3},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2313-433X/3/2/21},
	doi = {10.3390/jimaging3020021},
	abstract = {There are numerous applications of unmanned aerial vehicles ({UAVs}) in the management of civil infrastructure assets. A few examples include routine bridge inspections, disaster management, power line surveillance and traffic surveying. As {UAV} applications become widespread, increased levels of autonomy and independent decision-making are necessary to improve the safety, efficiency, and accuracy of the devices. This paper details the procedure and parameters used for the training of convolutional neural networks ({CNNs}) on a set of aerial images for efficient and automated object recognition. Potential application areas in the transportation field are also highlighted. The accuracy and reliability of {CNNs} depend on the network’s training and the selection of operational parameters. This paper details the {CNN} training procedure and parameter selection. The object recognition results show that by selecting a proper set of parameters, a {CNN} can detect and classify objects with a high level of accuracy (97.5\%) and computational efficiency. Furthermore, using a convolutional neural network implemented in the “{YOLO}” (“You Only Look Once”) platform, objects can be tracked, detected (“seen”), and classified (“comprehended”) from video feeds supplied by {UAVs} in real-time.},
	pages = {21},
	number = {2},
	journaltitle = {Journal of Imaging},
	author = {Radovic, Matija and Adarkwa, Offei and Wang, Qiaosong},
	urldate = {2019-11-22},
	date = {2017-06},
	langid = {english},
	keywords = {convolutional neural networks, object recognition and detection, Unmanned Aerial Vehicle ({UAV})},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/NBBX5CHW/Radovic et al. - 2017 - Object Recognition in Aerial Images Using Convolut.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/WHEBG2IP/htm.html:text/html}
}

@article{oshea_introduction_2015,
	title = {An Introduction to Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1511.08458},
	abstract = {The ﬁeld of machine learning has taken a dramatic twist in recent times, with the rise of the Artiﬁcial Neural Network ({ANN}). These biologically inspired computational models are able to far exceed the performance of previous forms of artiﬁcial intelligence in common machine learning tasks. One of the most impressive forms of {ANN} architecture is that of the Convolutional Neural Network ({CNN}). {CNNs} are primarily used to solve difﬁcult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simpliﬁed method of getting started with {ANNs}.},
	journaltitle = {{arXiv}:1511.08458 [cs]},
	author = {O'Shea, Keiron and Nash, Ryan},
	urldate = {2019-11-22},
	date = {2015-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.08458},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf:/home/danielsan/Zotero/storage/YW4HK2QC/O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf:application/pdf}
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2019-11-22},
	date = {2001-10-01},
	langid = {english},
	keywords = {classification, ensemble, regression},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/36YHF3LN/Breiman - 2001 - Random Forests.pdf:application/pdf}
}

@article{ren_faster_2017,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7485869/},
	doi = {10.1109/TPAMI.2016.2577031},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} [7] and Fast R-{CNN} [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network ({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. {RPNs} are trained end-to-end to generate highquality region proposals, which are used by Fast R-{CNN} for detection. With a simple alternating optimization, {RPN} and Fast R-{CNN} can be trained to share convolutional features. For the very deep {VGG}-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007 (73.2\% {mAP}) and 2012 (70.4\% {mAP}) using 300 proposals per image. Code is available at https://github.com/{ShaoqingRen}/faster\_rcnn.},
	pages = {1137--1149},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	urldate = {2019-11-22},
	date = {2017-06-01},
	langid = {english},
	file = {Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf:/home/danielsan/Zotero/storage/2XEWC8CM/Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf}
}

@incollection{hutchison_brief:_2010-1,
	location = {Berlin, Heidelberg},
	title = {{BRIEF}: Binary Robust Independent Elementary Features},
	volume = {6314},
	isbn = {978-3-642-15560-4 978-3-642-15561-1},
	url = {http://link.springer.com/10.1007/978-3-642-15561-1_56},
	shorttitle = {{BRIEF}},
	abstract = {We propose to use binary strings as an eﬃcient feature point descriptor, which we call {BRIEF}. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity diﬀerence tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very eﬃcient to compute, instead of the L2 norm as is usually done.},
	pages = {778--792},
	booktitle = {Computer Vision – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	urldate = {2019-11-22},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-15561-1_56},
	file = {Hutchison et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf:/home/danielsan/Zotero/storage/ALIUVABG/Hutchison et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf:application/pdf}
}

@article{pei_towards_2019,
	title = {Towards artificial general intelligence with hybrid Tianjic chip architecture},
	volume = {572},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1424-8},
	doi = {10.1038/s41586-019-1424-8},
	abstract = {The ‘Tianjic’ hybrid electronic chip combines neuroscience-oriented and computer-science-oriented approaches to artificial general intelligence, demonstrated by controlling an unmanned bicycle.},
	pages = {106--111},
	number = {7767},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Pei, Jing and Deng, Lei and Song, Sen and Zhao, Mingguo and Zhang, Youhui and Wu, Shuang and Wang, Guanrui and Zou, Zhe and Wu, Zhenzhi and He, Wei and Chen, Feng and Deng, Ning and Wu, Si and Wang, Yu and Wu, Yujie and Yang, Zheyu and Ma, Cheng and Li, Guoqi and Han, Wentao and Li, Huanglong and Wu, Huaqiang and Zhao, Rong and Xie, Yuan and Shi, Luping},
	urldate = {2019-12-05},
	date = {2019-08},
	langid = {english},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/F3MQK23L/Pei et al. - 2019 - Towards artificial general intelligence with hybri.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/PNRSE474/s41586-019-1424-8.html:text/html}
}

@article{wold_principal_1987,
	title = {Principal component analysis},
	volume = {2},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/0169743987800849},
	doi = {10.1016/0169-7439(87)80084-9},
	series = {Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists},
	abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of {PC} projections, {PLS} regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.},
	pages = {37--52},
	number = {1},
	journaltitle = {Chemometrics and Intelligent Laboratory Systems},
	shortjournal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Wold, Svante and Esbensen, Kim and Geladi, Paul},
	urldate = {2019-12-13},
	date = {1987-08-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/home/danielsan/Zotero/storage/XIL5LSJI/Wold et al. - 1987 - Principal component analysis.pdf:application/pdf;ScienceDirect Snapshot:/home/danielsan/Zotero/storage/6I2JP7CY/0169743987800849.html:text/html}
}

@article{oizumi_phenomenology_2014,
	title = {From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0},
	volume = {10},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1003588},
	doi = {10.1371/journal.pcbi.1003588},
	shorttitle = {From the Phenomenology to the Mechanisms of Consciousness},
	abstract = {This paper presents Integrated Information Theory ({IIT}) of consciousness 3.0, which incorporates several advances over previous formulations. {IIT} starts from phenomenological axioms: information says that each experience is specific – it is what it is by how it differs from alternative experiences; integration says that it is unified – irreducible to noninterdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ‘‘differences that make a difference’’ within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, {IIT} arrives at an identity: an experience is a maximally irreducible conceptual structure ({MICS}, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to {IIT}, a {MICS} specifies the quality of an experience and integrated information {WMax} its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated {MICS}; a complex can generate a {MICS} even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ‘‘zombies’’ – unconscious feed-forward systems that are functionally equivalent to conscious complexes.},
	pages = {e1003588},
	number = {5},
	journaltitle = {{PLoS} Computational Biology},
	author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
	editor = {Sporns, Olaf},
	urldate = {2020-01-09},
	date = {2014-05-08},
	langid = {english},
	file = {Oizumi et al. - 2014 - From the Phenomenology to the Mechanisms of Consci.pdf:/home/danielsan/Zotero/storage/G7EZSMYL/Oizumi et al. - 2014 - From the Phenomenology to the Mechanisms of Consci.pdf:application/pdf}
}

@article{oizumi_measuring_2016,
	title = {Measuring Integrated Information from the Decoding Perspective},
	volume = {12},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1004654},
	doi = {10.1371/journal.pcbi.1004654},
	pages = {e1004654},
	number = {1},
	journaltitle = {{PLOS} Computational Biology},
	author = {Oizumi, Masafumi and Amari, Shun-ichi and Yanagawa, Toru and Fujii, Naotaka and Tsuchiya, Naotsugu},
	editor = {Polani, Daniel},
	urldate = {2020-01-09},
	date = {2016-01-21},
	langid = {english},
	file = {Oizumi et al. - 2016 - Measuring Integrated Information from the Decoding.pdf:/home/danielsan/Zotero/storage/I3NC2YXQ/Oizumi et al. - 2016 - Measuring Integrated Information from the Decoding.pdf:application/pdf}
}

@article{tononi_integrated_2016,
	title = {Integrated information theory: from consciousness to its physical substrate},
	volume = {17},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/nrn.2016.44},
	doi = {10.1038/nrn.2016.44},
	shorttitle = {Integrated information theory},
	abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause–effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
	pages = {450--461},
	number = {7},
	journaltitle = {Nature Reviews Neuroscience},
	author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
	urldate = {2020-01-09},
	date = {2016-07},
	langid = {english},
	file = {Tononi et al. - 2016 - Integrated information theory from consciousness .pdf:/home/danielsan/Zotero/storage/X2JX7JHQ/Tononi et al. - 2016 - Integrated information theory from consciousness .pdf:application/pdf}
}

@article{schossau_information-theoretic_2016,
	title = {Information-Theoretic Neuro-Correlates Boost Evolution of Cognitive Systems},
	volume = {18},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/18/1/6},
	doi = {10.3390/e18010006},
	abstract = {Genetic Algorithms ({GA}) are a powerful set of tools for search and optimization that mimic the process of natural selection, and have been used successfully in a wide variety of problems, including evolving neural networks to solve cognitive tasks. Despite their success, {GAs} sometimes fail to locate the highest peaks of the fitness landscape, in particular if the landscape is rugged and contains multiple peaks. Reaching distant and higher peaks is difficult because valleys need to be crossed, in a process that (at least temporarily) runs against the fitness maximization objective. Here we propose and test a number of information-theoretic (as well as network-based) measures that can be used in conjunction with a fitness maximization objective (so-called “neuro-correlates”) to evolve neural controllers for two widely different tasks: a behavioral task that requires information integration, and a cognitive task that requires memory and logic. We find that judiciously chosen neuro-correlates can significantly aid {GAs} to find the highest peaks.},
	pages = {6},
	number = {1},
	journaltitle = {Entropy},
	author = {Schossau, Jory and Adami, Christoph and Hintze, Arend},
	urldate = {2020-01-09},
	date = {2016-01},
	langid = {english},
	keywords = {genetic algorithm, evolution, information theory, markov brain, neuro-correlate},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/ZEU7NIE2/Schossau et al. - 2016 - Information-Theoretic Neuro-Correlates Boost Evolu.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/R9DYNBYT/htm.html:text/html}
}

@article{albantakis_evolution_2014-1,
	title = {Evolution of Integrated Causal Structures in Animats Exposed to Environments of Increasing Complexity},
	volume = {10},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003966},
	doi = {10.1371/journal.pcbi.1003966},
	abstract = {Natural selection favors the evolution of brains that can capture fitness-relevant features of the environment’s causal structure. We investigated the evolution of small, adaptive logic-gate networks (‘‘animats’’) in task environments where falling blocks of different sizes have to be caught or avoided in a ‘Tetris-like’ game. Solving these tasks requires the integration of sensor inputs and memory. Evolved networks were evaluated using measures of information integration, including the number of evolved concepts and the total amount of integrated conceptual information. The results show that, over the course of the animats’ adaptation, i) the number of concepts grows; ii) integrated conceptual information increases; iii) this increase depends on the complexity of the environment, especially on the requirement for sequential memory. These results suggest that the need to capture the causal structure of a rich environment, given limited sensors and internal mechanisms, is an important driving force for organisms to develop highly integrated networks (‘‘brains’’) with many concepts, leading to an increase in their internal complexity.},
	pages = {e1003966},
	number = {12},
	journaltitle = {{PLoS} Computational Biology},
	author = {Albantakis, Larissa and Hintze, Arend and Koch, Christof and Adami, Christoph and Tononi, Giulio},
	editor = {Polani, Daniel},
	urldate = {2020-01-09},
	date = {2014-12-18},
	langid = {english},
	file = {Albantakis et al. - 2014 - Evolution of Integrated Causal Structures in Anima.pdf:/home/danielsan/Zotero/storage/RNBRWYDZ/Albantakis et al. - 2014 - Evolution of Integrated Causal Structures in Anima.pdf:application/pdf}
}

@article{edlund_integrated_2011,
	title = {Integrated Information Increases with Fitness in the Evolution of Animats},
	volume = {7},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1002236},
	doi = {10.1371/journal.pcbi.1002236},
	abstract = {One of the hallmarks of biological organisms is their ability to integrate disparate information sources to optimize their behavior in complex environments. How this capability can be quantified and related to the functional complexity of an organism remains a challenging problem, in particular since organismal functional complexity is not well-defined. We present here several candidate measures that quantify information and integration, and study their dependence on fitness as an artificial agent (‘‘animat’’) evolves over thousands of generations to solve a navigation task in a simple, simulated environment. We compare the ability of these measures to predict high fitness with more conventional informationtheoretic processing measures. As the animat adapts by increasing its ‘‘fit’’ to the world, information integration and processing increase commensurately along the evolutionary line of descent. We suggest that the correlation of fitness with information integration and with processing measures implies that high fitness requires both information processing as well as integration, but that information integration may be a better measure when the task requires memory. A correlation of measures of information integration (but also information processing) and fitness strongly suggests that these measures reflect the functional complexity of the animat, and that such measures can be used to quantify functional complexity even in the absence of fitness data.},
	pages = {e1002236},
	number = {10},
	journaltitle = {{PLoS} Computational Biology},
	author = {Edlund, Jeffrey A. and Chaumont, Nicolas and Hintze, Arend and Koch, Christof and Tononi, Giulio and Adami, Christoph},
	editor = {Graham, Lyle J.},
	urldate = {2020-01-09},
	date = {2011-10-20},
	langid = {english},
	file = {Edlund et al. - 2011 - Integrated Information Increases with Fitness in t.pdf:/home/danielsan/Zotero/storage/GEJWVBVE/Edlund et al. - 2011 - Integrated Information Increases with Fitness in t.pdf:application/pdf}
}

@article{joshi_minimal_2013,
	title = {The Minimal Complexity of Adapting Agents Increases with Fitness},
	volume = {9},
	abstract = {What is the relationship between the complexity and the fitness of evolved organisms, whether natural or artificial? It has been asserted, primarily based on empirical data, that the complexity of plants and animals increases as their fitness within a particular environment increases via evolution by natural selection. We simulate the evolution of the brains of simple organisms living in a planar maze that they have to traverse as rapidly as possible. Their connectome evolves over 10,000s of generations. We evaluate their circuit complexity, using four information-theoretical measures, including one that emphasizes the extent to which any network is an irreducible entity. We find that their minimal complexity increases with their fitness.},
	pages = {10},
	number = {7},
	journaltitle = {{PLOS} Computational Biology},
	author = {Joshi, Nikhil J and Tononi, Giulio and Koch, Christof},
	date = {2013},
	langid = {english},
	file = {Joshi et al. - 2013 - The Minimal Complexity of Adapting Agents Increase.pdf:/home/danielsan/Zotero/storage/MWXPAFAF/Joshi et al. - 2013 - The Minimal Complexity of Adapting Agents Increase.pdf:application/pdf}
}

@article{ay_predictive_2008,
	title = {Predictive information and explorative behavior of autonomous robots},
	volume = {63},
	issn = {1434-6028, 1434-6036},
	url = {http://link.springer.com/10.1140/epjb/e2008-00175-0},
	doi = {10.1140/epjb/e2008-00175-0},
	abstract = {Measures of complexity are of immediate interest for the ﬁeld of autonomous robots both as a means to classify the behavior and as an objective function for the autonomous development of robot behavior. In the present paper we consider predictive information in sensor space as a measure for the behavioral complexity of a two-wheel embodied robot moving in a rectangular arena with several obstacles. The mutual information ({MI}) between past and future sensor values is found empirically to have a maximum for a behavior which is both explorative and sensitive to the environment. This makes predictive information a prospective candidate as an objective function for the autonomous development of such behaviors. We derive theoretical expressions for the {MI} in order to obtain an explicit update rule for the gradient ascent dynamics. Interestingly, in the case of a linear or linearized model of the sensorimotor dynamics the structure of the learning rule derived depends only on the dynamical properties while the value of the {MI} inﬂuences only the learning rate. In this way the problem of the prohibitively large sampling times for information theoretic measures can be circumvented. This result can be generalized and may help to derive explicit learning rules from complexity theoretic measures.},
	pages = {329--339},
	number = {3},
	journaltitle = {The European Physical Journal B},
	author = {Ay, N. and Bertschinger, N. and Der, R. and Güttler, F. and Olbrich, E.},
	urldate = {2020-01-09},
	date = {2008-06},
	langid = {english},
	file = {Ay et al. - 2008 - Predictive information and explorative behavior of.pdf:/home/danielsan/Zotero/storage/YPWMAP67/Ay et al. - 2008 - Predictive information and explorative behavior of.pdf:application/pdf}
}

@article{mayner_pyphi_2018,
	title = {{PyPhi}: A toolbox for integrated information theory},
	volume = {14},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006343},
	doi = {10.1371/journal.pcbi.1006343},
	shorttitle = {{PyPhi}},
	abstract = {Integrated information theory provides a mathematical framework to fully characterize the cause-effect structure of a physical system. Here, we introduce {PyPhi}, a Python software package that implements this framework for causal analysis and unfolds the full cause-effect structure of discrete dynamical systems of binary elements. The software allows users to easily study these structures, serves as an up-to-date reference implementation of the formalisms of integrated information theory, and has been applied in research on complexity, emergence, and certain biological questions. We first provide an overview of the main algorithm and demonstrate {PyPhi}’s functionality in the course of analyzing an example system, and then describe details of the algorithm’s design and implementation. {PyPhi} can be installed with Python’s package manager via the command ‘pip install pyphi’ on Linux and {macOS} systems equipped with Python 3.4 or higher. {PyPhi} is open-source and licensed under the {GPLv}3; the source code is hosted on {GitHub} at https://github.com/wmayner/pyphi. Comprehensive and continually-updated documentation is available at https://pyphi.readthedocs.io. The pyphi-users mailing list can be joined at https://groups.google.com/forum/\#!forum/pyphi-users. A web-based graphical interface to the software is available at http://integratedinformationtheory.org/calculate.html.},
	pages = {e1006343},
	number = {7},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Mayner, William G. P. and Marshall, William and Albantakis, Larissa and Findlay, Graham and Marchman, Robert and Tononi, Giulio},
	urldate = {2020-01-09},
	date = {2018-07-26},
	langid = {english},
	keywords = {Algorithms, Calculus, Dynamical systems, Information theory, Optimization, Probability distribution, Software design, Source code},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/UGGL9VAZ/Mayner et al. - 2018 - PyPhi A toolbox for integrated information theory.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/65HRLK35/article.html:text/html}
}

@inproceedings{schuman_evolutionary_2016,
	title = {An evolutionary optimization framework for neural networks and neuromorphic architectures},
	doi = {10.1109/IJCNN.2016.7727192},
	abstract = {As new neural network and neuromorphic architectures are being developed, new training methods that operate within the constraints of the new architectures are required. Evolutionary optimization ({EO}) is a convenient training method for new architectures. In this work, we review a spiking neural network architecture and a neuromorphic architecture, and we describe an {EO} training framework for these architectures. We present the results of this training framework on four classification data sets and compare those results to other neural network and neuromorphic implementations. We also discuss how this {EO} framework may be extended to other architectures.},
	eventtitle = {2016 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {145--154},
	booktitle = {2016 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Schuman, Catherine D. and Plank, James S. and Disney, Adam and Reynolds, John},
	date = {2016-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Computer architecture, Neurons, Hardware, learning (artificial intelligence), Neuromorphics, evolutionary computation, Optimization, Artificial neural networks, {EO} training framework, evolutionary optimization framework, neural net architecture, neuromorphic architecture, optimisation, spiking neural network architecture, Training},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/U5TUD7RY/7727192.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/A7BX3IB8/Schuman et al. - 2016 - An evolutionary optimization framework for neural .pdf:application/pdf}
}

@online{noauthor_parallel_nodate,
	title = {Parallel evolutionary optimization for neuromorphic network training {\textbar} Proceedings of the Workshop on Machine Learning in High Performance Computing Environments},
	url = {https://dl.acm.org/doi/abs/10.5555/3018874.3018879},
	urldate = {2020-01-09},
	file = {Snapshot:/home/danielsan/Zotero/storage/45ZLVKEI/3018874.html:text/html}
}

@inproceedings{schuman_parallel_2016,
	location = {Salt Lake City, {UT}, {USA}},
	title = {Parallel Evolutionary Optimization for Neuromorphic Network Training},
	isbn = {978-1-5090-3882-4},
	url = {http://ieeexplore.ieee.org/document/7835813/},
	doi = {10.1109/MLHPC.2016.008},
	abstract = {One of the key impediments to the success of current neuromorphic computing architectures is the issue of how best to program them. Evolutionary optimization ({EO}) is one promising programming technique; in particular, its wide applicability makes it especially attractive for neuromorphic architectures, which can have many different characteristics. In this paper, we explore different facets of {EO} on a spiking neuromorphic computing model called {DANNA}. We focus on the performance of {EO} in the design of our {DANNA} simulator, and on how to structure {EO} on both multicore and massively parallel computing systems. We evaluate how our parallel methods impact the performance of {EO} on Titan, the U.S.’s largest open science supercomputer, and {BOB}, a Beowulf-style cluster of Raspberry Pi’s. We also focus on how to improve the {EO} by evaluating commonality in higher performing neural networks, and present the result of a study that evaluates the {EO} performed by Titan.},
	eventtitle = {2016 2nd Workshop on Machine Learning in {HPC} Environments ({MLHPC})},
	pages = {36--46},
	booktitle = {2016 2nd Workshop on Machine Learning in {HPC} Environments ({MLHPC})},
	publisher = {{IEEE}},
	author = {Schuman, Catherine D. and Disney, Adam and Singh, Susheela P. and Bruer, Grant and Mitchell, J. Parker and Klibisz, Aleksander and Plank, James S.},
	urldate = {2020-01-09},
	date = {2016-11},
	langid = {english},
	file = {Schuman et al. - 2016 - Parallel Evolutionary Optimization for Neuromorphi.pdf:/home/danielsan/Zotero/storage/PIBEG4C9/Schuman et al. - 2016 - Parallel Evolutionary Optimization for Neuromorphi.pdf:application/pdf}
}

@inproceedings{schuman_island_2019,
	location = {Prague, Czech Republic},
	title = {Island model for parallel evolutionary optimization of spiking neuromorphic computing},
	isbn = {978-1-4503-6748-6},
	url = {http://dl.acm.org/citation.cfm?doid=3319619.3322016},
	doi = {10.1145/3319619.3322016},
	abstract = {Parallel genetic algorithms ({PGAs}) can be used to accelerate optimization by exploiting large-scale computational resources. In this work, we describe a {PGA} framework for evolving spiking neural networks ({SNNs}) for neuromorphic hardware implementation. The {PGA} framework is based on an islands model with migration. We show that using this framework, better {SNNs} for neuromorphic systems can be evolved faster.},
	eventtitle = {the Genetic and Evolutionary Computation Conference Companion},
	pages = {306--307},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion on   - {GECCO} '19},
	publisher = {{ACM} Press},
	author = {Schuman, Catherine D. and Plank, James S. and Patton, Robert M. and Potok, Thomas E.},
	urldate = {2020-01-09},
	date = {2019},
	langid = {english},
	file = {Schuman et al. - 2019 - Island model for parallel evolutionary optimizatio.pdf:/home/danielsan/Zotero/storage/2LYHE4UP/Schuman et al. - 2019 - Island model for parallel evolutionary optimizatio.pdf:application/pdf}
}

@online{noauthor_spiking_nodate,
	title = {{SPIKING} {NEURAL} {NETWORKS} {\textbar} International Journal of Neural Systems},
	url = {https://www.worldscientific.com/doi/pdf/10.1142/S0129065709002002},
	urldate = {2020-01-09},
	file = {SPIKING NEURAL NETWORKS | International Journal of Neural Systems:/home/danielsan/Zotero/storage/476CXX85/S0129065709002002.html:text/html}
}

@incollection{rozenberg_computing_2012,
	location = {Berlin, Heidelberg},
	title = {Computing with Spiking Neuron Networks},
	isbn = {978-3-540-92909-3 978-3-540-92910-9},
	url = {http://link.springer.com/10.1007/978-3-540-92910-9_10},
	abstract = {Spiking Neuron Networks ({SNNs}) are often referred to as the third generation of neural networks. Highly inspired by natural computing in the brain and recent advances in neurosciences, they derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike ﬁring. {SNNs} overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity to memorize and a strong ability to do fast adaptation. Today, the main challenge is to discover efﬁcient learning rules that might take advantage of the speciﬁc features of {SNNs} while keeping the nice properties (general-purpose, easy-to-use, available simulators, etc.) of traditional connectionist models. This chapter relates the history of the ‘‘spiking neuron’’ in {\textgreater} Sect. 1 and summarizes the most currently-in-use models of neurons and synaptic plasticity in {\textgreater} Sect. 2. The computational power of {SNNs} is addressed in {\textgreater} Sect. 3 and the problem of learning in networks of spiking neurons is tackled in {\textgreater} Sect. 4, with insights into the tracks currently explored for solving it. Finally, {\textgreater} Sect. 5 discusses application domains, implementation issues and proposes several simulation frameworks.},
	pages = {335--376},
	booktitle = {Handbook of Natural Computing},
	publisher = {Springer Berlin Heidelberg},
	author = {Paugam-Moisy, Hélène and Bohte, Sander},
	editor = {Rozenberg, Grzegorz and Bäck, Thomas and Kok, Joost N.},
	urldate = {2020-01-09},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-540-92910-9_10},
	file = {Paugam-Moisy and Bohte - 2012 - Computing with Spiking Neuron Networks.pdf:/home/danielsan/Zotero/storage/4CG9RZCX/Paugam-Moisy and Bohte - 2012 - Computing with Spiking Neuron Networks.pdf:application/pdf}
}

@article{ghosh-dastidar_improved_2007,
	title = {Improved spiking neural networks for {EEG} classification and epilepsy and seizure detection},
	volume = {14},
	issn = {18758835, 10692509},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/ICA-2007-14301},
	doi = {10.3233/ICA-2007-14301},
	abstract = {The goal of this research is to develop an efﬁcient {SNN} model for epilepsy and epileptic seizure detection using electroencephalograms ({EEGs}), a complicated pattern recognition problem. Three training algorithms are investigated: {SpikeProp} (using both incremental and batch processing), {QuickProp}, and {RProp}. Since the epilepsy and epileptic seizure detection problem requires a large training dataset the efﬁcacy of these algorithms is investigated by ﬁrst applying them to the {XOR} and Fisher iris benchmark problems. Three measures of performance are investigated: number of convergence epochs, computational efﬁciency, and classiﬁcation accuracy. Extensive parametric analysis is performed to identify heuristic rules and optimum parameter values that increase the computational efﬁciency and classiﬁcation accuracy. The result is a remarkable increase in computational efﬁciency. For the {XOR} problem, the computational efﬁciency of {SpikeProp}, {QuickProp}, and {RProp} is increased by a factor of 588, 82, and 75, respectively, compared with the results reported in the literature. {EEGs} from three different subject groups are analyzed: (a) healthy subjects, (b) epileptic subjects during a seizure-free interval, and (c) epileptic subjects during a seizure. It is concluded that {RProp} is the best training algorithm because it has the highest classiﬁcation accuracy among all training algorithms specially for large size training datasets with about the same computational efﬁciency provided by {SpikeProp}. The {SNN} model for {EEG} classiﬁcation and epilepsy and seizure detection uses {RProp} as training algorithm. This model yields a high classiﬁcation accuracy of 92.5\%.},
	pages = {187--212},
	number = {3},
	journaltitle = {Integrated Computer-Aided Engineering},
	author = {Ghosh-Dastidar, Samanwoy and Adeli, Hojjat},
	urldate = {2020-01-09},
	date = {2007-05-13},
	langid = {english},
	file = {Ghosh-Dastidar and Adeli - 2007 - Improved spiking neural networks for EEG classific.pdf:/home/danielsan/Zotero/storage/B7BWAN7U/Ghosh-Dastidar and Adeli - 2007 - Improved spiking neural networks for EEG classific.pdf:application/pdf}
}

@article{sloss_2019_2019,
	title = {2019 Evolutionary Algorithms Review},
	url = {http://arxiv.org/abs/1906.08870},
	abstract = {Evolutionary algorithm research and applications began over 50 years ago. Like other artiﬁcial intelligence techniques, evolutionary algorithms will likely see increased use and development due to the increased availability of computation, more robust and available open source software libraries, and the increasing demand for artiﬁcial intelligence techniques. As these techniques become more adopted and capable, it is the right time to take a perspective of their ability to integrate into society and the human processes they intend to augment. In this review, we explore a new taxonomy of evolutionary algorithms and resulting classiﬁcations that look at ﬁve main areas: the ability to manage the control of the environment with limiters, the ability to explain and repeat the search process, the ability to understand input and output causality within a solution, the ability to manage algorithm bias due to data or user design, and lastly, the ability to add corrective measures. These areas are motivated by today’s pressures on industry to conform to both societies concerns and new government regulatory rules. As many reviews of evolutionary algorithms exist, after motivating this new taxonomy, we brieﬂy classify a broad range of algorithms and identify areas of future research.},
	journaltitle = {{arXiv}:1906.08870 [cs]},
	author = {Sloss, Andrew N. and Gustafson, Steven},
	urldate = {2020-01-12},
	date = {2019-06-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.08870},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {Sloss and Gustafson - 2019 - 2019 Evolutionary Algorithms Review.pdf:/home/danielsan/Zotero/storage/KSH6FN6M/Sloss and Gustafson - 2019 - 2019 Evolutionary Algorithms Review.pdf:application/pdf}
}

@book{wilson_animat_1991,
	title = {The Animat Path to {AI}},
	abstract = {A research methodology is proposed for understanding intelligence through simulation of artificial animals ("animats") in progressively more challenging environments while retaining characteristics of holism, pragmatism, perception, categorization, and adaptation that are often underrepresented in standard {AI} approaches to intelligence. It is suggested that basic elements of the methodology should include a theory/taxonomy of environments by which they can be ordered in difficulty---one is offered---and a theory of animat efficiency. It is also suggested that the methodology offers a new approach to the problem of perception.},
	author = {Wilson, S. W.},
	date = {1991},
	file = {Citeseer - Full Text PDF:/home/danielsan/Zotero/storage/R8GMQVLP/Wilson - 1991 - The Animat Path to AI.pdf:application/pdf;Citeseer - Snapshot:/home/danielsan/Zotero/storage/K3WFBE2L/summary.html:text/html}
}

@inproceedings{franklin_is_1997,
	location = {Berlin, Heidelberg},
	title = {Is It an agent, or just a program?: A taxonomy for autonomous agents},
	isbn = {978-3-540-68057-4},
	doi = {10.1007/BFb0013570},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Is It an agent, or just a program?},
	abstract = {The advent of software agents gave rise to much discussion of just what such an agent is, and of how they differ from programs in general. Here we propose a formal definition of an autonomous agent which clearly distinguishes a software agent from just any program. We also offer the beginnings of a natural kinds taxonomy of autonomous agents, and discuss possibilities for further classification. Finally, we discuss subagents and multiagent systems.},
	pages = {21--35},
	booktitle = {Intelligent Agents {III} Agent Theories, Architectures, and Languages},
	publisher = {Springer},
	author = {Franklin, Stan and Graesser, Art},
	editor = {Müller, Jörg P. and Wooldridge, Michael J. and Jennings, Nicholas R.},
	date = {1997},
	langid = {english},
	keywords = {Autonomous Agent, Intelligent Agent, Mobile Agent, Multiagent System, Natural Kind},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/PKZ8CHDQ/Franklin and Graesser - 1997 - Is It an agent, or just a program A taxonomy for.pdf:application/pdf}
}

@article{floreano_automatic_1994,
	title = {Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural Network Driven Robot},
	url = {http://hdl.handle.net/20.500.11850/82611},
	doi = {10.3929/ethz-a-010111549},
	shorttitle = {Automatic Creation of an Autonomous Agent},
	abstract = {The paper describes the results of the evolutionary development of a real, neural-network driven mobile robot. The evolutionary approach to the development of neural controllers for autonomous agents has been successfully used by many researchers, but most -if not all- studies have been carried out with computer simulations. Instead, in this research the whole evolutionary process takes places entirely on a real robot without human intervention. Although the experiments described here tackle a simple task of navigation and obstacle avoidance, we show a number of emergent phenomena that are characteristic of autonomous agents. The neural controllers of the evolved best individuals display a full exploitation of non-linear and recurrent connections that make them more e cient than analogous man-designed agents. In order to fully understand and describe the robot behavior, we have also employed quantitative ethological tools 13], and showed that the adaptation dynamics conform to predictions made for animals.},
	journaltitle = {{ETH} Zurich},
	author = {Floreano, Dario and Mondada, Francesco},
	urldate = {2020-01-12},
	date = {1994},
	langid = {english},
	file = {Floreano and Mondada - 1994 - Automatic Creation of an Autonomous Agent Genetic.pdf:/home/danielsan/Zotero/storage/FR5PYZRY/Floreano and Mondada - 1994 - Automatic Creation of an Autonomous Agent Genetic.pdf:application/pdf}
}

@incollection{turing_computing_2009,
	location = {Dordrecht},
	title = {Computing Machinery and Intelligence},
	isbn = {978-1-4020-6710-5},
	url = {https://doi.org/10.1007/978-1-4020-6710-5_3},
	abstract = {I propose to consider the question, “Can machines think?”♣ This should begin with definitions of the meaning of the terms “machine” and “think”. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words “machine” and “think” are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, “Can machines think?” is to be sought in a statistical survey such as a Gallup poll.},
	pages = {23--65},
	booktitle = {Parsing the Turing Test: Philosophical and Methodological Issues in the Quest for the Thinking Computer},
	publisher = {Springer Netherlands},
	author = {Turing, Alan M.},
	editor = {Epstein, Robert and Roberts, Gary and Beber, Grace},
	urldate = {2020-01-12},
	date = {2009},
	langid = {english},
	doi = {10.1007/978-1-4020-6710-5_3},
	keywords = {Computing Machinery, Digital Computer, Performance Capacity, Real Robot, Turing Machine},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/8C5EYRXW/Turing - 2009 - Computing Machinery and Intelligence.pdf:application/pdf}
}

@article{haun_why_2019,
	title = {Why Does Space Feel the Way it Does? Towards a Principled Account of Spatial Experience},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/21/12/1160},
	doi = {10.3390/e21121160},
	shorttitle = {Why Does Space Feel the Way it Does?},
	abstract = {There must be a reason why an experience feels the way it does. A good place to begin addressing this question is spatial experience, because it may be more penetrable by introspection than other qualities of consciousness such as color or pain. Moreover, much of experience is spatial, from that of our body to the visual world, which appears as if painted on an extended canvas in front of our eyes. Because it is \&lsquo;right there\&rsquo;, we usually take space for granted and overlook its qualitative properties. However, we should realize that a great number of phenomenal distinctions and relations are required for the canvas of space to feel \&lsquo;extended\&rsquo;. Here we argue that, to be experienced as extended, the canvas of space must be composed of countless spots, here and there, small and large, and these spots must be related to each other in a characteristic manner through connection, fusion, and inclusion. Other aspects of the structure of spatial experience follow from extendedness: every spot can be experienced as enclosing a particular region, with its particular location, size, boundary, and distance from other spots. We then propose an account of the phenomenal properties of spatial experiences based on integrated information theory ({IIT}). The theory provides a principled approach for characterizing both the quantity and quality of experience by unfolding the cause-effect structure of a physical substrate. Specifically, we show that a simple simulated substrate of units connected in a grid-like manner yields a cause-effect structure whose properties can account for the main properties of spatial experience. These results uphold the hypothesis that our experience of space is supported by brain areas whose units are linked by a grid-like connectivity. They also predict that changes in connectivity, even in the absence of changes in activity, should lead to a warping of experienced space. To the extent that this approach provides an initial account of phenomenal space, it may also serve as a starting point for investigating other aspects of the quality of experience and their physical correspondents.},
	pages = {1160},
	number = {12},
	journaltitle = {Entropy},
	author = {Haun, Andrew and Tononi, Giulio},
	urldate = {2020-01-12},
	date = {2019-12},
	langid = {english},
	keywords = {causal structure, consciousness, grid networks, integrated information theory, phenomenology, qualia},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/DWY4MN6S/Haun and Tononi - 2019 - Why Does Space Feel the Way it Does Towards a Pri.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/MV4PMTPP/1160.html:text/html}
}

@article{haenlein_brief_2019,
	title = {A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence},
	volume = {61},
	issn = {0008-1256},
	url = {https://doi.org/10.1177/0008125619864925},
	doi = {10.1177/0008125619864925},
	shorttitle = {A Brief History of Artificial Intelligence},
	abstract = {This introduction to this special issue discusses artificial intelligence ({AI}), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on {AI}, authored by several of the world’s leading experts and specialists in {AI}. It concludes by offering a comprehensive outlook on the future of {AI}, drawing on micro-, meso-, and macro-perspectives.},
	pages = {5--14},
	number = {4},
	journaltitle = {California Management Review},
	shortjournal = {California Management Review},
	author = {Haenlein, Michael and Kaplan, Andreas},
	urldate = {2020-01-13},
	date = {2019-08-01},
	langid = {english},
	keywords = {artificial intelligence, big data, machine-based learning, regulation, strategy},
	file = {SAGE PDF Full Text:/home/danielsan/Zotero/storage/QFVUB69E/Haenlein and Kaplan - 2019 - A Brief History of Artificial Intelligence On the.pdf:application/pdf}
}

@article{cowan_discussion_1990,
	title = {Discussion: {McCulloch}-Pitts and related neural nets from 1943 to 1989},
	volume = {52},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02459569},
	doi = {10.1007/BF02459569},
	shorttitle = {Discussion},
	abstract = {The {McCulloch}-Pitts paper “A Logical Calculus of the Ideas Immanent in Nervous Activity” was published in {theBulletin} of Mathematical Biophysics in 1943, a decade before the work of Hodgkin, Huxley, Katz and Eccles. The {McCulloch}-Pitts neuron is an extremely simplified representation of neural properties, based simply on the existence of a threshold for the activation of an action potential.},
	pages = {73--97},
	number = {1},
	journaltitle = {Bulletin of Mathematical Biology},
	shortjournal = {Bltn Mathcal Biology},
	author = {Cowan, Jack D.},
	urldate = {2020-01-13},
	date = {1990-01-01},
	langid = {english},
	keywords = {Associative Memory, Logical Function, Motor Unit, Receptive Field, Synaptic Weight},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/M2AVHSTU/Cowan - 1990 - Discussion McCulloch-Pitts and related neural net.pdf:application/pdf}
}

@article{burkitt_review_2006,
	title = {A Review of the Integrate-and-fire Neuron Model: I. Homogeneous Synaptic Input},
	volume = {95},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-006-0068-6},
	doi = {10.1007/s00422-006-0068-6},
	shorttitle = {A Review of the Integrate-and-fire Neuron Model},
	abstract = {The integrate-and-fire neuron model is one of the most widely used models for analyzing the behavior of neural systems. It describes the membrane potential of a neuron in terms of the synaptic inputs and the injected current that it receives. An action potential (spike) is generated when the membrane potential reaches a threshold, but the actual changes associated with the membrane voltage and conductances driving the action potential do not form part of the model. The synaptic inputs to the neuron are considered to be stochastic and are described as a temporally homogeneous Poisson process. Methods and results for both current synapses and conductance synapses are examined in the diffusion approximation, where the individual contributions to the postsynaptic potential are small. The focus of this review is upon the mathematical techniques that give the time distribution of output spikes, namely stochastic differential equations and the Fokker–Planck equation. The integrate-and-fire neuron model has become established as a canonical model for the description of spiking neurons because it is capable of being analyzed mathematically while at the same time being sufficiently complex to capture many of the essential features of neural processing. A number of variations of the model are discussed, together with the relationship with the Hodgkin–Huxley neuron model and the comparison with electrophysiological data. A brief overview is given of two issues in neural information processing that the integrate-and-fire neuron model has contributed to – the irregular nature of spiking in cortical neurons and neural gain modulation.},
	pages = {1--19},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Burkitt, A. N.},
	urldate = {2020-01-13},
	date = {2006-07-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/DRGJW4DA/Burkitt - 2006 - A Review of the Integrate-and-fire Neuron Model I.pdf:application/pdf}
}

@inproceedings{holt_11_2016,
	title = {1.1 Moore's law: A path going forward},
	doi = {10.1109/ISSCC.2016.7417888},
	shorttitle = {1.1 Moore's law},
	abstract = {Semiconductors continue to be the foundation for computing and communications solutions, the basis of the Internet of Everthing, and the primary driver in the future of electronics applications. Moore's Law has led to evermore-powerful smart phones, tablets, personal computers, and data centers. It has enabled computing to become a seamless and powerful force in our homes, offices, cars, factories, and much more. Much has been written about the end of Moore's Law. More recently, speculation has focused on the economic end of Moore's Law. Gordon Moore initially projected 10 years of visibility. [35] Over fifty years later, the Moore's Law horizon remains around 10 years. Moore's Law was never guaranteed. It has thrived and will continue to do so as the result of continuous innovation, rigorous planning, and technology execution. Even though it is getting more expensive to build wafers, improvements in density can provide real cost reduction at the most fundamental level, and this economic benefit drives the ability to continue investing in Moore's Law. Innovations have driven Moore's Law through numerous technological transitions and will continue to power us into the future of {CMOS} and beyond. As long as there is a cost benefit and rich options for future innovations there is no reason to predict an early end!},
	eventtitle = {2016 {IEEE} International Solid-State Circuits Conference ({ISSCC})},
	pages = {8--13},
	booktitle = {2016 {IEEE} International Solid-State Circuits Conference ({ISSCC})},
	author = {Holt, William M.},
	date = {2016-01},
	note = {{ISSN}: 2376-8606},
	keywords = {Bandwidth, {CMOS} future and beyond, {CMOS} integrated circuits, {CMOS} technology, data centers, electronic engineering computing, electronics applications, Gordon Moore, Internet of Everthing, Moores law, personal computers, semiconductor technology, Silicon, smart phones, tablets, Three-dimensional displays},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/LQ6RTSS7/7417888.html:text/html}
}

@online{noauthor_is_nodate,
	title = {Is Moore’s Law Slowing Down? What’s Next?},
	url = {https://www.computer.org/csdl/magazine/mi/2017/04/mmi2017040004/13rRUxBa5gP},
	urldate = {2020-01-13},
	file = {Is Moore’s Law Slowing Down? What’s Next?:/home/danielsan/Zotero/storage/B4D5TDLJ/13rRUxBa5gP.html:text/html}
}

@article{schuman_survey_2017,
	title = {A Survey of Neuromorphic Computing and Neural Networks in Hardware},
	url = {http://arxiv.org/abs/1705.06963},
	abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brain-like ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
	journaltitle = {{arXiv}:1705.06963 [cs]},
	author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
	urldate = {2020-01-13},
	date = {2017-05-19},
	eprinttype = {arxiv},
	eprint = {1705.06963},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/danielsan/Zotero/storage/BKYL7USS/Schuman et al. - 2017 - A Survey of Neuromorphic Computing and Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/home/danielsan/Zotero/storage/FTWRVVVP/1705.html:text/html}
}

@book{eiben_introduction_2015,
	location = {Heidelberg},
	edition = {2. ed},
	title = {Introduction to evolutionary computing},
	isbn = {978-3-662-44873-1 978-3-662-44874-8},
	series = {Natural computing series},
	pagetotal = {287},
	publisher = {Springer},
	author = {Eiben, Agoston E. and Smith, James E.},
	date = {2015},
	langid = {english},
	note = {{OCLC}: 934627991},
	file = {Eiben and Smith - 2015 - Introduction to evolutionary computing.pdf:/home/danielsan/Zotero/storage/FQ4VHIBN/Eiben and Smith - 2015 - Introduction to evolutionary computing.pdf:application/pdf}
}

@book{trefzer_evolvable_2015,
	location = {Berlin Heidelberg},
	title = {Evolvable hardware: from practice to application},
	isbn = {978-3-662-44615-7 978-3-662-44616-4},
	series = {Natural computing series},
	shorttitle = {Evolvable hardware},
	pagetotal = {411},
	publisher = {Springer},
	author = {Trefzer, Martin Albrecht and Tyrrell, Andrew M.},
	date = {2015},
	langid = {english},
	file = {Trefzer and Tyrrell - 2015 - Evolvable hardware from practice to application.pdf:/home/danielsan/Zotero/storage/QITPJAQY/Trefzer and Tyrrell - 2015 - Evolvable hardware from practice to application.pdf:application/pdf}
}

@article{ghosh-dastidar_spiking_2009,
	title = {Spiking neural networks},
	volume = {19},
	issn = {0129-0657},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0129065709002002},
	doi = {10.1142/S0129065709002002},
	abstract = {Most current Artificial Neural Network ({ANN}) models are based on highly simplified brain dynamics. They have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. {ANNs} have been evolving towards more powerful and more biologically realistic models. In the past decade, Spiking Neural Networks ({SNNs}) have been developed which comprise of spiking neurons. Information transfer in these neurons mimics the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. To facilitate learning in such networks, new learning algorithms based on varying degrees of biological plausibility have also been developed recently. Addition of the temporal dimension for information encoding in {SNNs} yields new insight into the dynamics of the human brain and could result in compact representations of large neural networks. As such, {SNNs} have great potential for solving complicated time-dependent pattern recognition problems because of their inherent dynamic representation. This article presents a state-of-the-art review of the development of spiking neurons and {SNNs}, and provides insight into their evolution as the third generation neural networks.},
	pages = {295--308},
	number = {4},
	journaltitle = {International Journal of Neural Systems},
	shortjournal = {Int. J. Neur. Syst.},
	author = {Ghosh-Dastidar, Samanwoy and Adeli, Hojjat},
	urldate = {2020-01-13},
	date = {2009-08-01},
	file = {Snapshot:/home/danielsan/Zotero/storage/Q4EXUJGG/S0129065709002002.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/L6AXNH8H/Ghosh-Dastidar and Adeli - 2009 - Spiking neural networks.pdf:application/pdf}
}

@article{cao_spiking_2015,
	title = {Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition},
	volume = {113},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0788-3},
	doi = {10.1007/s11263-014-0788-3},
	abstract = {Deep-learning neural networks such as convolutional neural network ({CNN}) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks ({SNN})-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep {CNN} into a {SNN} that enables mapping {CNN} to spike-based hardware architectures. Our approach first tailors the {CNN} architecture to fit the requirements of {SNN}, then trains the tailored {CNN} in the same way as one would with {CNN}, and finally applies the learned network weights to an {SNN} architecture derived from the tailored {CNN}. We evaluate the resulting {SNN} on publicly available Defense Advanced Research Projects Agency ({DARPA}) Neovision2 Tower and {CIFAR}-10 datasets and show similar object recognition accuracy as the original {CNN}. Our {SNN} implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the {DARPA} {SyNAPSE} program. Our hardware mapping analysis suggests that {SNN} implementation on such spike-based hardware is two orders of magnitude more energy-efficient than the original {CNN} implementation on off-the-shelf {FPGA}-based hardware.},
	pages = {54--66},
	number = {1},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {Int J Comput Vis},
	author = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
	urldate = {2020-01-13},
	date = {2015-05-01},
	langid = {english},
	keywords = {Machine learning, Convolutional neural networks, Deep learning, Neuromorphic circuits, Object recognition, Spiking neural networks},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/QSTIU58F/Cao et al. - 2015 - Spiking Deep Convolutional Neural Networks for Ene.pdf:application/pdf}
}

@article{lee_training_2016,
	title = {Training Deep Spiking Neural Networks Using Backpropagation},
	volume = {10},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2016.00508/full},
	doi = {10.3389/fnins.2016.00508},
	abstract = {Deep spiking neural networks ({SNNs}) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep {SNNs} that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original {MNIST} handwritten digit benchmark, and also on the N-{MNIST} benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous {SNN}, and also achieves a higher accuracy than a conventional convolutional neural network ({CNN}) trained and tested on the same data. We demonstrate in the context of the {MNIST} task that thanks to their event-driven operation, deep {SNNs} (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-{MNIST} example, equivalent accuracy is achieved with about five times fewer computational operations.},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
	urldate = {2020-01-13},
	date = {2016},
	keywords = {backpropagation, Deep neural network, {DVS}, {MNIST}, N-{MNIST}, Neuromorphic, Spiking Neural network},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/2NJ3W5DM/Lee et al. - 2016 - Training Deep Spiking Neural Networks Using Backpr.pdf:application/pdf}
}

@article{rosenblatt_perceptron_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0042519},
	shorttitle = {The perceptron},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {386--408},
	number = {6},
	journaltitle = {Psychological Review},
	author = {Rosenblatt, F.},
	date = {1958},
	keywords = {Brain, Cognition, Memory, Nervous System},
	file = {Snapshot:/home/danielsan/Zotero/storage/Y4K5GM89/1959-09865-001.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/9PMQUQ3Q/Rosenblatt - 1958 - The perceptron A probabilistic model for informat.pdf:application/pdf}
}

@article{newell_perceptrons_1969,
	title = {Perceptrons. An Introduction to Computational Geometry. Marvin Minsky and Seymour Papert. M.I.T. Press, Cambridge, Mass., 1969. vi + 258 pp., illus. Cloth, \$12; paper, \$4.95},
	volume = {165},
	rights = {© 1969},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/165/3895/780},
	doi = {10.1126/science.165.3895.780},
	pages = {780--782},
	number = {3895},
	journaltitle = {Science},
	author = {Newell, Allen},
	urldate = {2020-01-14},
	date = {1969-08-22},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/QFNZCKC8/780.html:text/html}
}

@book{minsky_perceptrons_2017,
	title = {Perceptrons: An Introduction to Computational Geometry},
	isbn = {978-0-262-53477-2},
	shorttitle = {Perceptrons},
	abstract = {The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by Léon {BottouIn} 1969, ten years after the discovery of the perceptron—which showed that a machine could be taught to perform certain tasks using examples—Marvin Minsky and Seymour Papert published Perceptrons, their analysis of the computational capabilities of perceptrons for specific tasks. As Léon Bottou writes in his foreword to this edition, “Their rigorous work and brilliant technique does not make the perceptron look very good.” Perhaps as a result, research turned away from the perceptron. Then the pendulum swung back, and machine learning became the fastest-growing field in computer science. Minsky and Papert's insistence on its theoretical foundations is newly relevant.Perceptrons—the first systematic study of parallelism in computation—marked a historic turn in artificial intelligence, returning to the idea that intelligence might emerge from the activity of networks of neuron-like entities. Minsky and Papert provided mathematical analysis that showed the limitations of a class of computing machines that could be considered as models of the brain. Minsky and Papert added a new chapter in 1987 in which they discuss the state of parallel computers, and note a central theoretical challenge: reaching a deeper understanding of how “objects” or “agents” with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called “society theories of mind.”},
	pagetotal = {317},
	publisher = {{MIT} Press},
	author = {Minsky, Marvin and Papert, Seymour A.},
	date = {2017-09-22},
	langid = {english},
	note = {Google-Books-{ID}: {PLQ}5DwAAQBAJ},
	keywords = {Computers / Computer Science}
}

@book{cicchinelli_frank_1956,
	title = {Frank Rosenblatt publications.},
	abstract = {Cornell Aeronautical Laboratory publications by Frank Rosenblatt include " The Perceptron: A Theory of Statistical Separability in Cognitive Systems," January 1958; Research Trends: "The Design of an Intelligent Automaton," Summer 1958; "Technical Memorandum \#2, An Anaylsis of Very Large Perceptrons in a Finite Universe," October 1958; "Two Theories of Statistical Separability in the Perceptron", November 1958; and Cognitive Systems Research Program, Report No. 4 Collected Technical Papers Volume 2, July 30, 1963. Also mimeographs of "Digital Application Series No. 2, Control Engineering, January 1956, Solving Scientific Problems" and "Electronic Digital Machines," by A. I. Kitov, Government Publishing House, Moscow, 1956 (2 pp.). Also a list of publications by Rosenblatt.},
	author = {Cicchinelli, Alexander L and Rosenblatt, Frank and Kitov, A. I and {Cornell Aeronautical Laboratory}},
	date = {1956},
	note = {{OCLC}: 64057171}
}

@online{noauthor_perceptrons_nodate,
	title = {Perceptrons : Marvin L. Minsky : Free Download, Borrow, and Streaming : Internet Archive},
	url = {https://archive.org/details/Perceptrons},
	urldate = {2020-01-14},
	file = {Perceptrons \: Marvin L. Minsky \: Free Download, Borrow, and Streaming \: Internet Archive:/home/danielsan/Zotero/storage/UNK693T2/Perceptrons.html:text/html}
}

@article{brown_legacy_2003,
	title = {The legacy of Donald O. Hebb: more than the Hebb Synapse},
	volume = {4},
	rights = {2003 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1257},
	doi = {10.1038/nrn1257},
	shorttitle = {The legacy of Donald O. Hebb},
	abstract = {Neuroscientists associate the name of Donald O. Hebb with the Hebbian synapse and the Hebbian learning rule, which underlie connectionist theories and synaptic plasticity, but Hebb's work has also influenced developmental psychology, neuropsychology, perception and the study of emotions, as well as learning and memory. Here, we review the work of Hebb and its lasting influence on neuroscience in honour of the 2004 centenary of his birth.},
	pages = {1013--1019},
	number = {12},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Brown, Richard E. and Milner, Peter M.},
	urldate = {2020-01-14},
	date = {2003-12},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/SUFMQ3I8/nrn1257.html:text/html}
}

@article{von_neumann_first_1993,
	title = {First draft of a report on the {EDVAC}},
	volume = {15},
	issn = {1934-1547},
	doi = {10.1109/85.238389},
	abstract = {The first draft of a report on the {EDVAC} written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine ({ACE}) as the definitive source for understanding the nature and design of a general-purpose digital computer.{\textless}{\textgreater}},
	pages = {27--75},
	number = {4},
	journaltitle = {{IEEE} Annals of the History of Computing},
	author = {von Neumann, J.},
	date = {1993},
	keywords = {automatic computing engine, digital computers, {EDVAC}, Electrical engineering, Engines, Forward contracts, general-purpose digital computer, history, History, Laboratories, Mathematics, Pain, Physics computing, Proposals, Statistics},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/ZLITNL3J/238389.html:text/html}
}

@online{lecun_gradient-based_1998,
	title = {Gradient-Based Learning Applied to Document Recognition},
	url = {http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf},
	author = {{LeCun}, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	urldate = {2019-11-22},
	date = {1998-11},
	file = {lecun-01a.pdf:/home/danielsan/Zotero/storage/4MSEHAVD/lecun-01a.pdf:application/pdf}
}

@article{agrawal_simple_nodate,
	title = {The Simple Economics of Machine Intelligence},
	pages = {3},
	author = {Agrawal, Ajay and Gans, Joshua and Goldfarb, Avi},
	langid = {english},
	file = {Agrawal et al. - The Simple Economics of Machine Intelligence.pdf:/home/danielsan/Zotero/storage/XUW34JTS/Agrawal et al. - The Simple Economics of Machine Intelligence.pdf:application/pdf}
}

@article{agrawal_what_nodate,
	title = {What to Expect From Artificial Intelligence},
	pages = {9},
	journaltitle = {{MIT} {SLOAN} {MANAGEMENT} {REVIEW}},
	author = {Agrawal, Ajay and Gans, Joshua S and Goldfarb, Avi},
	langid = {english},
	file = {Agrawal et al. - What to Expect From Artificial Intelligence.pdf:/home/danielsan/Zotero/storage/P63KKZ9S/Agrawal et al. - What to Expect From Artificial Intelligence.pdf:application/pdf}
}

@article{agrawal_human_2018,
	title = {Human Judgment and {AI} Pricing},
	volume = {108},
	issn = {2574-0768},
	url = {https://www.aeaweb.org/articles?id=10.1257/pandp.20181022},
	doi = {10.1257/pandp.20181022},
	abstract = {This paper examines the pricing choices of a provider of artificial intelligence ({AI}) services. It does so in the context of {AI} providing predictions to a decision-maker who also exercises what we term judgment; specifically, the discovery of payoffs from action/state pairs. An {AI} facilitates the decision-maker obtaining judgment through experience, which is one source of demand for {AI} services. The other source is prediction when (and if) the decision-maker has a need for state-contingent decision-making. We show that the need to encourage learning means that the {AI} provider is constrained in its ability to extract rents from decision-makers.},
	pages = {58--63},
	journaltitle = {{AEA} Papers and Proceedings},
	author = {Agrawal, Ajay and Gans, Joshua S. and Goldfarb, Avi},
	urldate = {2020-01-18},
	date = {2018-05},
	langid = {english},
	keywords = {Belief, Communication, Information and Knowledge, Labor Productivity, {IT} Management, Learning, Occupational Choice, Search, Skills, Unawareness, Human Capital},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/T2NW5MNW/Agrawal et al. - 2018 - Human Judgment and AI Pricing.pdf:application/pdf;Snapshot:/home/danielsan/Zotero/storage/8QHSDIJE/articles.html:text/html}
}

@report{agrawal_finding_2018,
	title = {Finding Needles in Haystacks: Artificial Intelligence and Recombinant Growth},
	url = {http://www.nber.org/papers/w24541},
	shorttitle = {Finding Needles in Haystacks},
	abstract = {Innovation is often predicated on discovering useful new combinations of existing knowledge in highly complex knowledge spaces. These needle-in-a-haystack type problems are pervasive in fields like genomics, drug discovery, materials science, and particle physics. We develop a combinatorial-based knowledge production function and embed it in the classic Jones growth model (1995) to explore how breakthroughs in artificial intelligence ({AI}) that dramatically improve prediction accuracy about which combinations have the highest potential could enhance discovery rates and consequently economic growth. This production function is a generalization (and reinterpretation) of the Romer/Jones knowledge production function. Separate parameters control the extent of individual-researcher knowledge access, the effects of fishing out/complexity, and the ease of forming research teams.},
	number = {24541},
	institution = {National Bureau of Economic Research},
	type = {Working Paper},
	author = {Agrawal, Ajay and {McHale}, John and Oettl, Alex},
	urldate = {2020-01-18},
	date = {2018-04},
	doi = {10.3386/w24541},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/ULB239X5/Agrawal et al. - 2018 - Finding Needles in Haystacks Artificial Intellige.pdf:application/pdf}
}

@article{heath_prediction_2019,
	title = {Prediction machines: the simple economics of artificial intelligence: by Ajay Agrawal, Joshua Gans and Avi Goldfarb, Published in 2018 by Harvard Business Review Press, 272 pp., \$30.00 (hardcover), Kindle Edition: \$16.19, {ISBN}:978-1-633695672},
	issn = {1522-8053, 2333-6897},
	url = {https://www.tandfonline.com/doi/full/10.1080/15228053.2019.1673511},
	doi = {10.1080/15228053.2019.1673511},
	shorttitle = {Prediction machines},
	pages = {1--4},
	journaltitle = {Journal of Information Technology Case and Application Research},
	author = {Heath, Donald R.},
	urldate = {2020-01-18},
	date = {2019-10-01},
	langid = {english},
	file = {Heath - 2019 - Prediction machines the simple economics of artif.pdf:/home/danielsan/Zotero/storage/S76XR5HY/Heath - 2019 - Prediction machines the simple economics of artif.pdf:application/pdf}
}

@article{sarasvathy_what_nodate,
	title = {What makes entrepreneurs entrepreneurial?},
	pages = {9},
	author = {Sarasvathy, Saras D},
	langid = {english},
	file = {Sarasvathy - What makes entrepreneurs entrepreneurial.pdf:/home/danielsan/Zotero/storage/2VWGLJPX/Sarasvathy - What makes entrepreneurs entrepreneurial.pdf:application/pdf}
}

@online{noauthor_causation_nodate,
	title = {Causation and Effectuation: Toward a Theoretical Shift from Economic Inevitability to Entrepreneurial Contingency {\textbar} Academy of Management Review},
	url = {https://journals.aom.org/doi/abs/10.5465/amr.2001.4378020},
	urldate = {2020-01-18},
	file = {Causation and Effectuation\: Toward a Theoretical Shift from Economic Inevitability to Entrepreneurial Contingency | Academy of Management Review:/home/danielsan/Zotero/storage/R3YEZWZY/amr.2001.html:text/html}
}

@article{sarasvathy_causation_nodate,
	title = {Causation and Effectuation: Toward a Theoretical Shift from Economic Inevitability to Entrepreneurial Contingency},
	pages = {22},
	author = {Sarasvathy, Saras D},
	langid = {english},
	file = {Sarasvathy - Causation and Effectuation Toward a Theoretical S.pdf:/home/danielsan/Zotero/storage/MTGBPC7E/Sarasvathy - Causation and Effectuation Toward a Theoretical S.pdf:application/pdf}
}

@article{karri_effectuation_2008,
	title = {Effectuation and Over–Trust: Response to Sarasvathy and Dew},
	volume = {32},
	issn = {1042-2587},
	url = {https://journals.sagepub.com/doi/abs/10.1111/j.1540-6520.2008.00251.x},
	doi = {10.1111/j.1540-6520.2008.00251.x},
	shorttitle = {Effectuation and Over–Trust},
	abstract = {In their response to our article, Sarasvathy and Dew (S\&D) agree with us that effectuation supposes over–trust and yet claim that trust is irrelevant to an effectual entrepreneur. They further claim that our approach to entrepreneurship is trait based. We respond to these comments by pointing out the more subtle ways in which entrepreneurs deal with trust. In addition, while acknowledging the utility (and limitations) of a trait–based approach in advancing entrepreneurship theory, we refute their assertions that our paper is based on this approach. Finally, we address the “alternate” behavioral assumptions that S\&D advance. Independent of the merit of these alternate assumptions, they are not contradicted in our article. We believe that these assumptions need to be developed further to contribute to a debate on their merits for advancing theory building in entrepreneurship.},
	pages = {739--748},
	number = {4},
	journaltitle = {Entrepreneurship Theory and Practice},
	shortjournal = {Entrepreneurship Theory and Practice},
	author = {Karri, Ranjan and Goel, Sanjay},
	urldate = {2020-01-18},
	date = {2008-07-01},
	langid = {english},
	file = {SAGE PDF Full Text:/home/danielsan/Zotero/storage/62D6WN9S/Karri and Goel - 2008 - Effectuation and Over–Trust Response to Sarasvath.pdf:application/pdf}
}

@article{read_knowing_2005,
	title = {Knowing What to Do and Doing What You Know: Effectuation as a Form of Entrepreneurial Expertise},
	volume = {9},
	rights = {© 2005 Pageant Media Ltd},
	issn = {1096-5572, 2168-8508},
	url = {https://jpe.pm-research.com/content/9/1/45},
	doi = {10.3905/jpe.2005.605370},
	shorttitle = {Knowing What to Do and Doing What You Know},
	abstract = {Entrepreneurship has traditionally been studied either as a set of psychological characteristics or as a residual of environmental structures such as social networks. In line with more recent process views, the authors propose the study of entrepreneurship as a form of expertise—a set of skills, models, and processes that can be acquired with time and deliberate practice. This article delineates the domain of entrepreneurial expertise and demarcates the role of deliberate practice within it, demonstrates the efficacy of effectuation as a theory about entrepreneurial expertise, and develops testable propositions about effectual action in the development of entrepreneurial expertise and firm growth.},
	pages = {45--62},
	number = {1},
	journaltitle = {The Journal of Private Equity},
	author = {Read, Stuart and Sarasvathy, Saras D.},
	urldate = {2020-01-18},
	date = {2005-11-30},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/H33HXM4W/45.html:text/html}
}

@article{chiles_lachmannian_2008,
	title = {On Lachmannian and Effectual Entrepreneurship: A Rejoinder to Sarasvathy and Dew (2008)},
	volume = {29},
	issn = {0170-8406, 1741-3044},
	url = {http://journals.sagepub.com/doi/10.1177/0170840607088154},
	doi = {10.1177/0170840607088154},
	shorttitle = {On Lachmannian and Effectual Entrepreneurship},
	pages = {247--253},
	number = {2},
	journaltitle = {Organization Studies},
	author = {Chiles, Todd H. and Gupta, Vishal K. and Bluedorn, Allen C.},
	urldate = {2020-01-18},
	date = {2008-02},
	langid = {english},
	file = {Chiles et al. - 2008 - On Lachmannian and Effectual Entrepreneurship A R.pdf:/home/danielsan/Zotero/storage/IIU4TA7Q/Chiles et al. - 2008 - On Lachmannian and Effectual Entrepreneurship A R.pdf:application/pdf}
}

@article{kamishima_can_2018,
	title = {Can Merging a Capability Approach with Effectual Processes Help Us Define a Permissible Action Range for {AI} Robotics Entrepreneurship?},
	volume = {17},
	issn = {2052-9597},
	url = {https://doi.org/10.1007/s40926-017-0059-9},
	doi = {10.1007/s40926-017-0059-9},
	abstract = {In this paper, we first enumerate the problems that humans might face with a new type of technology such as robots with artificial intelligence ({AI} robots). Robotics entrepreneurs are calling for discussions about goals and values because {AI} robots, which are potentially more intelligent than humans, can no longer be fully understood and controlled by humans. {AI} robots could even develop into ethically “bad” agents and become very harmful. We consider these discussions as part of a process of developing responsible innovations in {AI} robotics in order to prevent catastrophic risks on a global scale. To deal with these issues, we propose the capability-effectual approach, drawing on two bodies of research: the capability approach from ethics, and the effectual process model from entrepreneurship research. The capability approach provides central human capabilities, guiding the effectual process through individual goals and aspirations in the collaborative design process of stakeholders. More precisely, by assuming and understanding correspondences between goals, purposes, desires, and aspirations in the languages of different disciplines, the capability-effectual approach clarifies both how a capability list working globally could affect the aspirations and end-goals of individuals, and how local aspirations and end-goals could either energise or limit effectual processes. Theoretically, the capability-effectual approach links the collaboration of stakeholders and the design process in responsible innovation research. Practically, this approach could potentially contribute to the robust development of {AI} robots by providing robotics entrepreneurs with a tool for establishing a permissible action range within which to develop {AI} robotics.},
	pages = {97--113},
	number = {1},
	journaltitle = {Philosophy of Management},
	shortjournal = {Philosophy of Management},
	author = {Kamishima, Yuko and Gremmen, Bart and Akizawa, Hikari},
	urldate = {2020-01-18},
	date = {2018-02-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/YDAMLCW3/Kamishima et al. - 2018 - Can Merging a Capability Approach with Effectual P.pdf:application/pdf}
}

@incollection{petrovici_artificial_2016,
	location = {Cham},
	title = {Artificial Brains: Simulation and Emulation of Neural Networks},
	isbn = {978-3-319-39552-4},
	url = {https://doi.org/10.1007/978-3-319-39552-4_3},
	series = {Springer Theses},
	shorttitle = {Artificial Brains},
	abstract = {When describing increasingly complex systems, the required array of equations equivalently grows in size and complexity. In many (usually simple) cases, statistical methods can be applied to distill macroscopic equations from those governing the microscopic components of a system, with thermodynamics offering a paradigmatic example.},
	pages = {59--81},
	booktitle = {Form Versus Function: Theory and Models for Neuronal Substrates},
	publisher = {Springer International Publishing},
	author = {Petrovici, Mihai Alexandru},
	editor = {Petrovici, Mihai Alexandru},
	urldate = {2020-01-26},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-39552-4_3},
	keywords = {Synaptic Weight, Biological Domain, Neuron Model, Priority Encoder, Source Neuron}
}

@inproceedings{langeheine_cmos_2001,
	title = {A {CMOS} {FPTA} chip for intrinsic hardware evolution of analog electronic circuits},
	doi = {10.1109/EH.2001.937959},
	abstract = {This paper describes and discusses an intrinsic approach to hardware evolution of analog electronic circuits using a Field Programmable Transistor Array ({FPTA}). The {FPTA} is fabricated in a 0.6 /spl mu/m {CMOS} process and consists of 16/spl times/16 transistor cells. The chip allows to configure the gate geometry as well as the connectivity of each of the 256 transistors. Evolutionary algorithms are to be run on a commercial {PC} to produce the new circuit configurations that are downloaded to the chip via a {PCI} card. In contrast to extrinsic hardware evolution all environmental conditions present on the device under test have to be taken into account by the evolutionary algorithm. Thus a selection pressure is raised towards solutions that actually work on real dice.},
	eventtitle = {Proceedings Third {NASA}/{DoD} Workshop on Evolvable Hardware. {EH}-2001},
	pages = {172--175},
	booktitle = {Proceedings Third {NASA}/{DoD} Workshop on Evolvable Hardware. {EH}-2001},
	author = {Langeheine, J. and Becker, J. and Folling, S. and Meier, K. and Schemmel, J.},
	date = {2001-07},
	note = {{ISSN}: null},
	keywords = {Hardware, evolutionary computation, Analog circuits, analog electronic circuits, {CMOS} analog integrated circuits, {CMOS} analogue integrated circuits, {CMOS} {FPTA} chip, Design automation, Electronic circuits, evolutionary algorithm, Field Programmable Transistor Array, Geometry, hardware evolution, {MOSFETs}, Physics, programmable logic devices, Space exploration, Tuned circuits},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/XQVEJ353/937959.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/E9EDRUWT/Langeheine et al. - 2001 - A CMOS FPTA chip for intrinsic hardware evolution .pdf:application/pdf}
}

@inproceedings{schmitz_speeding_2003,
	location = {Berlin, Heidelberg},
	title = {Speeding up Hardware Evolution: A Coprocessor for Evolutionary Algorithms},
	isbn = {978-3-540-36553-2},
	doi = {10.1007/3-540-36553-2_25},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Speeding up Hardware Evolution},
	abstract = {This paper proposes a coprocessor architecture to speed up hardware evolution. It is designed to be implemented in an {FPGA} with an integrated microprocessor core. The coprocessor resides in the configurable logic, it can execute common genetic operators like crossover and mutation with a targeted data throughput of 420 {MByte}/s. Together with the microprocessor core, a complex evolutionary algorithm can be developed in software, but is processed at the speed of dedicated hardware.},
	pages = {274--285},
	booktitle = {Evolvable Systems: From Biology to Hardware},
	publisher = {Springer},
	author = {Schmitz, Tillmann and Hohmann, Steffen and Meier, Karlheinz and Schemmel, Johannes and Schürmann, Felix},
	editor = {Tyrrell, {AAndy} M. and Haddow, Pauline C. and Torresen, Jim},
	date = {2003},
	langid = {english},
	file = {Submitted Version:/home/danielsan/Zotero/storage/8Y7IDJ4R/Schmitz et al. - 2003 - Speeding up Hardware Evolution A Coprocessor for .pdf:application/pdf}
}

@inproceedings{trefzer_operational_2005,
	location = {Berlin, Heidelberg},
	title = {Operational Amplifiers: An Example for Multi-objective Optimization on an Analog Evolvable Hardware Platform},
	isbn = {978-3-540-28737-7},
	doi = {10.1007/11549703_9},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Operational Amplifiers},
	abstract = {This work tackles the problem of synthesizing transferable and reusable operational amplifiers on a field programmable transistor array: the Heidelberg {FPTA}. A multi-objective evolutionary algorithm is developed, in order to be able to include various specifications of an operational amplifier into the process of circuit synthesis. Additionally, the presented algorithm is designed to preserve the diversity within the population troughout evolution and is therefore able to efficiently explore the design space. Furthermore, the evolved circuits are proven to work on the chip as well as in simulation outside the {FPTA}. Schematics of good solutions are presented and their characteristics are compared to those of basic manually created reference designs.},
	pages = {86--97},
	booktitle = {Evolvable Systems: From Biology to Hardware},
	publisher = {Springer},
	author = {Trefzer, Martin and Langeheine, Jörg and Meier, Karlheinz and Schemmel, Johannes},
	editor = {Moreno, J. Manuel and Madrenas, Jordi and Cosp, Jordi},
	date = {2005},
	langid = {english},
	keywords = {Evolvable Hardware, Longe Wire, Tournament Selection, Transistor Array, Transistor Circuit}
}

@inproceedings{schurmann_towards_2002,
	title = {Towards an artificial neural network framework},
	doi = {10.1109/EH.2002.1029893},
	abstract = {This paper proposes a framework for hardware artificial neural networks ({ANN}) combining scalability with the flexibility of software solutions and the speed of hardware {ANNs}. Our implementation consists of analog neural network blocks realized as {ASICs} configurable to form arbitrary and large networks having simple elementary resources, i.e. synapses and neurons. Scalability is assured by confining the analog processing of the synapses to blocks and using digital signalling between them. With the help of a genetic algorithm we train the network to combine its elementary resources to form variable network building blocks. We demonstrate how three binary input neurons can act as a single 3-bit neuron and how a group of neurons and synapses can be trained to form a 3-bit output neuron with linear and sigmoid activation functions.},
	eventtitle = {Proceedings 2002 {NASA}/{DoD} Conference on Evolvable Hardware},
	pages = {266--273},
	booktitle = {Proceedings 2002 {NASA}/{DoD} Conference on Evolvable Hardware},
	author = {Schurmann, F. and Hohmann, S. and Schemmel, J. and Meier, K.},
	date = {2002-07},
	note = {{ISSN}: null},
	keywords = {genetic algorithm, Neural networks, Neurons, Field programmable gate arrays, Artificial neural networks, neural net architecture, analog neural net, {ANN}, Application specific integrated circuits, artificial neural networks, Digital circuits, digital signalling, flexibility, Network topology, Neural network hardware, scalability, Scalability, Signal processing, variable network building blocks},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/L93MXFNJ/1029893.html:text/html}
}

@inproceedings{langeheine_initial_2001,
	location = {Berlin, Heidelberg},
	title = {Initial Studies of a New {VLSI} Field Programmable Transistor Array},
	isbn = {978-3-540-45443-4},
	doi = {10.1007/3-540-45443-8_6},
	series = {Lecture Notes in Computer Science},
	abstract = {A system for intrinsic hardware evolution of analog electronic circuits is presented.It consists of a {VLSI} chip featuring 16 × 16 programmable transistor cells, an {FPGA} based {PCI} card and a software package for setup and control of the experiment.The {PCI} card serves as a link between the chip and the computer that runs the genetic algorithm to produce the configurations for the Field Programmable Transistor Array ({FPTA}).First measurement results prove chip and system to be working as well as they indicate the tradeoff between performance and configurability.The system is now ready to host a wide variety of evolution experiments.},
	pages = {62--73},
	booktitle = {Evolvable Systems: From Biology to Hardware},
	publisher = {Springer},
	author = {Langeheine, Jörg and Becker, Joachim and Fölling, Simon and Meier, Karlheinz and Schemmel, Johannes},
	editor = {Liu, Yong and Tanaka, Kiyoshi and Iwata, Masaya and Higuchi, Tetsuya and Yasunaga, Moritoshi},
	date = {2001},
	langid = {english},
	file = {Submitted Version:/home/danielsan/Zotero/storage/6WITDC94/Langeheine et al. - 2001 - Initial Studies of a New VLSI Field Programmable T.pdf:application/pdf}
}

@article{langeheine_intrinsic_nodate,
	title = {{INTRINSIC} {EVOLUTION} {OF} {ANALOG} {ELECTRONIC} {CIRCUITS} {USING} A {CMOS} {FPTA} {CHIP}},
	abstract = {This paper surveys the research on intrinsic evolution of analog electronic circuits done at the University of Heidelberg. The aims of the project are discussed with reference to the related ﬁelds of evolvable hardware and analog design automation. A Field Programmable Transistor Array ({FPTA}) is used as the substrate for the artiﬁcial evolution process. It consists of 16 × 16 transistor cells fabricated in a 0.6 µm {CMOS} process. Static as well as dynamic properties of the programmable transistor array are estimated by characterization measurements of the chip. The chip is embedded in an evolution system consisting of a {PC} running the evolutionary algorithm and a {PCI} card that connects the {PC} to the {FPTA} and provides the conversion between digital and analog signals. As case studies the quasi dc behavior of different logic gates as well as a Gaussian output characteristic are evolved.},
	pages = {12},
	author = {Langeheine, Jorg and Meier, Karlheinz and Schemmel, Johannes},
	langid = {english},
	file = {Langeheine et al. - INTRINSIC EVOLUTION OF ANALOG ELECTRONIC CIRCUITS .pdf:/home/danielsan/Zotero/storage/BTXT7BVZ/Langeheine et al. - INTRINSIC EVOLUTION OF ANALOG ELECTRONIC CIRCUITS .pdf:application/pdf}
}

@online{noauthor_intrinsic_nodate,
	title = {Intrinsic evolution of digital-to-analog converters using a {CMOS} {FPTA} chip - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/1310804},
	urldate = {2020-01-26},
	file = {Intrinsic evolution of digital-to-analog converters using a CMOS FPTA chip - IEEE Conference Publication:/home/danielsan/Zotero/storage/Z7TJALNQ/1310804.html:text/html}
}

@inproceedings{langeheine_intrinsic_2004,
	title = {Intrinsic evolution of digital-to-analog converters using a {CMOS} {FPTA} chip},
	doi = {10.1109/EH.2004.1310804},
	abstract = {The work presented here tackles the problem of designing a unipolar 6-bit digital-to-analog converter ({DAC}) with a voltage mode output by hardware evolution. Thereby a field programmable transistor array ({FPTA}) is used as the analog substrate for testing the candidate solutions. The {FPTA} features 256 programmable transistors, whose channel geometry and routing can be configured to form a large variety of transistor level analog circuits. A series of experiments reveals that variations of the output voltage range influence evolution's success more severely than varying the amount of available electronic resources or the geometrical setup. Although a considerable number of runs yield converters with a nonlinearity of less than 1 bit, no {DAC} is found to maintain a nonlinearity of less than 0.5 bits under worst case conditions, as required for a true 6-bit resolution. While the evolved circuits work comparably well at different time scales as well as on different dice, they lack the ability to abstract from the analog voltage levels of the digital input signals. It is experimentally verified that this can be remedied by inserting digital buffers at the circuits' inputs.},
	eventtitle = {Proceedings. 2004 {NASA}/{DoD} Conference on Evolvable Hardware, 2004.},
	pages = {18--25},
	booktitle = {Proceedings. 2004 {NASA}/{DoD} Conference on Evolvable Hardware, 2004.},
	author = {Langeheine, J. and Meier, K. and Schemmel, J. and Trefzer, M.},
	date = {2004-06},
	note = {{ISSN}: null},
	keywords = {Hardware, evolutionary computation, Silicon, {CMOS} analogue integrated circuits, {CMOS} {FPTA} chip, hardware evolution, Physics, 6 bits, analog substrate, analog voltage levels, circuit optimisation, Circuit testing, digital buffers, digital input signals, Digital-analog conversion, digital-analogue conversion, digital-to-analog converters, Field programmable analog arrays, field programmable analogue arrays, field programmable transistor array, Humans, integrated circuit design, {SPICE}, transistor level analog circuits, Transistors, unipolar {DAC}, Voltage, voltage mode output},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/KP9I59TE/1310804.html:text/html}
}

@article{carrillo_scalable_2013,
	title = {Scalable Hierarchical Network-on-Chip Architecture for Spiking Neural Network Hardware Implementations},
	volume = {24},
	issn = {2161-9883},
	doi = {10.1109/TPDS.2012.289},
	abstract = {Spiking neural networks ({SNNs}) attempt to emulate information processing in the mammalian brain based on massively parallel arrays of neurons that communicate via spike events. {SNNs} offer the possibility to implement embedded neuromorphic circuits, with high parallelism and low power consumption compared to the traditional von Neumann computer paradigms. Nevertheless, the lack of modularity and poor connectivity shown by traditional neuron interconnect implementations based on shared bus topologies is prohibiting scalable hardware implementations of {SNNs}. This paper presents a novel hierarchical network-on-chip (H-{NoC}) architecture for {SNN} hardware, which aims to address the scalability issue by creating a modular array of clusters of neurons using a hierarchical structure of low and high-level routers. The proposed H-{NoC} architecture incorporates a spike traffic compression technique to exploit {SNN} traffic patterns and locality between neurons, thus reducing traffic overhead and improving throughput on the network. In addition, adaptive routing capabilities between clusters balance local and global traffic loads to sustain throughput under bursting activity. Analytical results show the scalability of the proposed H-{NoC} approach under different scenarios, while simulation and synthesis analysis using 65-nm {CMOS} technology demonstrate high-throughput, low-cost area, and power consumption per cluster, respectively.},
	pages = {2451--2461},
	number = {12},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Carrillo, Snaider and Harkin, Jim and {McDaid}, Liam J. and Morgan, Fearghal and Pande, Sandeep and Cawley, Seamus and {McGinley}, Brian},
	date = {2013-12},
	keywords = {Computer architecture, neural nets, Neural networks, network routing, {CMOS} technology, Network topology, adaptive routing capabilities, clusters balance local traffic loads, computer architecture, global traffic loads, H-{NoC} architecture, Interconnection architecture, Microprocessors, network synthesis, network-on-chip, neurocomputers, neuron cluster modular array, On chip architectures, real-time distributed, scalable hierarchical network-on-chip architecture, {SNN} hardware, {SNN} traffic patterns, spike traffic compression technique, spiking neural network hardware implementations, spiking neural networks, traffic overhead reduction},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/NJBUQXQH/6322959.html:text/html}
}

@online{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} Establishing a novel modeling tool: a python-based interface for a neuromorphic hardware system {\textbar} Frontiers in Neuroinformatics},
	url = {https://www.frontiersin.org/articles/10.3389/neuro.11.017.2009/full},
	urldate = {2020-01-26},
	file = {Frontiers | Establishing a novel modeling tool\: a python-based interface for a neuromorphic hardware system | Frontiers in Neuroinformatics:/home/danielsan/Zotero/storage/JYZBBWHB/full.html:text/html}
}

@inproceedings{schemmel_implementing_2006,
	title = {Implementing Synaptic Plasticity in a {VLSI} Spiking Neural Network Model},
	doi = {10.1109/IJCNN.2006.246651},
	abstract = {This paper describes an area-efficient mixed-signal implementation of synapse-based long term plasticity realized in a {VLSI} model of a spiking neural network. The artificial synapses are based on an implementation of spike time dependent plasticity ({STDP}). In the biological specimen, {STDP} is a mechanism acting locally in each synapse. The presented electronic implementation succeeds in maintaining this high level of parallelism and simultaneously achieves a synapse density of more than 9k synapses per mm2 in a 180 nm technology. This allows the construction of neural micro-circuits close to the biological specimen while maintaining a speed several orders of magnitude faster than biological real time. The large acceleration factor enhances the possibilities to investigate key aspects of plasticity, e.g. by performing extensive parameter searches.},
	eventtitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
	pages = {1--6},
	booktitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
	author = {Schemmel, J. and Grubl, A. and Meier, K. and Mueller, E.},
	date = {2006-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Neurons, neural chips, Biological system modeling, Artificial neural networks, biological specimen, Biomembranes, Brain modeling, Circuits, Intelligent networks, mixed analogue-digital integrated circuits, neural micro-circuits, Numerical simulation, size 180 nm, spike time dependent plasticity, spiking neural network model, synaptic plasticity, Very large scale integration, {VLSI}},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/XD7YGFTE/1716062.html:text/html}
}

@article{cawley_hardware_2011,
	title = {Hardware spiking neural network prototyping and application},
	volume = {12},
	issn = {1573-7632},
	url = {https://doi.org/10.1007/s10710-011-9130-9},
	doi = {10.1007/s10710-011-9130-9},
	abstract = {{EMBRACE} has been proposed as a scalable, reconfigurable, mixed signal, embedded hardware Spiking Neural Network ({SNN}) device. {EMBRACE}, which is yet to be realised, targets the issues of area, power and scalability through the use of a low area, low power analogue neuron/synapse cell, and a digital packet-based Network on Chip ({NoC}) communication architecture. The paper describes the implementation and testing of {EMBRACE}-{FPGA}, an {FPGA}-based hardware {SNN} prototype. The operation of the {NoC} inter-neuron communication approach and its ability to support large scale, reconfigurable, highly interconnected {SNNs} is illustrated. The paper describes an integrated training and configuration platform and an on-chip fitness function, which supports {GA}-based evolution of {SNN} parameters. The practicalities of using the {SNN} development platform and {SNN} configuration toolset are described. The paper considers the impact of latency jitter noise introduced by the {NoC} router and the {EMBRACE}-{FPGA} processor-based neuron/synapse model on {SNN} accuracy and evolution time. Benchmark {SNN} applications are described and results demonstrate the evolution of high quality and robust solutions in the presence of noise. The reconfigurable {EMBRACE} architecture enables future investigation of adaptive hardware applications and self repair in evolvable hardware.},
	pages = {257--280},
	number = {3},
	journaltitle = {Genetic Programming and Evolvable Machines},
	shortjournal = {Genet Program Evolvable Mach},
	author = {Cawley, Seamus and Morgan, Fearghal and {McGinley}, Brian and Pande, Sandeep and {McDaid}, Liam and Carrillo, Snaider and Harkin, Jim},
	urldate = {2020-01-26},
	date = {2011-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/GCXZG3EV/Cawley et al. - 2011 - Hardware spiking neural network prototyping and ap.pdf:application/pdf}
}

@article{markram_introducing_2011,
	title = {Introducing the Human Brain Project},
	volume = {7},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050911006806},
	doi = {10.1016/j.procs.2011.12.015},
	series = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 ({FET} 11)},
	abstract = {The Human Brain Project ({HBP}) is a candidate project in the European Union's {FET} Flagship Program, funded by the {ICT} Program in the Seventh Framework Program. The project will develop a new integrated strategy for understanding the human brain and a novel research platform that will integrate all the data and knowledge we can acquire about the structure and function of the brain and use it to build unifying models that can be validated by simulations running on supercomputers. The project will drive the development of supercomputing for the life sciences, generate new neuroscientific data as a benchmark for modeling, develop radically new tools for informatics, modeling and simulation, and build virtual laboratories for collaborative basic and clinical studies, drug simulation and virtual prototyping of neuroprosthetic, neuromorphic, and robotic devices.},
	pages = {39--42},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Markram, Henry and Meier, Karlheinz and Lippert, Thomas and Grillner, Sten and Frackowiak, Richard and Dehaene, Stanislas and Knoll, Alois and Sompolinsky, Haim and Verstreken, Kris and {DeFelipe}, Javier and Grant, Seth and Changeux, Jean-Pierre and Saria, Alois},
	urldate = {2020-01-26},
	date = {2011-01-01},
	langid = {english},
	keywords = {{HPC}, Human brain, medicine, modeling, neuroinformatics, neuromorphics, neuroprosthetics, neurorobotics, neuroscience, simulation, supercomputing},
	file = {ScienceDirect Full Text PDF:/home/danielsan/Zotero/storage/EYQFKSQK/Markram et al. - 2011 - Introducing the Human Brain Project.pdf:application/pdf;ScienceDirect Snapshot:/home/danielsan/Zotero/storage/GDC4JQTC/S1877050911006806.html:text/html}
}

@inproceedings{schemmel_wafer-scale_2008,
	title = {Wafer-scale integration of analog neural networks},
	doi = {10.1109/IJCNN.2008.4633828},
	abstract = {This paper introduces a novel design of an artificial neural network tailored for wafer-scale integration. The presented {VLSI} implementation includes continuous-time analog neurons with up to 16 k inputs. A novel interconnection and routing scheme allows the mapping of a multitude of network models derived from biology on the {VLSI} neural network while maintaining a high resource usage. A single 20 cm wafer contains about 60 million synapses. The implemented neurons are highly accelerated compared to biological real time. The power consumption of the dense interconnection network providing the necessary communication bandwidth is a critical aspect of the system integration. A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology.},
	eventtitle = {2008 {IEEE} International Joint Conference on Neural Networks ({IEEE} World Congress on Computational Intelligence)},
	pages = {431--438},
	booktitle = {2008 {IEEE} International Joint Conference on Neural Networks ({IEEE} World Congress on Computational Intelligence)},
	author = {Schemmel, Johannes and Fieres, Johannes and Meier, Karlheinz},
	date = {2008-06},
	note = {{ISSN}: 2161-4407},
	keywords = {Neurons, integrated circuit interconnections, network routing, neural chips, Artificial neural networks, Acceleration, analogue integrated circuits, artificial neural network, asynchronous low-voltage signaling scheme, continuous-time analog neural networks, Driver circuits, interconnection network, interconnection-routing scheme, Metals, power consumption, Power demand, programmable network topology, {VLSI} implementation, wafer-scale integration, Wire},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/TERDPLJH/4633828.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/AALN5HYS/Schemmel et al. - 2008 - Wafer-scale integration of analog neural networks.pdf:application/pdf}
}

@inproceedings{stoica_evolution_2000,
	title = {Evolution of analog circuits on field programmable transistor arrays},
	doi = {10.1109/EH.2000.869347},
	abstract = {Evolvable Hardware ({EHW}) refers to {HW} design and self reconfiguration using evolutionary/genetic mechanisms. The paper presents an overview of some key concepts of {EHW}, describing also a set of selected applications. A fine-grained Field Programmable Transistor Array ({FPTA}) architecture for reconfigurable hardware is presented as an example of an initial effort toward evolution-oriented devices. Evolutionary experiments in simulations and with a {FPTA} chip in-the-loop demonstrate automatic synthesis of electronic circuits. Unconventional circuits, for which there are no textbook design guidelines, are particularly appealing to evolvable hardware. To illustrate this situation, one demonstrates here the evolution of circuits implementing parametrical connectives for fuzzy logics. In addition to synthesizing circuits for new functions, evolvable hardware can be used to preserve existing functions and achieve fault-tolerance, determining circuit configurations that circumvent the faults. In addition, we illustrate with an example how evolution can recover functionality lost due to an increase in temperature. In the particular case of space applications, these characteristics are extremely important for enabling spacecraft to survive harsh environments and to have long life.},
	eventtitle = {Proceedings. The Second {NASA}/{DoD} Workshop on Evolvable Hardware},
	pages = {99--108},
	booktitle = {Proceedings. The Second {NASA}/{DoD} Workshop on Evolvable Hardware},
	author = {Stoica, A. and Keymeulen, D. and Zebulum, R. and Thakoor, A. and Daud, T. and Klimeck, Y. and Tawel, R. and Duong, V.},
	date = {2000-07},
	note = {{ISSN}: null},
	keywords = {genetic algorithms, Hardware, Analog circuits, Electronic circuits, Field programmable analog arrays, Transistors, analog circuits evolution, chip in-the-loop, circuit configurations, Circuit simulation, Circuit synthesis, evolution-oriented devices, evolvable hardware, field programmable gate arrays, field programmable transistor array architecture, fuzzy logic, Fuzzy logic, fuzzy logics, genetic mechanisms, Genetics, Guidelines, hardware design, hardware-software codesign, harsh environments, logic design, parametrical connectives, self reconfiguration},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/RFJTCRAV/869347.html:text/html}
}

@online{noauthor_neuromorphic_nodate,
	title = {Neuromorphic hardware in the loop: Training a deep spiking network on the {BrainScaleS} wafer-scale system - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/7966125},
	urldate = {2020-01-26},
	file = {Neuromorphic hardware in the loop\: Training a deep spiking network on the BrainScaleS wafer-scale system - IEEE Conference Publication:/home/danielsan/Zotero/storage/KSMLUR5V/7966125.html:text/html}
}

@article{meier_special_2017,
	title = {Special report : Can we copy the brain? - The brain as computer},
	volume = {54},
	issn = {1939-9340},
	doi = {10.1109/MSPEC.2017.7934228},
	shorttitle = {Special report},
	abstract = {Painful exercises in basic arithmetic are a vivid part of our elementary school memories. A multiplication like 3,752 × 6,901 carried out with just pencil and paper for assistance may well take up to a minute. Of course, today, with a cellphone always at hand, we can quickly check that the result of our little exercise is 25,892,552. Indeed, the processors in modern cellphones can together carry out more than 100 billion such operations per second. What's more, the chips consume just a few watts of power, making them vastly more efficient than our slow brains, which consume about 20 watts and need significantly more time to achieve the same result. Of course, the brain didn't evolve to perform arithmetic. So it does that rather badly. But it excels at processing a continuous stream of information from our surroundings. And it acts on that information-sometimes far more rapidly than we're aware of. No matter how much energy a conventional computer consumes, it will struggle with feats the brain finds easy, such as understanding language and running up a flight of stairs.},
	pages = {28--33},
	number = {6},
	journaltitle = {{IEEE} Spectrum},
	author = {Meier, Karlheinz},
	date = {2017-06},
	keywords = {Neurons, Computers, Biomembranes, Brain modeling, brain as computer, cellphone, chips, Computational modeling, elementary school memories, microprocessor chips, modern cellphones, multiplication, processors, Threshold voltage, understanding language},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/L7KWGP86/7934228.html:text/html}
}

@inproceedings{meier_mixed-signal_2015,
	title = {A mixed-signal universal neuromorphic computing system},
	doi = {10.1109/IEDM.2015.7409627},
	abstract = {Neuromorphic information processing systems offer the potential to overcome imminent problems of state-of-the-art computers, in particular the energy efficiency problem, the device reliability problem and the software complexity problem. This paper starts with a short overview of state-of-the-art neuromorphic hardware implementations and their applications. It then describes the time-accelerated mixed-signal approach of the {BrainScaleS} project in some detail.},
	eventtitle = {2015 {IEEE} International Electron Devices Meeting ({IEDM})},
	pages = {4.6.1--4.6.4},
	booktitle = {2015 {IEEE} International Electron Devices Meeting ({IEDM})},
	author = {Meier, Karlheinz},
	date = {2015-12},
	note = {{ISSN}: 2156-017X},
	keywords = {Neurons, Biological system modeling, Neuromorphics, neural net architecture, Brain modeling, {VLSI}, Computational modeling, {BrainScaleS} project, device reliability problem, Energy efficiency, energy efficiency problem, mixed-signal universal neuromorphic computing system, neuromorphic hardware implementations, neuromorphic information processing systems, power aware computing, software complexity problem, software metrics, time-accelerated mixed-signal approach},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/8A748S85/7409627.html:text/html}
}

@incollection{millner_vlsi_2010,
	title = {A {VLSI} Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model},
	url = {http://papers.nips.cc/paper/3995-a-vlsi-implementation-of-the-adaptive-exponential-integrate-and-fire-neuron-model.pdf},
	pages = {1642--1650},
	booktitle = {Advances in Neural Information Processing Systems 23},
	publisher = {Curran Associates, Inc.},
	author = {Millner, Sebastian and Grübl, Andreas and Meier, Karlheinz and Schemmel, Johannes and Schwartz, Marc-olivier},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	urldate = {2020-01-26},
	date = {2010},
	file = {NIPS Full Text PDF:/home/danielsan/Zotero/storage/K5ETCCK3/Millner et al. - 2010 - A VLSI Implementation of the Adaptive Exponential .pdf:application/pdf;NIPS Snapshot:/home/danielsan/Zotero/storage/T9WHYNGY/3995-a-vlsi-implementation-of-the-adaptive-exponential-integrate-and-fire-neuron-model.html:text/html}
}

@inproceedings{meier_mixed-signal_2015-1,
	title = {A mixed-signal universal neuromorphic computing system},
	doi = {10.1109/IEDM.2015.7409627},
	abstract = {Neuromorphic information processing systems offer the potential to overcome imminent problems of state-of-the-art computers, in particular the energy efficiency problem, the device reliability problem and the software complexity problem. This paper starts with a short overview of state-of-the-art neuromorphic hardware implementations and their applications. It then describes the time-accelerated mixed-signal approach of the {BrainScaleS} project in some detail.},
	eventtitle = {2015 {IEEE} International Electron Devices Meeting ({IEDM})},
	pages = {4.6.1--4.6.4},
	booktitle = {2015 {IEEE} International Electron Devices Meeting ({IEDM})},
	author = {Meier, Karlheinz},
	date = {2015-12},
	note = {{ISSN}: 2156-017X},
	keywords = {Neurons, Biological system modeling, Neuromorphics, neural net architecture, Brain modeling, {VLSI}, Computational modeling, {BrainScaleS} project, device reliability problem, Energy efficiency, energy efficiency problem, mixed-signal universal neuromorphic computing system, neuromorphic hardware implementations, neuromorphic information processing systems, power aware computing, software complexity problem, software metrics, time-accelerated mixed-signal approach},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/TXY5YALD/7409627.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/27DCLTA4/Meier - 2015 - A mixed-signal universal neuromorphic computing sy.pdf:application/pdf}
}

@incollection{petrovici_artificial_2016-1,
	location = {Cham},
	title = {Artificial Brains: Simulation and Emulation of Neural Networks},
	isbn = {978-3-319-39552-4},
	url = {https://doi.org/10.1007/978-3-319-39552-4_3},
	series = {Springer Theses},
	shorttitle = {Artificial Brains},
	abstract = {When describing increasingly complex systems, the required array of equations equivalently grows in size and complexity. In many (usually simple) cases, statistical methods can be applied to distill macroscopic equations from those governing the microscopic components of a system, with thermodynamics offering a paradigmatic example.},
	pages = {59--81},
	booktitle = {Form Versus Function: Theory and Models for Neuronal Substrates},
	publisher = {Springer International Publishing},
	author = {Petrovici, Mihai Alexandru},
	editor = {Petrovici, Mihai Alexandru},
	urldate = {2020-01-27},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-39552-4_3},
	keywords = {Synaptic Weight, Biological Domain, Neuron Model, Priority Encoder, Source Neuron},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/TT9HFEFU/Petrovici - 2016 - Artificial Brains Simulation and Emulation of Neu.pdf:application/pdf}
}

@online{noauthor_evolution_nodate,
	title = {Evolution of analog circuits on field programmable transistor arrays - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/869347},
	urldate = {2020-01-27},
	file = {Evolution of analog circuits on field programmable transistor arrays - IEEE Conference Publication:/home/danielsan/Zotero/storage/DFLMJMWI/869347.html:text/html}
}

@inproceedings{stoica_evolution_2000-1,
	title = {Evolution of analog circuits on field programmable transistor arrays},
	doi = {10.1109/EH.2000.869347},
	abstract = {Evolvable Hardware ({EHW}) refers to {HW} design and self reconfiguration using evolutionary/genetic mechanisms. The paper presents an overview of some key concepts of {EHW}, describing also a set of selected applications. A fine-grained Field Programmable Transistor Array ({FPTA}) architecture for reconfigurable hardware is presented as an example of an initial effort toward evolution-oriented devices. Evolutionary experiments in simulations and with a {FPTA} chip in-the-loop demonstrate automatic synthesis of electronic circuits. Unconventional circuits, for which there are no textbook design guidelines, are particularly appealing to evolvable hardware. To illustrate this situation, one demonstrates here the evolution of circuits implementing parametrical connectives for fuzzy logics. In addition to synthesizing circuits for new functions, evolvable hardware can be used to preserve existing functions and achieve fault-tolerance, determining circuit configurations that circumvent the faults. In addition, we illustrate with an example how evolution can recover functionality lost due to an increase in temperature. In the particular case of space applications, these characteristics are extremely important for enabling spacecraft to survive harsh environments and to have long life.},
	eventtitle = {Proceedings. The Second {NASA}/{DoD} Workshop on Evolvable Hardware},
	pages = {99--108},
	booktitle = {Proceedings. The Second {NASA}/{DoD} Workshop on Evolvable Hardware},
	author = {Stoica, A. and Keymeulen, D. and Zebulum, R. and Thakoor, A. and Daud, T. and Klimeck, Y. and Tawel, R. and Duong, V.},
	date = {2000-07},
	note = {{ISSN}: null},
	keywords = {genetic algorithms, Hardware, Analog circuits, Electronic circuits, Field programmable analog arrays, Transistors, analog circuits evolution, chip in-the-loop, circuit configurations, Circuit simulation, Circuit synthesis, evolution-oriented devices, evolvable hardware, field programmable gate arrays, field programmable transistor array architecture, fuzzy logic, Fuzzy logic, fuzzy logics, genetic mechanisms, Genetics, Guidelines, hardware design, hardware-software codesign, harsh environments, logic design, parametrical connectives, self reconfiguration},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/AIFLWQJS/869347.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/KELAKQ2P/Stoica et al. - 2000 - Evolution of analog circuits on field programmable.pdf:application/pdf}
}

@article{von_neumann_first_1993-1,
	title = {First draft of a report on the {EDVAC}},
	volume = {15},
	issn = {1934-1547},
	doi = {10.1109/85.238389},
	abstract = {The first draft of a report on the {EDVAC} written by John von Neumann is presented. This first draft contains a wealth of information, and it had a pervasive influence when it was first written. Most prominently, Alan Turing cites it in his proposal for the Pilot automatic computing engine ({ACE}) as the definitive source for understanding the nature and design of a general-purpose digital computer.{\textless}{\textgreater}},
	pages = {27--75},
	number = {4},
	journaltitle = {{IEEE} Annals of the History of Computing},
	author = {von Neumann, J.},
	date = {1993},
	keywords = {automatic computing engine, digital computers, {EDVAC}, Electrical engineering, Engines, Forward contracts, general-purpose digital computer, history, History, Laboratories, Mathematics, Pain, Physics computing, Proposals, Statistics},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/BJTU5SE6/238389.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/R2459YFG/von Neumann - 1993 - First draft of a report on the EDVAC.pdf:application/pdf}
}

@article{bruderle_establishing_2009,
	title = {Establishing a novel modeling tool: a python-based interface for a neuromorphic hardware system},
	volume = {3},
	issn = {1662-5196},
	url = {https://www.frontiersin.org/articles/10.3389/neuro.11.017.2009/full},
	doi = {10.3389/neuro.11.017.2009},
	shorttitle = {Establishing a novel modeling tool},
	abstract = {Neuromorphic hardware systems provide new possibilities for the neuroscience modeling community. Due to the intrinsic parallelism of the micro-electronic emulation of neural computation, such models are highly scalable without a loss of speed. However, the communities of software simulator users and neuromorphic engineering in neuroscience are rather disjoint. We present a software concept that provides the possibility to establish such hardware devices as valuable modeling tools. It is based on the integration of the hardware interface into a simulator-independent language which allows for unified experiment descriptions that can be run on various simulation platforms without modification, implying experiment portability and a huge simplification of the quantitative comparison of hardware and simulator results. We introduce an accelerated neuromorphic hardware device and describe the implementation of the proposed concept for this system. An example setup and results acquired by utilizing both the hardware system and a software simulator are demonstrated.},
	journaltitle = {Frontiers in Neuroinformatics},
	shortjournal = {Front. Neuroinform.},
	author = {Brüderle, Daniel and Müller, Eric and Davison, Andrew P. and Muller, Eilif and Schemmel, Johannes and Meier, Karlheinz},
	urldate = {2020-01-27},
	date = {2009},
	keywords = {Neuromorphic, {VLSI}, modeling, computational neuroscience, hardware, {PyNN}, python, Software},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/R3FRNCC9/Brüderle et al. - 2009 - Establishing a novel modeling tool a python-based.pdf:application/pdf}
}

@inproceedings{schemmel_implementing_2006-1,
	title = {Implementing Synaptic Plasticity in a {VLSI} Spiking Neural Network Model},
	doi = {10.1109/IJCNN.2006.246651},
	abstract = {This paper describes an area-efficient mixed-signal implementation of synapse-based long term plasticity realized in a {VLSI} model of a spiking neural network. The artificial synapses are based on an implementation of spike time dependent plasticity ({STDP}). In the biological specimen, {STDP} is a mechanism acting locally in each synapse. The presented electronic implementation succeeds in maintaining this high level of parallelism and simultaneously achieves a synapse density of more than 9k synapses per mm2 in a 180 nm technology. This allows the construction of neural micro-circuits close to the biological specimen while maintaining a speed several orders of magnitude faster than biological real time. The large acceleration factor enhances the possibilities to investigate key aspects of plasticity, e.g. by performing extensive parameter searches.},
	eventtitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
	pages = {1--6},
	booktitle = {The 2006 {IEEE} International Joint Conference on Neural Network Proceedings},
	author = {Schemmel, J. and Grubl, A. and Meier, K. and Mueller, E.},
	date = {2006-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Neural networks, Neurons, neural chips, Biological system modeling, Artificial neural networks, biological specimen, Biomembranes, Brain modeling, Circuits, Intelligent networks, mixed analogue-digital integrated circuits, neural micro-circuits, Numerical simulation, size 180 nm, spike time dependent plasticity, spiking neural network model, synaptic plasticity, Very large scale integration, {VLSI}},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/88G3N9CW/1716062.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/ZY4AIIN5/Schemmel et al. - 2006 - Implementing Synaptic Plasticity in a VLSI Spiking.pdf:application/pdf}
}

@online{noauthor_intrinsic_nodate-1,
	title = {Intrinsic evolution of digital-to-analog converters using a {CMOS} {FPTA} chip - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/1310804},
	urldate = {2020-01-27},
	file = {Intrinsic evolution of digital-to-analog converters using a CMOS FPTA chip - IEEE Conference Publication:/home/danielsan/Zotero/storage/KYGQ3UNY/1310804.html:text/html}
}

@inproceedings{langeheine_intrinsic_2004-1,
	title = {Intrinsic evolution of digital-to-analog converters using a {CMOS} {FPTA} chip},
	doi = {10.1109/EH.2004.1310804},
	abstract = {The work presented here tackles the problem of designing a unipolar 6-bit digital-to-analog converter ({DAC}) with a voltage mode output by hardware evolution. Thereby a field programmable transistor array ({FPTA}) is used as the analog substrate for testing the candidate solutions. The {FPTA} features 256 programmable transistors, whose channel geometry and routing can be configured to form a large variety of transistor level analog circuits. A series of experiments reveals that variations of the output voltage range influence evolution's success more severely than varying the amount of available electronic resources or the geometrical setup. Although a considerable number of runs yield converters with a nonlinearity of less than 1 bit, no {DAC} is found to maintain a nonlinearity of less than 0.5 bits under worst case conditions, as required for a true 6-bit resolution. While the evolved circuits work comparably well at different time scales as well as on different dice, they lack the ability to abstract from the analog voltage levels of the digital input signals. It is experimentally verified that this can be remedied by inserting digital buffers at the circuits' inputs.},
	eventtitle = {Proceedings. 2004 {NASA}/{DoD} Conference on Evolvable Hardware, 2004.},
	pages = {18--25},
	booktitle = {Proceedings. 2004 {NASA}/{DoD} Conference on Evolvable Hardware, 2004.},
	author = {Langeheine, J. and Meier, K. and Schemmel, J. and Trefzer, M.},
	date = {2004-06},
	note = {{ISSN}: null},
	keywords = {Hardware, evolutionary computation, Silicon, {CMOS} analogue integrated circuits, {CMOS} {FPTA} chip, hardware evolution, Physics, 6 bits, analog substrate, analog voltage levels, circuit optimisation, Circuit testing, digital buffers, digital input signals, Digital-analog conversion, digital-analogue conversion, digital-to-analog converters, Field programmable analog arrays, field programmable analogue arrays, field programmable transistor array, Humans, integrated circuit design, {SPICE}, transistor level analog circuits, Transistors, unipolar {DAC}, Voltage, voltage mode output},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/QAFZ96NS/1310804.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/8594WV3Y/Langeheine et al. - 2004 - Intrinsic evolution of digital-to-analog converter.pdf:application/pdf}
}

@inproceedings{trefzer_operational_2005-1,
	location = {Berlin, Heidelberg},
	title = {Operational Amplifiers: An Example for Multi-objective Optimization on an Analog Evolvable Hardware Platform},
	isbn = {978-3-540-28737-7},
	doi = {10.1007/11549703_9},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Operational Amplifiers},
	abstract = {This work tackles the problem of synthesizing transferable and reusable operational amplifiers on a field programmable transistor array: the Heidelberg {FPTA}. A multi-objective evolutionary algorithm is developed, in order to be able to include various specifications of an operational amplifier into the process of circuit synthesis. Additionally, the presented algorithm is designed to preserve the diversity within the population troughout evolution and is therefore able to efficiently explore the design space. Furthermore, the evolved circuits are proven to work on the chip as well as in simulation outside the {FPTA}. Schematics of good solutions are presented and their characteristics are compared to those of basic manually created reference designs.},
	pages = {86--97},
	booktitle = {Evolvable Systems: From Biology to Hardware},
	publisher = {Springer},
	author = {Trefzer, Martin and Langeheine, Jörg and Meier, Karlheinz and Schemmel, Johannes},
	editor = {Moreno, J. Manuel and Madrenas, Jordi and Cosp, Jordi},
	date = {2005},
	langid = {english},
	keywords = {Evolvable Hardware, Longe Wire, Tournament Selection, Transistor Array, Transistor Circuit},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/VCMVPWMC/Trefzer et al. - 2005 - Operational Amplifiers An Example for Multi-objec.pdf:application/pdf}
}

@article{carrillo_scalable_2013-1,
	title = {Scalable Hierarchical Network-on-Chip Architecture for Spiking Neural Network Hardware Implementations},
	volume = {24},
	issn = {2161-9883},
	doi = {10.1109/TPDS.2012.289},
	abstract = {Spiking neural networks ({SNNs}) attempt to emulate information processing in the mammalian brain based on massively parallel arrays of neurons that communicate via spike events. {SNNs} offer the possibility to implement embedded neuromorphic circuits, with high parallelism and low power consumption compared to the traditional von Neumann computer paradigms. Nevertheless, the lack of modularity and poor connectivity shown by traditional neuron interconnect implementations based on shared bus topologies is prohibiting scalable hardware implementations of {SNNs}. This paper presents a novel hierarchical network-on-chip (H-{NoC}) architecture for {SNN} hardware, which aims to address the scalability issue by creating a modular array of clusters of neurons using a hierarchical structure of low and high-level routers. The proposed H-{NoC} architecture incorporates a spike traffic compression technique to exploit {SNN} traffic patterns and locality between neurons, thus reducing traffic overhead and improving throughput on the network. In addition, adaptive routing capabilities between clusters balance local and global traffic loads to sustain throughput under bursting activity. Analytical results show the scalability of the proposed H-{NoC} approach under different scenarios, while simulation and synthesis analysis using 65-nm {CMOS} technology demonstrate high-throughput, low-cost area, and power consumption per cluster, respectively.},
	pages = {2451--2461},
	number = {12},
	journaltitle = {{IEEE} Transactions on Parallel and Distributed Systems},
	author = {Carrillo, Snaider and Harkin, Jim and {McDaid}, Liam J. and Morgan, Fearghal and Pande, Sandeep and Cawley, Seamus and {McGinley}, Brian},
	date = {2013-12},
	keywords = {Computer architecture, neural nets, Neural networks, network routing, {CMOS} technology, Network topology, adaptive routing capabilities, clusters balance local traffic loads, computer architecture, global traffic loads, H-{NoC} architecture, Interconnection architecture, Microprocessors, network synthesis, network-on-chip, neurocomputers, neuron cluster modular array, On chip architectures, real-time distributed, scalable hierarchical network-on-chip architecture, {SNN} hardware, {SNN} traffic patterns, spike traffic compression technique, spiking neural network hardware implementations, spiking neural networks, traffic overhead reduction},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/CEQ9CA7Y/6322959.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/UTN7NIYU/Carrillo et al. - 2013 - Scalable Hierarchical Network-on-Chip Architecture.pdf:application/pdf}
}

@online{noauthor_special_nodate,
	title = {Special report : Can we copy the brain? - The brain as computer - {IEEE} Journals \& Magazine},
	url = {https://ieeexplore.ieee.org/abstract/document/7934228},
	urldate = {2020-01-27},
	file = {Special report \: Can we copy the brain? - The brain as computer - IEEE Journals & Magazine:/home/danielsan/Zotero/storage/WMM4733A/7934228.html:text/html}
}

@article{meier_special_2017-1,
	title = {Special report : Can we copy the brain? - The brain as computer},
	volume = {54},
	issn = {1939-9340},
	doi = {10.1109/MSPEC.2017.7934228},
	shorttitle = {Special report},
	abstract = {Painful exercises in basic arithmetic are a vivid part of our elementary school memories. A multiplication like 3,752 × 6,901 carried out with just pencil and paper for assistance may well take up to a minute. Of course, today, with a cellphone always at hand, we can quickly check that the result of our little exercise is 25,892,552. Indeed, the processors in modern cellphones can together carry out more than 100 billion such operations per second. What's more, the chips consume just a few watts of power, making them vastly more efficient than our slow brains, which consume about 20 watts and need significantly more time to achieve the same result. Of course, the brain didn't evolve to perform arithmetic. So it does that rather badly. But it excels at processing a continuous stream of information from our surroundings. And it acts on that information-sometimes far more rapidly than we're aware of. No matter how much energy a conventional computer consumes, it will struggle with feats the brain finds easy, such as understanding language and running up a flight of stairs.},
	pages = {28--33},
	number = {6},
	journaltitle = {{IEEE} Spectrum},
	author = {Meier, Karlheinz},
	date = {2017-06},
	keywords = {Neurons, Computers, Biomembranes, Brain modeling, brain as computer, cellphone, chips, Computational modeling, elementary school memories, microprocessor chips, modern cellphones, multiplication, processors, Threshold voltage, understanding language},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/XV5G43R5/7934228.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/N5FPZWRC/Meier - 2017 - Special report  Can we copy the brain - The brai.pdf:application/pdf}
}

@inproceedings{schurmann_towards_2002-1,
	title = {Towards an artificial neural network framework},
	doi = {10.1109/EH.2002.1029893},
	abstract = {This paper proposes a framework for hardware artificial neural networks ({ANN}) combining scalability with the flexibility of software solutions and the speed of hardware {ANNs}. Our implementation consists of analog neural network blocks realized as {ASICs} configurable to form arbitrary and large networks having simple elementary resources, i.e. synapses and neurons. Scalability is assured by confining the analog processing of the synapses to blocks and using digital signalling between them. With the help of a genetic algorithm we train the network to combine its elementary resources to form variable network building blocks. We demonstrate how three binary input neurons can act as a single 3-bit neuron and how a group of neurons and synapses can be trained to form a 3-bit output neuron with linear and sigmoid activation functions.},
	eventtitle = {Proceedings 2002 {NASA}/{DoD} Conference on Evolvable Hardware},
	pages = {266--273},
	booktitle = {Proceedings 2002 {NASA}/{DoD} Conference on Evolvable Hardware},
	author = {Schurmann, F. and Hohmann, S. and Schemmel, J. and Meier, K.},
	date = {2002-07},
	note = {{ISSN}: null},
	keywords = {genetic algorithm, Neural networks, Neurons, Field programmable gate arrays, Artificial neural networks, neural net architecture, analog neural net, {ANN}, Application specific integrated circuits, artificial neural networks, Digital circuits, digital signalling, flexibility, Network topology, Neural network hardware, scalability, Scalability, Signal processing, variable network building blocks},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/A57M36DP/1029893.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/NQRQEJRV/Schurmann et al. - 2002 - Towards an artificial neural network framework.pdf:application/pdf}
}

@article{mead_neuromorphic_1990,
	title = {Neuromorphic electronic systems},
	volume = {78},
	issn = {1558-2256},
	doi = {10.1109/5.58356},
	abstract = {It is shown that for many problems, particularly those in which the input data are ill-conditioned and the computation can be specified in a relative manner, biological solutions are many orders of magnitude more effective than those using digital methods. This advantage can be attributed principally to the use of elementary physical phenomena as computational primitives, and to the representation of information by the relative values of analog signals rather than by the absolute values of digital signals. This approach requires adaptive techniques to mitigate the effects of component differences. This kind of adaptation leads naturally to systems that learn about their environment. Large-scale adaptive analog systems are more robust to component degradation and failure than are more conventional systems, and they use far less power. For this reason, adaptive analog technology can be expected to utilize the full potential of wafer-scale silicon fabrication.{\textless}{\textgreater}},
	pages = {1629--1636},
	number = {10},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Mead, C.},
	date = {1990-10},
	keywords = {Robustness, neural nets, Neuromorphics, Silicon, Physics computing, {VLSI}, adaptive analog systems, adaptive systems, Adaptive systems, Analog computers, analog signals, analogue circuits, Biology computing, Degradation, Fabrication, Large-scale systems, neuromorphic electronic systems},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/6ATQFY6Q/58356.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/KVCW8A7N/Mead - 1990 - Neuromorphic electronic systems.pdf:application/pdf}
}

@online{noauthor_john_nodate,
	title = {John von Neumann’s The Computer and the Brain},
	url = {https://www.leydesdorff.net/vonneumann/},
	urldate = {2020-01-29},
	file = {John von Neumann’s The Computer and the Brain:/home/danielsan/Zotero/storage/IEFYGLRU/vonneumann.html:text/html}
}

@book{boole_mathematical_1847,
	title = {The Mathematical Analysis of Logic},
	pagetotal = {328},
	publisher = {Philosophical Library},
	author = {Boole, George},
	date = {1847},
	langid = {english},
	note = {Google-Books-{ID}: zv4YAQAAIAAJ}
}

@book{boole_mathematical_1847-1,
	title = {The Mathematical Analysis of Logic},
	pagetotal = {328},
	publisher = {Philosophical Library},
	author = {Boole, George},
	date = {1847},
	langid = {english},
	note = {Google-Books-{ID}: zv4YAQAAIAAJ}
}

@online{noauthor_engineeringhis_nodate,
	title = {Engineering{\textbar}His Legacy{\textbar} Computer Science {\textbar} Who invented Boolean{\textbar} George Boole 200},
	url = {http://georgeboole.com/boole/legacy/engineering/},
	abstract = {George Boole laid the mathematical foundations of the information age.},
	urldate = {2020-01-29},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/DWYHQ2IS/engineering.html:text/html}
}

@book{boole_investigation_1854,
	title = {An Investigation of the Laws of Thought: On which are Founded the Mathematical Theories of Logic and Probabilities},
	shorttitle = {An Investigation of the Laws of Thought},
	pagetotal = {476},
	publisher = {Dover Publications},
	author = {Boole, George},
	date = {1854},
	langid = {english},
	note = {Google-Books-{ID}: pn\_HORLo1uIC}
}

@article{shannon_symbolic_1938,
	title = {A symbolic analysis of relay and switching circuits},
	volume = {57},
	issn = {2376-7804},
	doi = {10.1109/EE.1938.6431064},
	abstract = {{IN} {THE} {CONTROL} and protective circuits of complex electrical systems it is frequently necessary to make intricate interconnections of relay contacts and switches. Examples of these circuits occur in automatic telephone exchanges, industrial motor-control equipment, and in almost any circuits designed to perform complex operations automatically. In this paper a mathematical analysis of certain of the properties of such networks will be made. Particular attention will be given to the problem of network synthesis. Given certain characteristics, it is required to find a circuit incorporating these characteristics. The solution of this type of problem is not unique and methods of finding those particular circuits requiring the least number of relay contacts and switch blades will be studied. Methods will also be described for finding any number of circuits equivalent to a given circuit in all operating characteristics. It will be shown that several of the well-known theorems on impedance networks have roughly analogous theorems in relay circuits. Notable among these are the delta-wye and star-mesh transformations, and the duality theorem.},
	pages = {713--723},
	number = {12},
	journaltitle = {Electrical Engineering},
	author = {Shannon, Claude E.},
	date = {1938-12},
	keywords = {Calculus, Algebra, Contacts, Equations, Impedance, Joining processes, Relays},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/42R6BAAS/6431064.html:text/html;Submitted Version:/home/danielsan/Zotero/storage/L3LMQTIJ/Shannon - 1938 - A symbolic analysis of relay and switching circuit.pdf:application/pdf}
}

@article{dehaene_arithmetic_2004,
	title = {Arithmetic and the brain},
	volume = {14},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438804000406},
	doi = {10.1016/j.conb.2004.03.008},
	abstract = {Recent studies in human neuroimaging, primate neurophysiology, and developmental neuropsychology indicate that the human ability for arithmetic has a tangible cerebral substrate. The human intraparietal sulcus is systematically activated in all number tasks and could host a central amodal representation of quantity. Areas of the precentral and inferior prefrontal cortex also activate when subjects engage in mental calculation. A monkey analogue of these parieto-frontal regions has recently been identified, and a neuronal population code for number has been characterized. Finally, pathologies of this system, leading to acalculia in adults or to developmental dyscalculia in children, are beginning to be understood, thus paving the way for brain-oriented intervention studies.},
	pages = {218--224},
	number = {2},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Dehaene, Stanislas and Molko, Nicolas and Cohen, Laurent and Wilson, Anna J},
	urldate = {2020-01-29},
	date = {2004-04-01},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/danielsan/Zotero/storage/JVBSKQ7V/S0959438804000406.html:text/html}
}

@book{symposium_monkey_2005,
	title = {From Monkey Brain to Human Brain: A Fyssen Foundation Symposium},
	isbn = {978-0-262-04223-9},
	shorttitle = {From Monkey Brain to Human Brain},
	abstract = {Leaders in cognitive psychology, comparative biology, and neuroscience discuss patterns of convergence and divergence seen in studies of human and nonhuman primate brains. The extraordinary overlap between human and chimpanzee genomes does not result in an equal overlap between human and chimpanzee thoughts, sensations, perceptions, and emotions; there are considerable similarities but also considerable differences between human and nonhuman primate brains. From Monkey Brain to Human Brain uses the latest findings in cognitive psychology, comparative biology, and neuroscience to look at the complex patterns of convergence and divergence in primate cortical organization and function. Several chapters examine the use of modern technologies to study primate brains, analyzing the potentials and the limitations of neuroimaging as well as genetic and computational approaches. These methods, which can be applied identically across different species of primates, help to highlight the paradox of nonlinear primate evolution--the fact that major changes in brain size and functional complexity resulted from small changes in the genome. Other chapters identify plausible analogs or homologs in nonhuman primates for such human cognitive functions as arithmetic, reading, theory of mind, and altruism; examine the role of parietofrontal circuits in the production and comprehension of actions; analyze the contributions of the prefrontal and cingulate cortices to cognitive control; and explore to what extent visual recognition and visual attention are related in humans and other primates. The Fyssen Foundation is dedicated to encouraging scientific inquiry into the cognitive mechanisms that underlie animal and human behavior and has long sponsored symposia on topics of central importance to the cognitive sciences.},
	pagetotal = {444},
	publisher = {{MIT} Press},
	author = {Symposium, Fondation Fyssen},
	date = {2005},
	langid = {english},
	note = {Google-Books-{ID}: 7xSkrSKkxP8C},
	keywords = {Psychology / Cognitive Psychology \& Cognition}
}

@article{moore_cramming_1998,
	title = {Cramming More Components Onto Integrated Circuits},
	volume = {86},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/658762/},
	doi = {10.1109/JPROC.1998.658762},
	pages = {82--85},
	number = {1},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Moore, G.E.},
	urldate = {2020-01-30},
	date = {1998-01},
	langid = {english},
	file = {Moore - 1998 - Cramming More Components Onto Integrated Circuits.pdf:/home/danielsan/Zotero/storage/7ZJFUDDP/Moore - 1998 - Cramming More Components Onto Integrated Circuits.pdf:application/pdf}
}

@article{haddow_challenges_2011,
	title = {Challenges of evolvable hardware: past, present and the path to a promising future},
	volume = {12},
	issn = {1573-7632},
	url = {https://doi.org/10.1007/s10710-011-9141-6},
	doi = {10.1007/s10710-011-9141-6},
	shorttitle = {Challenges of evolvable hardware},
	abstract = {Nature is phenomenal. The achievements in, for example, evolution are everywhere to be seen: complexity, resilience, inventive solutions and beauty. Evolvable Hardware ({EH}) is a field of evolutionary computation ({EC}) that focuses on the embodiment of evolution in a physical media. If {EH} could achieve even a small step in natural evolution’s achievements, it would be a significant step for hardware designers. Before the field of {EH} began, {EC} had already shown artificial evolution to be a highly competitive problem solver. {EH} thus started off as a new and exciting field with much promise. It seemed only a matter of time before researchers would find ways to convert such techniques into hardware problem solvers and further refine the techniques to achieve systems that were competitive with or better than human designs. However, 15 years on—it appears that problems solved by {EH} are only of the size and complexity of that achievable in {EC} 15 years ago and seldom compete with traditional designs. A critical review of the field is presented. Whilst highlighting some of the successes, it also considers why the field is far from reaching these goals. The paper further redefines the field and speculates where the field should go in the next 10 years.},
	pages = {183--215},
	number = {3},
	journaltitle = {Genetic Programming and Evolvable Machines},
	shortjournal = {Genet Program Evolvable Mach},
	author = {Haddow, Pauline C. and Tyrrell, Andy M.},
	urldate = {2020-01-30},
	date = {2011-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/MNDY6J99/Haddow and Tyrrell - 2011 - Challenges of evolvable hardware past, present an.pdf:application/pdf}
}

@online{noauthor_frontiers_nodate-1,
	title = {Frontiers {\textbar} Establishing a novel modeling tool: a python-based interface for a neuromorphic hardware system {\textbar} Frontiers in Neuroinformatics},
	url = {https://www.frontiersin.org/articles/10.3389/neuro.11.017.2009/full},
	urldate = {2020-01-30},
	file = {Frontiers | Establishing a novel modeling tool\: a python-based interface for a neuromorphic hardware system | Frontiers in Neuroinformatics:/home/danielsan/Zotero/storage/XBJNFRB5/full.html:text/html}
}

@book{symposium_monkey_2005-1,
	title = {From Monkey Brain to Human Brain: A Fyssen Foundation Symposium},
	isbn = {978-0-262-04223-9},
	shorttitle = {From Monkey Brain to Human Brain},
	abstract = {Leaders in cognitive psychology, comparative biology, and neuroscience discuss patterns of convergence and divergence seen in studies of human and nonhuman primate brains. The extraordinary overlap between human and chimpanzee genomes does not result in an equal overlap between human and chimpanzee thoughts, sensations, perceptions, and emotions; there are considerable similarities but also considerable differences between human and nonhuman primate brains. From Monkey Brain to Human Brain uses the latest findings in cognitive psychology, comparative biology, and neuroscience to look at the complex patterns of convergence and divergence in primate cortical organization and function. Several chapters examine the use of modern technologies to study primate brains, analyzing the potentials and the limitations of neuroimaging as well as genetic and computational approaches. These methods, which can be applied identically across different species of primates, help to highlight the paradox of nonlinear primate evolution--the fact that major changes in brain size and functional complexity resulted from small changes in the genome. Other chapters identify plausible analogs or homologs in nonhuman primates for such human cognitive functions as arithmetic, reading, theory of mind, and altruism; examine the role of parietofrontal circuits in the production and comprehension of actions; analyze the contributions of the prefrontal and cingulate cortices to cognitive control; and explore to what extent visual recognition and visual attention are related in humans and other primates. The Fyssen Foundation is dedicated to encouraging scientific inquiry into the cognitive mechanisms that underlie animal and human behavior and has long sponsored symposia on topics of central importance to the cognitive sciences.},
	pagetotal = {444},
	publisher = {{MIT} Press},
	author = {Symposium, Fondation Fyssen},
	date = {2005},
	langid = {english},
	note = {Google-Books-{ID}: 7xSkrSKkxP8C},
	keywords = {Psychology / Cognitive Psychology \& Cognition}
}

@inproceedings{trefzer_operational_2005-2,
	location = {Berlin, Heidelberg},
	title = {Operational Amplifiers: An Example for Multi-objective Optimization on an Analog Evolvable Hardware Platform},
	isbn = {978-3-540-28737-7},
	doi = {10.1007/11549703_9},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Operational Amplifiers},
	abstract = {This work tackles the problem of synthesizing transferable and reusable operational amplifiers on a field programmable transistor array: the Heidelberg {FPTA}. A multi-objective evolutionary algorithm is developed, in order to be able to include various specifications of an operational amplifier into the process of circuit synthesis. Additionally, the presented algorithm is designed to preserve the diversity within the population troughout evolution and is therefore able to efficiently explore the design space. Furthermore, the evolved circuits are proven to work on the chip as well as in simulation outside the {FPTA}. Schematics of good solutions are presented and their characteristics are compared to those of basic manually created reference designs.},
	pages = {86--97},
	booktitle = {Evolvable Systems: From Biology to Hardware},
	publisher = {Springer},
	author = {Trefzer, Martin and Langeheine, Jörg and Meier, Karlheinz and Schemmel, Johannes},
	editor = {Moreno, J. Manuel and Madrenas, Jordi and Cosp, Jordi},
	date = {2005},
	langid = {english},
	keywords = {Evolvable Hardware, Longe Wire, Tournament Selection, Transistor Array, Transistor Circuit},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/TI6HG5Y5/Trefzer et al. - 2005 - Operational Amplifiers An Example for Multi-objec.pdf:application/pdf}
}

@book{minsky_perceptrons_2017-1,
	title = {Perceptrons: An Introduction to Computational Geometry},
	isbn = {978-0-262-53477-2},
	shorttitle = {Perceptrons},
	abstract = {The first systematic study of parallelism in computation by two pioneers in the field.Reissue of the 1988 Expanded Edition with a new foreword by Léon {BottouIn} 1969, ten years after the discovery of the perceptron—which showed that a machine could be taught to perform certain tasks using examples—Marvin Minsky and Seymour Papert published Perceptrons, their analysis of the computational capabilities of perceptrons for specific tasks. As Léon Bottou writes in his foreword to this edition, “Their rigorous work and brilliant technique does not make the perceptron look very good.” Perhaps as a result, research turned away from the perceptron. Then the pendulum swung back, and machine learning became the fastest-growing field in computer science. Minsky and Papert's insistence on its theoretical foundations is newly relevant.Perceptrons—the first systematic study of parallelism in computation—marked a historic turn in artificial intelligence, returning to the idea that intelligence might emerge from the activity of networks of neuron-like entities. Minsky and Papert provided mathematical analysis that showed the limitations of a class of computing machines that could be considered as models of the brain. Minsky and Papert added a new chapter in 1987 in which they discuss the state of parallel computers, and note a central theoretical challenge: reaching a deeper understanding of how “objects” or “agents” with individuality can emerge in a network. Progress in this area would link connectionism with what the authors have called “society theories of mind.”},
	pagetotal = {317},
	publisher = {{MIT} Press},
	author = {Minsky, Marvin and Papert, Seymour A.},
	date = {2017-09-22},
	langid = {english},
	note = {Google-Books-{ID}: {PLQ}5DwAAQBAJ},
	keywords = {Computers / Computer Science}
}

@online{noauthor_investigation_nodate,
	title = {An Investigation of the Laws of Thought: On which are Founded the ... - George Boole - Google Bøker},
	url = {https://books.google.no/books?id=pn_HORLo1uIC},
	urldate = {2020-01-30},
	file = {An Investigation of the Laws of Thought\: On which are Founded the ... - George Boole - Google Bøker:/home/danielsan/Zotero/storage/LSF2INAE/books.html:text/html}
}

@thesis{jackson_algebraic_2019,
	title = {Algebraic Neural Architecture Representation, Evolutionary Neural Architecture Search, and Novelty Search in Deep Reinforcement Learning},
	abstract = {Evolutionary algorithms have recently re-emerged as powerful tools for machine learning and artificial intelligence, especially when combined with advances in deep learning developed over the last decade. In contrast to the use of fixed architectures and rigid learning algorithms, we leveraged the open-endedness of evolutionary algorithms to make both theoretical and methodological contributions to deep reinforcement learning. This thesis explores and develops two major areas at the intersection of evolutionary algorithms and deep reinforcement learning: generative network architectures and behaviour-based optimization. Over three distinct contributions, both theoretical and experimental methods were applied to deliver a novel mathematical framework and experimental method for generative, modular neural network architecture search for reinforcement learning, and a generalized formulation of a behaviour-based optimization framework for reinforcement learning called novelty search. Experimental results indicate that both alternative, behaviour-based optimization and neural architecture search can each be used to improve learning in the popular Atari 2600 benchmark compared to {DQN} --- a popular gradient-based method. These results are in-line with related work demonstrating that strictly gradient-free methods are competitive with gradient-based reinforcement learning. These contributions, together with other successful combinations of evolutionary algorithms and deep learning, demonstrate that alternative architectures and learning algorithms to those conventionally used in deep learning should be seriously investigated in an effort to drive progress in artificial intelligence.},
	type = {phdthesis},
	author = {Jackson, Ethan and Program, Graduate},
	date = {2019-09-01},
	file = {Full Text PDF:/home/danielsan/Zotero/storage/P3FCNBPJ/Jackson and Program - 2019 - Algebraic Neural Architecture Representation, Evol.pdf:application/pdf}
}

@article{tyrrell_editorial_2004,
	title = {Editorial - evolvable hardware},
	volume = {151},
	issn = {1350-2387},
	doi = {10.1049/ip-cdt:20040768},
	pages = {265--266},
	number = {4},
	journaltitle = {{IEE} Proceedings - Computers and Digital Techniques},
	author = {Tyrrell, A.},
	date = {2004-07},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/GQWMMJHK/1318860.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/QIQU3WCU/Tyrrell - 2004 - Editorial - evolvable hardware.pdf:application/pdf}
}

@article{haddow_challenges_2011-1,
	title = {Challenges of evolvable hardware: past, present and the path to a promising future},
	volume = {12},
	issn = {1573-7632},
	url = {https://doi.org/10.1007/s10710-011-9141-6},
	doi = {10.1007/s10710-011-9141-6},
	shorttitle = {Challenges of evolvable hardware},
	abstract = {Nature is phenomenal. The achievements in, for example, evolution are everywhere to be seen: complexity, resilience, inventive solutions and beauty. Evolvable Hardware ({EH}) is a field of evolutionary computation ({EC}) that focuses on the embodiment of evolution in a physical media. If {EH} could achieve even a small step in natural evolution’s achievements, it would be a significant step for hardware designers. Before the field of {EH} began, {EC} had already shown artificial evolution to be a highly competitive problem solver. {EH} thus started off as a new and exciting field with much promise. It seemed only a matter of time before researchers would find ways to convert such techniques into hardware problem solvers and further refine the techniques to achieve systems that were competitive with or better than human designs. However, 15 years on—it appears that problems solved by {EH} are only of the size and complexity of that achievable in {EC} 15 years ago and seldom compete with traditional designs. A critical review of the field is presented. Whilst highlighting some of the successes, it also considers why the field is far from reaching these goals. The paper further redefines the field and speculates where the field should go in the next 10 years.},
	pages = {183--215},
	number = {3},
	journaltitle = {Genetic Programming and Evolvable Machines},
	shortjournal = {Genet Program Evolvable Mach},
	author = {Haddow, Pauline C. and Tyrrell, Andy M.},
	urldate = {2020-01-30},
	date = {2011-09-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/danielsan/Zotero/storage/T8ZMX32B/Haddow and Tyrrell - 2011 - Challenges of evolvable hardware past, present an.pdf:application/pdf}
}

@online{noauthor_problem_nodate,
	title = {The Problem with {STDP} and Neuromorphic Mimicry – Knowm.org},
	url = {https://knowm.org/the-problem-with-stdp-and-neuromorphic-mimicry/},
	urldate = {2020-02-03},
	langid = {american},
	file = {Snapshot:/home/danielsan/Zotero/storage/SDM8URJH/the-problem-with-stdp-and-neuromorphic-mimicry.html:text/html}
}

@inproceedings{hassan_hybrid_2017,
	title = {Hybrid spiking-based multi-layered self-learning neuromorphic system based on memristor crossbar arrays},
	doi = {10.23919/DATE.2017.7927094},
	abstract = {Neuromorphic computing systems are under heavy investigation as a potential substitute for the traditional von Neumann systems in high-speed low-power applications. Recently, memristor crossbar arrays were utilized in realizing spiking-based neuromorphic system, where memristor conductance values correspond to synaptic weights. Most of these systems are composed of a single crossbar layer, in which system training is done off-chip, using computer based simulations, then the trained weights are pre-programmed to the memristor crossbar array. However, multi-layered, on-chip trained systems become crucial for handling massive amount of data and to overcome the resistance shift that occurs to memristors overtime. In this work, we propose a spiking-based multi-layered neuromorphic computing system capable of online training. The system performance is evaluated using three different datasets showing improved results versus previous work. In addition, studying the system accuracy versus memristor resistance shift shows promising results.},
	eventtitle = {Design, Automation Test in Europe Conference Exhibition ({DATE}), 2017},
	pages = {776--781},
	booktitle = {Design, Automation Test in Europe Conference Exhibition ({DATE}), 2017},
	author = {Hassan, Amr M. and Yang, Chaofei and Liu, Chenchen and Li, Hai Helen and Chen, Yiran},
	date = {2017-03},
	note = {{ISSN}: 1558-1101},
	keywords = {Neurons, neural chips, Hardware, learning (artificial intelligence), Neuromorphics, Training, Algorithm design and analysis, Biological neural networks, computer based simulations, high-speed low-power applications, hybrid spiking-based multilayered self-learning neuromorphic system, low-power electronics, memristor circuits, memristor conductance values, memristor crossbar arrays, Memristors, multilayer perceptrons, multilayered on-chip trained systems, online training, single crossbar layer, spiking-based multilayered neuromorphic computing system, synaptic weights, von Neumann systems},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/YHRLNXKV/7927094.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/LIS9ZNN8/Hassan et al. - 2017 - Hybrid spiking-based multi-layered self-learning n.pdf:application/pdf}
}

@inproceedings{schemmel_wafer-scale_2010-1,
	title = {A wafer-scale neuromorphic hardware system for large-scale neural modeling},
	doi = {10.1109/ISCAS.2010.5536970},
	abstract = {Modeling neural tissue is an important tool to investigate biological neural networks. Until recently, most of this modeling has been done using numerical methods. In the European research project "{FACETS}" this computational approach is complemented by different kinds of neuromorphic systems. A special emphasis lies in the usability of these systems for neuroscience. To accomplish this goal an integrated software/hardware framework has been developed which is centered around a unified neural system description language, called {PyNN}, that allows the scientist to describe a model and execute it in a transparent fashion on either a neuromorphic hardware system or a numerical simulator. A very large analog neuromorphic hardware system developed within {FACETS} is able to use complex neural models as well as realistic network topologies, i.e. it can realize more than 10000 synapses per neuron, to allow the direct execution of models which previously could have been simulated numerically only.},
	eventtitle = {Proceedings of 2010 {IEEE} International Symposium on Circuits and Systems},
	pages = {1947--1950},
	booktitle = {Proceedings of 2010 {IEEE} International Symposium on Circuits and Systems},
	author = {Schemmel, Johannes and Briiderle, Daniel and Griibl, Andreas and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
	date = {2010-05},
	note = {{ISSN}: 2158-1525},
	keywords = {neural nets, neurophysiology, Biological system modeling, Hardware, Neuromorphics, Numerical simulation, neuroscience, Biology computing, Large-scale systems, Biological neural networks, biological neural networks, Biological tissues, complex networks, European research project, {FACETS} project, large scale neural modeling, neural tissue, {PyNN} language, Semiconductor device modeling, Usability, wafer scale neuromorphic hardware system},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/CLCAZ848/5536970.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/BBEAEL4D/Schemmel et al. - 2010 - A wafer-scale neuromorphic hardware system for lar.pdf:application/pdf}
}

@inproceedings{schemmel_wafer-scale_2008-1,
	title = {Wafer-scale integration of analog neural networks},
	doi = {10.1109/IJCNN.2008.4633828},
	abstract = {This paper introduces a novel design of an artificial neural network tailored for wafer-scale integration. The presented {VLSI} implementation includes continuous-time analog neurons with up to 16 k inputs. A novel interconnection and routing scheme allows the mapping of a multitude of network models derived from biology on the {VLSI} neural network while maintaining a high resource usage. A single 20 cm wafer contains about 60 million synapses. The implemented neurons are highly accelerated compared to biological real time. The power consumption of the dense interconnection network providing the necessary communication bandwidth is a critical aspect of the system integration. A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology.},
	eventtitle = {2008 {IEEE} International Joint Conference on Neural Networks ({IEEE} World Congress on Computational Intelligence)},
	pages = {431--438},
	booktitle = {2008 {IEEE} International Joint Conference on Neural Networks ({IEEE} World Congress on Computational Intelligence)},
	author = {Schemmel, Johannes and Fieres, Johannes and Meier, Karlheinz},
	date = {2008-06},
	note = {{ISSN}: 2161-4407},
	keywords = {Neurons, integrated circuit interconnections, network routing, neural chips, Artificial neural networks, Acceleration, analogue integrated circuits, artificial neural network, asynchronous low-voltage signaling scheme, continuous-time analog neural networks, Driver circuits, interconnection network, interconnection-routing scheme, Metals, power consumption, Power demand, programmable network topology, {VLSI} implementation, wafer-scale integration, Wire},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/4H45FQZK/4633828.html:text/html;IEEE Xplore Full Text PDF:/home/danielsan/Zotero/storage/C659HHHC/Schemmel et al. - 2008 - Wafer-scale integration of analog neural networks.pdf:application/pdf}
}

@article{pfeil_six_2013,
	title = {Six networks on a universal neuromorphic computing substrate},
	volume = {7},
	issn = {1662-4548},
	url = {http://arxiv.org/abs/1210.7083},
	doi = {10.3389/fnins.2013.00011},
	abstract = {In this study, we present a highly configurable neuromorphic computing substrate and use it for emulating several types of neural networks. At the heart of this system lies a mixed-signal chip, with analog implementations of neurons and synapses and digital transmission of action potentials. Major advantages of this emulation device, which has been explicitly designed as a universal neural network emulator, are its inherent parallelism and high acceleration factor compared to conventional computers. Its configurability allows the realization of almost arbitrary network topologies and the use of widely varied neuronal and synaptic parameters. Fixed-pattern noise inherent to analog circuitry is reduced by calibration routines. An integrated development environment allows neuroscientists to operate the device without any prior knowledge of neuromorphic circuit design. As a showcase for the capabilities of the system, we describe the successful emulation of six different neural networks which cover a broad spectrum of both structure and functionality.},
	journaltitle = {Frontiers in Neuroscience},
	shortjournal = {Front. Neurosci.},
	author = {Pfeil, Thomas and Grübl, Andreas and Jeltsch, Sebastian and Müller, Eric and Müller, Paul and Petrovici, Mihai A. and Schmuker, Michael and Brüderle, Daniel and Schemmel, Johannes and Meier, Karlheinz},
	urldate = {2020-02-10},
	date = {2013},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1210.7083},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {Pfeil et al. - 2013 - Six networks on a universal neuromorphic computing.pdf:/home/danielsan/Zotero/storage/AAKUT2QI/Pfeil et al. - 2013 - Six networks on a universal neuromorphic computing.pdf:application/pdf}
}

@online{noauthor_digital_nodate,
	title = {Digital Learning System},
	url = {http://www.kip.uni-heidelberg.de/vision/research/dls/},
	urldate = {2020-02-10},
	file = {Digital Learning System:/home/danielsan/Zotero/storage/2ZTLDKAK/dls.html:text/html}
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: The third generation of neural network models},
	volume = {10},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	shorttitle = {Networks of spiking neurons},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on {McCulloch} Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	pages = {1659--1671},
	number = {9},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Maass, Wolfgang},
	urldate = {2020-02-11},
	date = {1997-12-01},
	langid = {english},
	keywords = {Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron},
	file = {ScienceDirect Snapshot:/home/danielsan/Zotero/storage/YCNQN3KF/S0893608097000117.html:text/html}
}

@online{noauthor_network_nodate,
	title = {A network on chip architecture and design methodology - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/abstract/document/1016885},
	urldate = {2020-02-11},
	file = {A network on chip architecture and design methodology - IEEE Conference Publication:/home/danielsan/Zotero/storage/9A9YA9IU/1016885.html:text/html}
}

@article{benini_networks_2002,
	title = {Networks on chips: a new {SoC} paradigm},
	volume = {35},
	issn = {1558-0814},
	doi = {10.1109/2.976921},
	shorttitle = {Networks on chips},
	abstract = {On-chip micronetworks, designed with a layered methodology, will meet the distinctive challenges of providing functionally correct, reliable operation of interacting system-on-chip components. A system on chip ({SoC}) can provide an integrated solution to challenging design problems in the telecommunications, multimedia, and consumer electronics domains. Much of the progress in these fields hinges on the designers' ability to conceive complex electronic engines under strong time-to-market pressure. Success will require using appropriate design and process technologies, as well as interconnecting existing components reliably in a plug-and-play fashion. Focusing on using probabilistic metrics such as average values or variance to quantify design objectives such as performance and power will lead to a major change in {SoC} design methodologies. Overall, these designs will be based on both deterministic and stochastic models. Creating complex {SoCs} requires a modular, component-based approach to both hardware and software design. Despite numerous challenges, the authors believe that developers will solve the problems of designing {SoC} networks. At the same time, they believe that a layered micronetwork design methodology will likely be the only path to mastering the complexity of future {SoC} designs.},
	pages = {70--78},
	number = {1},
	journaltitle = {Computer},
	author = {Benini, L. and De Micheli, G.},
	date = {2002-01},
	keywords = {average values, complex electronic engines, complex {SoCs}, Consumer electronics, Design methodology, design objectives, deterministic models, Engines, Fasteners, future {SoC} designs, hardware design, hardware-software codesign, integrated solution, interacting system-on-chip components, interconnecting components, layered methodology, layered micronetwork design methodology, microprocessor chips, modular component-based approach, Multimedia systems, multiprocessor interconnection networks, Network-on-a-chip, networks on chips, on-chip micronetworks, plug-and-play fashion, probabilistic metrics, Process design, process technologies, reconfigurable architectures, {SoC} design methodologies, {SoC} paradigm, software design, stochastic models, system on chip, System-on-a-chip, Telecommunication network reliability, Time to market, time-to-market pressure},
	file = {Full Text:/home/danielsan/Zotero/storage/9CVFNUW3/Benini and De Micheli - 2002 - Networks on chips a new SoC paradigm.pdf:application/pdf;IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/7CU49A8U/references.html:text/html}
}

@inproceedings{gehlhaar_neuromorphic_2014,
	location = {Salt Lake City, Utah, {USA}},
	title = {Neuromorphic processing: a new frontier in scaling computer architecture},
	isbn = {978-1-4503-2305-5},
	url = {http://dl.acm.org/citation.cfm?doid=2541940.2564710},
	doi = {10.1145/2541940.2564710},
	shorttitle = {Neuromorphic processing},
	eventtitle = {the 19th international conference},
	pages = {317--318},
	booktitle = {Proceedings of the 19th international conference on Architectural support for programming languages and operating systems - {ASPLOS} '14},
	publisher = {{ACM} Press},
	author = {Gehlhaar, Jeff},
	urldate = {2020-02-11},
	date = {2014},
	langid = {english},
	file = {Gehlhaar - 2014 - Neuromorphic processing a new frontier in scaling.pdf:/home/danielsan/Zotero/storage/CQLF3SI5/Gehlhaar - 2014 - Neuromorphic processing a new frontier in scaling.pdf:application/pdf}
}

@online{noauthor_ibm_2015,
	title = {{IBM} Research: Brain-inspired Chip},
	url = {http://www.research.ibm.com/articles/brain-chip.shtml},
	shorttitle = {{IBM} Research},
	abstract = {Introducing A Brain-inspired Computer and an End-to-End Ecosystem that Could Revolutionize Computing},
	urldate = {2020-02-11},
	date = {2015-08-05},
	langid = {english},
	file = {Snapshot:/home/danielsan/Zotero/storage/62TU8XPB/brain-chip.html:text/html}
}

@article{davies_loihi_2018,
	title = {Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
	volume = {38},
	issn = {1937-4143},
	doi = {10.1109/MM.2018.112130359},
	shorttitle = {Loihi},
	abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve {LASSO} optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a {CPU} iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
	pages = {82--99},
	number = {1},
	journaltitle = {{IEEE} Micro},
	author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and {McCoy}, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
	date = {2018-01},
	keywords = {Algorithm design and analysis, artificial intelligence, Biological neural networks, circuit optimisation, Computational modeling, Computer architecture, {CPU} iso-process-voltage-area, dendritic compartments, hierarchical connectivity, integrated circuit modelling, Intels process, {LASSO} optimization problems, learning (artificial intelligence), locally competitive algorithm, Loihi, machine learning, magnitude superior energy-delay-product, microprocessor chips, multiprocessing systems, neural chips, neuromorphic computing, neuromorphic manycore processor, Neuromorphics, Neurons, on-chip learning, programmable synaptic learning rules, size 14 nm, spike-based computation, spiking neural networks, synaptic delays},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/PZI8EBK7/8259423.html:text/html}
}

@article{debole_truenorth_2019,
	title = {{TrueNorth}: Accelerating From Zero to 64 Million Neurons in 10 Years},
	volume = {52},
	issn = {1558-0814},
	doi = {10.1109/MC.2019.2903009},
	shorttitle = {{TrueNorth}},
	abstract = {{IBM}'s brain-inspired processor is a massively parallel neural network inference engine containing 1 million spiking neurons and 256 million low-precision synapses. Now, after a decade of fundamental research spanning neuroscience, architecture, chips, systems, software and algorithms, {IBM} has delivered the largest neurosynaptic computer ever built.},
	pages = {20--29},
	number = {5},
	journaltitle = {Computer},
	author = {{DeBole}, Michael V. and Taba, Brian and Amir, Arnon and Akopyan, Filipp and Andreopoulos, Alexander and Risk, William P. and Kusnitz, Jeff and Ortega Otero, Carlos and Nayak, Tapan K. and Appuswamy, Rathinakumar and Carlson, Peter J. and Cassidy, Andrew S. and Datta, Pallab and Esser, Steven K. and Garreau, Guillaume J. and Holland, Kevin L. and Lekuch, Scott and Mastro, Michael and {McKinstry}, Jeff and di Nolfo, Carmelo and Paulovicks, Brent and Sawada, Jun and Schleupen, Kai and Shaw, Benjamin G. and Klamo, Jennifer L. and Flickner, Myron D. and Arthur, John V. and Modha, Dharmendra S.},
	date = {2019-05},
	keywords = {Biological neural networks, Brain modeling, Computational modeling, Computer architecture, {IBM} brain-inspired processor, inference mechanisms, low-precision synapses, neural chips, neural net architecture, Neurons, neurosynaptic computer, parallel neural network inference engine, Random access memory, spanning neuroscience, spiking neurons, Synapses, {TrueNorth}},
	file = {IEEE Xplore Abstract Record:/home/danielsan/Zotero/storage/SLY8BEF2/8713821.html:text/html}
}